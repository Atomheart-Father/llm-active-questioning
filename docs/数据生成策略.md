方法总览 (Overall Methods)

在大型语言模型(LLM)生成推理数据方面，近年来涌现出多种先进方法。这些方法旨在让模型在回答问题前“先思考后作答”，通过**链式思维 (Chain-of-Thought, CoT)**、**自我提问 (Self-Ask)**、**推理-行动 (ReAct)** 等策略，引导模型主动提出澄清问题并构建合理的推理路径\[1\][\[2\]](https://arxiv.org/abs/2210.03350%23:~:text=,questions,%2520which%2520additionally%2520improves%2520accuracy)。最新的前沿模型（如Google的Gemini 2.5系列）甚至默认集成了“先思考再回答”的机制，被称为“AI推理模型”，在回答前会暂停进行内部推理[\[3\]](https://techcrunch.com/2025/03/25/google-unveils-a-next-gen-ai-reasoning-model/%23:~:text=On%2520Tuesday,%2520Google%2520unveiled%2520Gemini,%25E2%2580%259Cthink%25E2%2580%259D%2520before%2520answering%2520a%2520question)[\[4\]](https://techcrunch.com/2025/03/25/google-unveils-a-next-gen-ai-reasoning-model/%23:~:text=Moving%2520forward,%2520Google%2520says%2520all,have%2520reasoning%2520capabilities%2520baked%2520in)。这些技术使得模型在数学、编程等复杂任务上表现显著提升，并被视为构建**多轮交互式推理代理**的关键[\[5\]](https://techcrunch.com/2025/03/25/google-unveils-a-next-gen-ai-reasoning-model/%23:~:text=Reasoning%2520techniques%2520have%2520helped%2520AI,models%2520are%2520also%2520more%2520expensive)[\[6\]](https://www.promptingguide.ai/techniques/react%23:~:text=The%2520ReAct%2520framework%2520can%2520allow,more%2520reliable%2520and%2520factual%2520responses)。

**多轮链式推理方法概览：** 研究者提出了多种Prompt模板与生成机制，实现模型逐步推理和多轮对话：

* **链式思维提示 (CoT Prompting)：** Wei等人（2022）发现，在提示中加入“让我们一步步思考”等指令，能诱导模型输出中间推理步骤，从而显著提升复杂推理任务性能\[7\]。CoT通常作为few-shot示例呈现，让模型在得出最终答案前罗列一系列推理步骤\[1\]。这一方法提高了模型的逻辑连贯性，但传统CoT是一次性输出完整推理链，缺乏与用户互动。  
* **Self-Ask 自问自答：** Press等人（2022）提出了“自问”策略，即模型在回答主问题前**先问自己一个或多个澄清子问题并自行回答**，再综合推理得到最终答案[\[2\]](https://arxiv.org/abs/2210.03350%23:~:text=,questions,%2520which%2520additionally%2520improves%2520accuracy)。例如模型面对多跳问题，会生成**子问题Q1、回答A1、子问题Q2、回答A2…**的链条，最后给出总解答[\[2\]](https://arxiv.org/abs/2210.03350%23:~:text=,questions,%2520which%2520additionally%2520improves%2520accuracy)。Self-Ask可结合外部检索：模型自问子问题时调用搜索引擎查询，再根据检索结果作答，从而显著提高知识问答的准确度[\[8\]](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/%23:~:text=*%2520Self,it%2520back%2520into%2520the%2520context)。这种方法实质上将链式思维拆解为**问答交互**，每个子问题聚焦特定细节，直到足以回答原始问题。  
* **ReAct 推理-行动：** Yao等人（2022）提出了ReAct框架，将**“思考”(Thought)**与**“动作”(Act)**交替生成[\[9\]](https://www.promptingguide.ai/techniques/react%23:~:text=Yao%2520et%2520al,actions%2520in%2520an%2520interleaved%2520manner)。模型一方面产生自然语言的推理过程，另一方面可以发出动作指令与外部环境交互（如检索知识、调用工具）[\[9\]](https://www.promptingguide.ai/techniques/react%23:~:text=Yao%2520et%2520al,actions%2520in%2520an%2520interleaved%2520manner)[\[10\]](https://www.promptingguide.ai/techniques/react%23:~:text=ReAct%2520is%2520a%2520general%2520paradigm,involved%2520to%2520perform%2520question%2520answering)。具体流程为：模型读入问题后，反复执行 *Thought* → *Action* → *Observation* 循环。例如在开放域问答中，Thought用于分析下一步需要的信息，Action用于查询搜索引擎，Observation则是检索结果，然后进入下一个Thought[\[11\]](https://www.promptingguide.ai/techniques/react%23:~:text=propagation)[\[12\]](https://www.promptingguide.ai/techniques/react%23:~:text=Image%2520Source:%2520Yao%2520et%2520al,opens%2520in%2520a%2520new%2520tab)。ReAct让模型在动态规划中获取新信息，解决了CoT纯靠内知识可能产生幻觉的问题[\[13\]](https://www.promptingguide.ai/techniques/react%23:~:text=Chain,fact%2520hallucination%2520and%2520error%2520propagation)[\[10\]](https://www.promptingguide.ai/techniques/react%23:~:text=ReAct%2520is%2520a%2520general%2520paradigm,involved%2520to%2520perform%2520question%2520answering)。实验表明，ReAct在知识问答和决策规划等任务上优于只凭静态思维的基线，并提高了推理过程的可解释性[\[14\]](https://www.promptingguide.ai/techniques/react%23:~:text=additional%2520information%2520that%2520leads%2520to,more%2520reliable%2520and%2520factual%2520responses)。  
* **PreAct 预测-推理-行动：** Fu等人（2024）进一步提出PreAct，将**预判 (prediction)**融入推理-行动循环[\[15\]](https://arxiv.org/abs/2402.11534%23:~:text=reflection,%2520thus%2520promoting%2520accurate%2520planning,with%2520other%2520memory%2520or%2520selection)。在PreAct中，模型在每一步行动前先预测可能的后续观察结果，据此调整推理策略[\[15\]](https://arxiv.org/abs/2402.11534%23:~:text=reflection,%2520thus%2520promoting%2520accurate%2520planning,with%2520other%2520memory%2520or%2520selection)。这种“先预想未来再行动”的机制让模型规划更深远，减少无效尝试。实验证明PreAct在复杂任务上比ReAct取得更高成功率，并可结合记忆模块增强长程推理[\[15\]](https://arxiv.org/abs/2402.11534%23:~:text=reflection,%2520thus%2520promoting%2520accurate%2520planning,with%2520other%2520memory%2520or%2520selection)。PreAct体现了通过预测校准推理路径的前沿思路。

以上方法为LLM构建多轮推理提供了不同侧重：CoT偏重内部连贯思考，Self-Ask/ReAct突出与环境或自身对话互动，PreAct则强调对未来步骤的前瞻。这些策略可结合使用，例如在ReAct框架中嵌入CoT示例，从而既利用外部知识又保持严谨的链式逻辑[\[16\]](https://www.promptingguide.ai/techniques/react%23:~:text=Results%2520show%2520that%2520ReAct%2520can,external%2520information%2520obtained%2520during%2520reasoning)[\[17\]](https://www.promptingguide.ai/techniques/react%23:~:text=interpretability%2520and%2520trustworthiness%2520of%2520LLMs,external%2520information%2520obtained%2520during%2520reasoning)。

数据合成流程设计 (Data Synthesis Process Design)

自动构造包含“**多轮对话 \+ 推理链 \+ 澄清问答**”的高质量数据集是当前研究热点。常用的合成流程包括：**利用强大基模生成对话、精巧Prompt工程控制输出结构、以及生成后自动过滤与人工后处理**。下面是几种具有代表性的公开方案：

* **大型模型自生成对话：** 类似Self-Instruct的方法被扩展用于对话数据。Facebook AI最新提出的**CoT-Self-Instruct**方法，让LLM先根据种子任务**思考推理步骤**，再据此生成新的复杂指令及其回答[\[18\]](https://arxiv.org/html/2507.23751v1%23:~:text=We%2520propose%2520CoT,Hard)。这一过程产出了大量高质量指令及解答，用于微调下游模型。关键在于通过在Prompt中要求模型**“先连贯推理，再给出问题和答案”**，让模型自行产出包含推理链的问答对[\[18\]](https://arxiv.org/html/2507.23751v1%23:~:text=We%2520propose%2520CoT,Hard)。生成后，研究者对数据进行筛选，过滤掉不一致或错误样本（详见下文质量控制）。这一流程表明，大模型（如GPT-4、Gemini等）可以充当“数据生成器”，**迭代自我对话**产生多轮推理示例\[19\][\[20\]](https://arxiv.org/html/2508.18743v1%23:~:text=To%2520build%2520a%2520high,CoT%2520corpora%2520or%2520complex%2520pipelines)。实践中，有项目使用Google Gemini-2.0或GPT-4在Colab环境运行特殊Prompt，批量生成包含澄清问答和思维链的对话\[19\][\[20\]](https://arxiv.org/html/2508.18743v1%23:~:text=To%2520build%2520a%2520high,CoT%2520corpora%2520or%2520complex%2520pipelines)。  
* **角色扮演式多Agent对话：** CAMEL等开源项目采用双代理自我对话来生成数据[\[21\]](https://ghli.org/camel.pdf%23:~:text=,and%2520capabilities%2520of%2520chat%2520agents)。例如CAMEL框架设定两个LLM代理（用户和助手）基于给定任务展开多轮互动，通过**角色扮演**产出逼真的对话数据。Huggingface上公开的CAMEL role-playing数据集就包含约22.9万条由两个ChatGPT角色自动生成的多轮对话[\[22\]](https://huggingface.co/camel-ai/CAMEL-13B-Role-Playing-Data%23:~:text=CAMEL,)。这种方法的优势是可以产生**复杂场景下的连续对话**，覆盖提问、澄清、回答的完整交互过程。不过需要精心设计初始任务和角色设定，以确保对话质量和多样性。  
* **澄清问答数据构造：** AmbigQA等工作提供了从单轮问答扩展出澄清对话的数据构造思路。Min等人（2020）发布的AmbigQA数据集，首先收集开放域中存在歧义的问题，并为每个问题人工标注多个**不同解释对应的答案**[\[23\]](https://www.researchgate.net/publication/347233146_AmbigQA_Answering_Ambiguous_Open-domain_Questions%23:~:text=They%2520introduced%2520AmbigQA,%2520a%2520dataset,of%2520ambiguity%2520such%2520as)。在此基础上，衍生研究通过**模拟澄清过程**生成对话：例如*Kracheninnikov*等人（*2023*）利用AmbigQA的数据，构建了**ClarifyingQA**对话集[\[24\]](https://github.com/krasheninnikov/clarifyingqa%23:~:text=ClarifyingQA%2520dataset%2520from%2520the%2520paper,Assistance%2520with%2520large%2520language%2520models)。具体做法是：取AmbigQA中的一个模糊问题当作用户提问，请人工标注员充当助手提出**澄清问题**，再由标注员扮演用户，根据AmbigQA提供的其中一个具体问题解释来回答澄清问[\[25\]](https://github.com/krasheninnikov/clarifyingqa%23:~:text=A%2520subset%2520of%2520AmbigQA%2520,ask%2520a%2520human%2520labeler%2520to)。最终助手据明确后的问题给出答案[\[26\]](https://github.com/krasheninnikov/clarifyingqa%23:~:text=,is%2520labeled%2520by%2520our%2520annotators)。ClarifyingQA 数据集中每条记录就是**“模糊问题 → 澄清提问 → 用户澄清 → 最终答案”**四轮对话[\[24\]](https://github.com/krasheninnikov/clarifyingqa%23:~:text=ClarifyingQA%2520dataset%2520from%2520the%2520paper,Assistance%2520with%2520large%2520language%2520models)[\[26\]](https://github.com/krasheninnikov/clarifyingqa%23:~:text=,is%2520labeled%2520by%2520our%2520annotators)。类似地，Findings EMNLP 2023的一项研究通过**InstructGPT生成澄清问句并人工校对**的方法，高效构造了大规模澄清问答对，用于训练下游模型[\[27\]](https://aclanthology.org/2023.findings-emnlp.772/%23:~:text=Asking%2520Clarification%2520Questions%2520to%2520Handle,define%2520a%2520pipeline%2520of)。这些流程体现了**人机结合的数据合成**：先用强模型生成对话草稿，再由人审校润色，最终得到高质量多轮澄清对话数据。  
* **Prompt工程与结构化标签：** 在数据生成过程中，精巧的提示设计至关重要。研究者常用**模板式Prompt**明确要求模型产出所需格式。例如我们可以设计系统提示：“**请根据给定问题生成一段多轮对话：用户首先提问，助手接着提出澄清问题…最后助手给出包含推理过程的答案**”\[28\]。在实际实现中，可构造占位符如：  
   系统提示*:* 你是一位耐心的导师，接下来根据用户问题进行多轮提问和解答。  
   用户*:* {原始问题（经过模糊化改写）}  
   助手*:* 问题澄清1？  
   用户*:* 澄清回答1。  
   助手*:* 最终答案（包括推理过程和结论）。  
   这样的模板确保每次生成都严格包含**澄清提问**和**逐步推理**要素\[29\]\[28\]。此外，输出中往往加入结构化标签或格式，比如显式列出“推理步骤1、2、3…”，或在澄清问题中使用固定句型“哪一个…: A 或 B？”[\[30\]](https://aclanthology.org/2023.findings-emnlp.772.pdf%23:~:text=3,If)。这些结构化约束有助于后续自动解析和质量监控。

总的来说，前沿项目倾向于**组合使用大模型生成+Prompt模板+人工校验**来合成推理对话数据。其中典型代表包括Stanford Self-Instruct（生成单轮指令数据）的方法扩展、Camel自主对话、AmbigQA衍生的数据构造流程等。设计良好的数据合成流程能够在减少人工成本的同时，产出多样且高质量的训练样本，为模型的主动推理能力打下基础。

数据质量控制方法 (Data Quality Control)

大规模生成数据时，如何确保**样本多样性、逻辑一致性和语义准确**是重大挑战。为此，近期研究和实践提出了多种质量控制机制，包括自动审查、分桶采样和质量评分等：

* **多样性保障：** 为避免生成数据千篇一律，常用的方法是监控词汇和语句的多样性指标。例如计算**TTR (Type-Token Ratio)**、**distinct-n**以及分布直方图等，量化数据集的重复率[\[31\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L13-L16)。如果发现模型生成的对话在措辞或结构上过于单一，可以通过**增加Prompt变体**或引入**采样温度**来提高变异度。另外，有研究对生成的问题进行**聚类采样**：将语料按照语义相似度聚类，只从每个簇中选取少量代表问题用于few-shot示例或数据集，从而防止模型反复练习某一类有限模式[\[32\]](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/%23:~:text=Zhang%2520et%2520al,a%2520diverse%2520set%2520of%2520examples)。这种“**分簇采样**”思路保证了覆盖广泛的问题类型，提升了数据集的覆盖面。  
* **逻辑一致性与正确性：** 对于包含推理链的输出，需要确保推理过程前后衔接、结论正确。一种自动审查办法是利用LLM自身来充当“**审稿人**”。比如在CoT-Self-Instruct流程中，作者引入了**答案一致性检查**：令模型对同一道题独立推理多次，看得到的答案是否一致；若生成的最终答案与模型多数投票结果不符，则认为该样本可能有误而丢弃[\[33\]](https://arxiv.org/html/2507.23751v1%23:~:text=in%2520both%2520cases%2520using%2520Chain,Self)。这一机制确保了保留下来的推理样本其结论高度可信。此外，可以设计**硬规则**检测：如验证推理链中的数学计算是否正确、知识引用是否与资料吻合、澄清问题是否真的消除了歧义等[\[34\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L18-L21)[\[35\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L26-L33)。对于不满足硬性逻辑的对话，可自动剔除或标记人工复核。  
* **难度分层与采样：** 为保证数据集中不同难度样本兼具，许多项目对生成的问题按难易度进行**分桶 (bucketing)**[\[31\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L13-L16)。具体做法是为每个问题计算一个难度分数（例如根据需要的推理步数、涉及知识的冷门程度等），再将样本划分为简单、中等、困难三级，并控制各类比例。这可防止模型训练时过度偏向简单模式或极端复杂模式，提升模型在各难度水平下的稳健性。一些开源工作提供了**难度度量脚本**来自动为数据打标签，并输出难度报告[\[36\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L13-L21)。在生成新数据时，可以针对缺乏的难度级别多生成一些，以均衡整体分布。  
* **混合质量评分机制：** 当前较有效的方案是结合**规则评估**和**模型评估**。例如本项目设计了**硬规则+模型打分的混合奖励**函数，对每条对话的多个维度打分[\[37\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L18-L26)。硬规则部分关注客观指标，如是否包含澄清问、思维链长度达标、格式规范等，每项通过正则或程序判断[\[35\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L26-L33)。模型打分部分则调用一个高性能LLM（如GPT-4或Gemini本身）从**流畅度、逻辑完整性、澄清恰当性**等主观维度给分[\[35\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L26-L33)。两者按权重融合得到总分，用于筛除低质量样本或作为强化学习的奖励信号。比如在我们设计的奖励函数中，逻辑严谨性、提问质量、推理完整性、交互自然度各有评分，并最终汇总为主奖励\[38\]\[39\]。这样的多信号机制能捕捉对话质量的不同方面，实现更全面的质量控制。  
* **人工审核与反馈迭代：** 尽管自动化手段可以过滤大部分低质数据，人工spot-check仍不可或缺。很多项目在生成数据初期安排专家对少量样本进行审核打分，将结果用于调整自动评分标准或Prompt策略。例如发现模型倾向于提出过度冗长的澄清问题时，可在人反馈基础上加入**“过度澄清惩罚”**项，降低这类对话的评分[\[36\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L13-L21)。同时，引入人类偏好数据（如让人类标注一批对话的优劣）训练一个质量判别模型，也是提高自动评分可靠性的途径之一。这方面OpenAI的做法是用RLHF（人类反馈强化学习）来微调模型，使其学会偏好高质量对话。近期有研究专门针对澄清场景设计了偏好标签方案：**模拟后续对话结果**来打分。如果模型提出澄清问能够带来更准确的答案，就给予高奖励；反之若直接作答可能答非所问，则鼓励模型选择澄清[\[40\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=TL;DR:%2520We%2520RLHF%2520train%2520LLMs,expected%2520outcomes%2520in%2520future%2520turns)[\[41\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=only%2520on%2520their%2520prior%2520contexts,against%2520the%2520answer%2520set%2520from)。这种“考虑未来回合收益”的偏好标注，使模型在训练中学会权衡**何时该询问澄清、何时直接回答**[\[42\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=ask%2520clarifying%2520questions%2520when%2520it,can%2520be%2520used%2520to%2520train)[\[43\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=conversation%2520turns,in%2520accuracy%2520of%2520such%2520judgments)。

通过上述多层次的质量控制，生成流程可以在保证**多样性**的同时，提高**准确性和连贯性**。简言之，先靠**规则**把关底线，再借助**模型/人工**精评细节，最后持续**迭代优化**。在实际工程中，我们会将这些机制融入数据管道，例如每生成N条对话就跑一次质量评估脚本，输出报告供开发者审阅，以便及时调整生成策略，确保整个数据集保持高质量水准[\[44\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L26-L34)[\[45\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L28-L33)。

澄清型问答生成 (Clarification Q\&A Generation)

在用户提问可能含糊不清时，引导模型**主动澄清**用户意图是提升问答质量的重要方向。围绕AmbigQA、ASQA等任务，学术界提出了一系列方法来增强模型的澄清能力，并设计了相应的数据和评估机制：

* **AmbigQA任务及数据：** AmbigQA[\[23\]](https://www.researchgate.net/publication/347233146_AmbigQA_Answering_Ambiguous_Open-domain_Questions%23:~:text=They%2520introduced%2520AmbigQA,%2520a%2520dataset,of%2520ambiguity%2520such%2520as)是华盛顿大学Min等人提出的一项开放域问答任务，关注**具有多重解释的用户问题**。AmbigQA数据集中，每个问题都被标注了多个可能的明确查询及其答案，以及（可选）一个澄清问题。模型需要输出一组问-答对，或者给出一个澄清性的问题再选择正确的答案。该数据揭示了超过一半自然问题存在歧义，涵盖**指代不清、人名重合、时间依赖**等多种模糊类型[\[46\]](https://www.researchgate.net/publication/347233146_AmbigQA_Answering_Ambiguous_Open-domain_Questions%23:~:text=,)[\[47\]](https://www.researchgate.net/publication/347233146_AmbigQA_Answering_Ambiguous_Open-domain_Questions%23:~:text=purposes%2520and%2520contain%2520very%2520specific,)。在AmbigQA基础上，后续衍生出**ASQA**（Ambiguous Question Answering with long answers）数据集[\[48\]](https://aclanthology.org/2023.findings-emnlp.772.pdf%23:~:text=Ming,Yang%2520Liu,%2520Yichong%2520Xu,%2520Chenguang)。Chang等人（2022）在ASQA中为AmbigQA每个问题增加了上下文和长答案，让模型**用长篇回答覆盖所有可能解释**[\[49\]](https://openreview.net/forum?id=vDvFT7IX4O&noteId=aTDttbspvZ%23:~:text=Abstract:%2520Questions%2520in%2520open,shot%2520prompting)[\[50\]](https://openreview.net/forum?id=vDvFT7IX4O&noteId=aTDttbspvZ%23:~:text=propose%2520a%2520novel%2520framework,%2520Tree,clarifications)。这相当于不麻烦用户，直接由系统穷举阐述。这类数据集为澄清型问答提供了评测基准。  
* **澄清问题生成方法：** 早期工作采用Pipeline：先检测问题是否歧义，再生成澄清问题，然后在获得用户澄清后再回答[\[51\]](https://aclanthology.org/2023.findings-emnlp.772.pdf%23:~:text=Figure%25202:%2520Overview%2520of%2520our,a%2520third%2520option,%2520evaluation%2520methods)[\[52\]](https://aclanthology.org/2023.findings-emnlp.772.pdf%23:~:text=3%2520Task%2520Overview%2520We%2520propose,question%2520and%2520relevant%2520passages,%2520the)。澄清问题通常以**选择问句**形式呈现，概括几种可能语义供用户选择。例如Findings EMNLP 2023提出澄清问模板：“**Which \[类别\]: \[选项1\], \[选项2\], ...?**”，每个选项对应原问题的一种可能解释[\[30\]](https://aclanthology.org/2023.findings-emnlp.772.pdf%23:~:text=3,If)。例如用户问：“乔丹什么时候退役的？”，澄清问可生成：““Which *person* named Jordan do you mean: Michael Jordan, or Jordan Peterson?””。研究表明，这种枚举式问句可以有效获取用户意图[\[30\]](https://aclanthology.org/2023.findings-emnlp.772.pdf%23:~:text=3,If)[\[53\]](https://aclanthology.org/2023.findings-emnlp.772.pdf%23:~:text=%25E2%2580%259CWhich%2520,Also)。澄清问生成时会评估分类别和选项质量，常用BLEU、BERTScore等评价生成问句与人工参考的接近程度[\[54\]](https://aclanthology.org/2023.findings-emnlp.772.pdf%23:~:text=There%2520should%2520be%2520an%2520,grained%2520evaluation)。此外，一些工作（如Aliannejadi等人在信息检索领域的ClariQ挑战）探索了利用模板和查询扩展技术零样本生成澄清问[\[55\]](https://www.microsoft.com/en-us/research/wp-content/uploads/2020/01/webconf-2020-camera-zamani-et-al.pdf%23:~:text=Microsoft%2520www,based%2520model%2520to%2520generate%2520them)[\[56\]](https://www.microsoft.com/en-us/research/publication/generating-clarifying-questions-for-information-retrieval/%23:~:text=Generating%2520Clarifying%2520Questions%2520for%2520Information,learned%2520from%2520weak%2520supervision%2520data)。总体来看，高质量的澄清问题应当**针对性强且覆盖所有歧义选项**，这需要模型具备一定的归纳总结能力。  
* **强化学习与奖励机制：** 针对澄清行为的训练，近期有研究引入了**偏好建模和强化学习**。Zhang等人（2025）提出，通过**模拟未来对话**来给模型行为打分[\[40\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=TL;DR:%2520We%2520RLHF%2520train%2520LLMs,expected%2520outcomes%2520in%2520future%2520turns)。具体而言，他们让模型对每个用户请求同时生成两种响应：(1) 不澄清直接回答；(2) 提出澄清问题并等待细节后再回答。然后模拟每种情况下未来用户的反馈和答案正确率。如果提出澄清能够覆盖不同用户意图并分别给出正确答案，则这一路径在偏好模型中得分更高，反之则直接回答可能因猜错用户意图而扣分[\[57\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=users%2520who%2520intended%2520a%2520different,our%2520proposed%2520preference%2520labeling%2520methods)[\[42\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=ask%2520clarifying%2520questions%2520when%2520it,can%2520be%2520used%2520to%2520train)。基于此偏好数据进行RLHF微调后，模型在AmbigQA风格的数据上能够**更明智地决定何时澄清**，成绩相比传统方法提高约5%[\[58\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=multiple%2520annotations,%2520we%2520evaluate%2520systems,In%2520our%2520experiments,%2520we)[\[59\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=Our%2520method%2520achieves%2520a%25205,in%2520accuracy%2520of%2520such%2520judgments)，在需要澄清时提出问题，不需要时直接回答且准确率更高[\[43\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=conversation%2520turns,in%2520accuracy%2520of%2520such%2520judgments)。另有一些尝试通过设计**奖励函数**鼓励澄清行为，例如对包含澄清问的对话给予额外奖励分，或引入惩罚项避免模型过度频繁地澄清用户已明确的信息[\[36\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L13-L21)。这些奖励策略可与上述混合评分结合，在强化训练中引导模型更好地平衡澄清与直接回答。  
* **代表性对话格式：** 从实际数据来看，**澄清型对话**往往遵循固定结构，这也为我们设计训练样本提供了范例。以ClarifyingQA数据集为例，其对话格式为：  
   **User:** 模糊问题 (原始提问，可能多义)  
   **Assistant:** 澄清问题 (询问用户具体指哪一种含义或细节)  
   **User:** 澄清回答 (用户针对澄清问题做出选择或提供细节)  
   **Assistant:** 最终答案 (根据明确后的问题给出答案，附解释)  
   这种四段式结构在许多澄清QA场景中通用[\[24\]](https://github.com/krasheninnikov/clarifyingqa%23:~:text=ClarifyingQA%2520dataset%2520from%2520the%2520paper,Assistance%2520with%2520large%2520language%2520models)[\[26\]](https://github.com/krasheninnikov/clarifyingqa%23:~:text=,is%2520labeled%2520by%2520our%2520annotators)。在实际生成数据时，我们可以参考这一格式来撰写Prompt模板和期望输出。同时，在澄清问题措辞上，AmbigQA的研究提供了“Which…or…?”这种模式供参考[\[30\]](https://aclanthology.org/2023.findings-emnlp.772.pdf%23:~:text=3,If)；而在最终答案部分，模型可被要求给出**逐步推理**（Chain-of-Thought）再得到答案，以保证回答的可靠性\[60\]\[61\]。通过借鉴公开数据的格式和内容，我们能更好地构建符合任务需求的对话样本。  
* **评价指标：** 为评估模型的澄清问答能力，学术界也制定了专门的指标。例如ASQA采用**Disambig-F1**和**Disambig-ROUGE**，用于度量模型产出是否涵盖了所有预期解释[\[50\]](https://openreview.net/forum?id=vDvFT7IX4O&noteId=aTDttbspvZ%23:~:text=propose%2520a%2520novel%2520framework,%2520Tree,clarifications)。在澄清问题生成上，有论文针对生成的类别和选项分别计算**准确率(EM)**和**BLEU-1**来评估澄清问句是否恰当地概括了歧义点[\[54\]](https://aclanthology.org/2023.findings-emnlp.772.pdf%23:~:text=There%2520should%2520be%2520an%2520,grained%2520evaluation)。另外，如果最终答案是长文，可能用ROUGE衡量其对各子问题答案的覆盖度。对于对话整体，也有使用人类评价打分的做法，考察**对话自然度、澄清有效性、答案正确性**等维度[\[45\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L28-L33)。这些评价标准为我们检验模型在澄清场景下的表现提供了依据。在构造训练数据和算法时，同样可以考虑这些指标作为优化目标的一部分。

综上，澄清型问答生成已经形成**数据-方法-评测**的完整闭环：有AmbigQA/ASQA等数据集提供真实案例，有Self-Ask、AT-CoT等方法提升澄清提问质量，还有RLHF等手段鼓励模型学会提问澄清。借鉴这些工作并结合本项目需求，我们可以设计模型在遇到不确定用户意图时，先礼貌提问核实，再给出详尽准确的回答，从而显著改善用户体验和答案的正确性。

可用于本项目的开源工具与数据集 (Recommended Tools and Datasets)

基于以上调研，我们推荐以下开源资源（符合MIT/Apache等宽松许可）供 *llm-active-questioning* 项目使用，这些工具和数据有望在Colab/Gemini环境下顺利运行：

* **Prompt模板与生成工具:**  
* *Guidance/Promptist*: 开源的Prompt设计库，方便编排多轮对话模板并调用LLM生成数据（Apache许可）。例如可以用Guidance定义上述ClarifyingQA格式的对话模板，一键生成上千条对话样本。  
* *LangChain*: 一款流行的LLM orkestration框架 (MIT许可)[\[11\]](https://www.promptingguide.ai/techniques/react%23:~:text=propagation)。可用于实现ReAct/自问等代理流程。在Colab中结合LangChain，可以方便地调用Gemini API执行搜索查询等动作，生成带工具交互的推理数据。  
* *trlX (Transformer Reinforcement Learning X)*: 一个开源的RLHF训练库 (Apache-2.0)，可用于在偏好数据上微调模型学会澄清问答。该工具提供了在Colab上训练小型模型的示例，如果本项目计划实施偏好优化，可考虑使用。  
* **对话与推理数据集:**  
* *AmbigQA*: 开放域歧义问答数据集[\[23\]](https://www.researchgate.net/publication/347233146_AmbigQA_Answering_Ambiguous_Open-domain_Questions%23:~:text=They%2520introduced%2520AmbigQA,%2520a%2520dataset,of%2520ambiguity%2520such%2520as)。可从Hugging Face下载（CC BY-NC 4.0）。其中包含的问题及多答案非常适合作为澄清问答场景的真实语料。我们可提取其中的模糊问题来设计生成任务，或直接利用其提供的澄清问题作为few-shot示例。  
* *ClarifyingQA*: 由ClarifyingQA论文公开的**四轮澄清对话**数据[\[24\]](https://github.com/krasheninnikov/clarifyingqa%23:~:text=ClarifyingQA%2520dataset%2520from%2520the%2520paper,Assistance%2520with%2520large%2520language%2520models) (MIT许可)。共611条模糊问题及对应1771条明确问题，附带人工撰写的澄清问和澄清回答[\[25\]](https://github.com/krasheninnikov/clarifyingqa%23:~:text=A%2520subset%2520of%2520AmbigQA%2520,ask%2520a%2520human%2520labeler%2520to)。这是高度相关的现成数据，可直接用于模型微调或作为验证集评估模型澄清能力。其结构与本项目目标吻合，可作为输出格式的标杆。  
* *Chain-of-Thought Collection*: 一个汇总了约1.88M条带推理链答案的数据集[\[62\]](https://www.kaggle.com/datasets/konradb/chain-of-thought-collection%23:~:text=Chain,CoT%2520rationales%2520across%25201,060%2520tasks)（开放获取）。涵盖数学、常识、代码等1060项任务，每条都有问题、答案和由GPT-3/4生成的思维链。该集合可用于训练模型“思考过程”的表达。不过其中多数为单轮问答形式，需改造成多轮交互形式。本项目可将其中的一些复杂问题改写模糊化，然后通过Prompt让模型补充澄清对话。  
* *Camel Role-Playing Conversations*: CAMEL项目公开的由ChatGPT自动生成的大规模对话数据（推测为MIT许可）[\[22\]](https://huggingface.co/camel-ai/CAMEL-13B-Role-Playing-Data%23:~:text=CAMEL,)。其中包含任务驱动的多轮对话，可筛选出包含解释性答案的部分。虽然不是专门的澄清QA数据，但其中角色扮演和协作问题求解的对话对本项目有借鉴意义。我们可以参考其对话风格，或提取其中链式推理的段落用作few-shot示例。  
* **模型与平台:**  
* *Qwen-7B/Qwen-14B (*通义千问*)*: 由阿里达摩院开源的中文/多语种大模型系列，采用Apache-2.0许可。Qwen模型在指令跟随和对话方面表现优秀，并支持在推理时输出思维链，非常适合用作本地推理的数据生成模型或微调基模。如果Colab上有足够GPU，可加载Qwen-7B进行少量样本测试。  
* *Google Colab \+ Gemini Code*: 如果已获取Gemini的使用权限，可以在Colab Enterprise笔记本中调用Gemini-2.5-Pro模型\[63\]。Gemini具备“思考后回答”特性，非常契合本项目需要。在Colab中使用Gemini还能借助其强大的推理能力批量生成数据\[64\]。需注意Gemini API的速率限制，可结合缓存策略分段生成。  
* *OpenAI GPT-4 (*有权限时*)*: 尽管GPT-4非开源，但如果团队有API权限，它依然是高质量数据生成的利器。GPT-4擅长产生详细的链式推理和富有逻辑的澄清提问，可用于小批量构造高难度对话样本作为“教科书”案例。生成后再由开源模型模仿扩充。  
* **其他工具:**  
* 自动评估脚本: 推荐编写或使用现有的质量评估脚本，例如计算distinct-1/2、思维链长度、答案准确率等的工具。此前提到的那些指标计算可以整合到一个Python脚本中，在每轮数据生成后运行，快速得出质量报告[\[35\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L26-L33)。开源项目如**NLPerf**或**Language Model Evaluation Harness**可能已实现了一些指标的计算函数，可加以利用。  
* 数据可视化: 为方便人工分析数据分布，多用一些Plot库将不同类别问题的比例、各维度评分绘制图表。例如matplotlib绘制难度直方图、或用TSNE将问题embedding降维展示多样性。这有助于直观发现数据生成中的偏差并迭代改进。

在选择以上资源时，我们需留意许可证兼容性。例如AmbigQA为非商用许可证，需确保项目用途符合其要求；ClarifyingQA为MIT则使用无顾虑。所幸大部分模型/工具均提供了宽松许可，可直接在Colab/Gemini环境下安装使用。综合运用这些开源数据和工具，将大大加快本项目开发进程，并避免从零开始造轮子。

风险与注意事项 (Risks and Considerations)

最后，需要强调在实现主动提问推理系统时应注意的潜在风险和应对策略：

* **过度澄清 vs. 漏澄清:** 模型可能陷入两个极端：要么对几乎所有问题都先问一堆澄清（哪怕用户问得很明确），要么在需要澄清时贸然作答。前者会让对话拖沓冗长，降低用户体验；后者则可能答非所问，产生误导[\[65\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=question%2520to%2520elicit%2520more%2520information,on%2520their%2520ability%2520to%2520ask)。为避免过度澄清，可在Prompt或奖励函数中加入约束，如检测到用户问题已经清晰具体时，模型**不应提出澄清**（可设计二分类器先判断歧义度）。同时，通过偏好学习让模型学会**识别澄清的收益**[\[41\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=only%2520on%2520their%2520prior%2520contexts,against%2520the%2520answer%2520set%2520from)，只有当澄清能显著提高答案正确率时才选择提问[\[59\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=Our%2520method%2520achieves%2520a%25205,in%2520accuracy%2520of%2520such%2520judgments)。在人工评测环节，也应专门观测这一点，确保模型行为适当平衡。  
* **幻觉与错误推理:** 让模型输出链式思维有时会暴露其潜在的幻觉，即编造不真实的中间推理或事实。若这些谬误未经检查就纳入最终答案，反而增加错误风险。对此，除前述一致性检查外，还可采取**引入检索验证**的措施：对模型生成的每条关键推理结论，调用搜索引擎或知识库验证其真伪。如不符则提示模型纠正。这类似于Tool-augmented CoT思想，将内在推理与**外部证据**对照，提升正确率。此外，应谨慎使用模型生成的数据来再训练模型，因为模型可能学习到某些细微错误推理模式。最好在人审或引入对抗验证后，再将数据用于训练。  
* **数据偏Bias:** 合成数据可能继承或放大预训练模型的偏见。例如模型提出的澄清问题选项可能隐含刻板印象（如默认职业性别等）。应对方法是在生成澄清问之前，对**敏感属性**进行检测过滤，或者在Prompt中明示要求“澄清问题不得涉及主观偏见”。同时，多样性控制也有助于减少单一角度偏见——确保模型生成不同风格、不同领域的问题，让模型见到更全面的场景。  
* **格式与解析错误:** 链式推理通常格式较复杂，如多步骤编号、Thought/Action标签等。在多轮对话场景中还涉及不同讲话人。模型有时可能输出格式不完善（例如漏写“助手:”标识）。为此在数据后处理环节，需要编写健壮的解析器或正则规则，捕获并修正常见格式错误。例如如果缺少“用户:”前缀，可以补上；若思维链步骤序号错乱，可重新排序。采用JSON等机器可读格式也是办法之一：比如让模型最终输出一个包含推理步骤和答案的JSON对象，以便程序自动检查字段完整性[\[37\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L18-L26)。  
* **许可和合规:** 虽然我们优先选用MIT/Apache许可的数据和模型，但仍需注意某些数据集（如AmbigQA）限制商用。如果本项目有商业化倾向，需要提前确认数据许可或寻求替代方案。此外，从线上抓取资料生成对话时，要避免无意中包含受版权保护的文本片段。一般来说，让模型生成的内容尽量自拟而非直接复制，是比较安全的做法。如果引用了维基百科等资料库，也应遵守相应的署名要求（如CC BY-SA协议）。  
* **系统性能:** 多轮推理和澄清会增加对话轮数，这对推理速度和成本提出挑战。每增加一轮问答，等于多一次模型调用。在Colab/Gemini环境中，应权衡模型规模与响应速度，或考虑**响应剪枝**：当模型非常有把握时直接回答，省去澄清回合。另一个技巧是在推理加速上应用量化、缓存等手段\[66\]\[64\]。例如Gemini模型可部署低精度INT4推理以提高速度\[66\]，同时将常见问题的回答缓存，避免重复生成。这些工程优化能减轻链式对话带来的性能开销。

总之，在开发LLM主动提问系统时，我们需要综合考虑用户体验、答案准确、运行效率和法律合规等方面的风险。通过参考先进方法并吸取前人经验，我们可以趋利避害，打造一个**既聪明又稳健**的推理问答系统。

**参考文献：**

1. Google DeepMind博客. *Gemini 2.5: Our newest Gemini model with thinking*. 2025年3月25日发布[\[3\]](https://techcrunch.com/2025/03/25/google-unveils-a-next-gen-ai-reasoning-model/%23:~:text=On%2520Tuesday,%2520Google%2520unveiled%2520Gemini,%25E2%2580%259Cthink%25E2%2580%259D%2520before%2520answering%2520a%2520question)[\[4\]](https://techcrunch.com/2025/03/25/google-unveils-a-next-gen-ai-reasoning-model/%23:~:text=Moving%2520forward,%2520Google%2520says%2520all,have%2520reasoning%2520capabilities%2520baked%2520in).  
2. Wei et al. *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models*. arXiv:2201.11903, 2022\[7\].  
3. Press et al. *Measuring and Narrowing the Compositionality Gap in Language Models*. EMNLP Findings 2023 (提出Self-Ask方法)[\[2\]](https://arxiv.org/abs/2210.03350%23:~:text=,questions,%2520which%2520additionally%2520improves%2520accuracy).  
4. Yao et al. *ReAct: Synergizing Reasoning and Acting in Language Models*. arXiv:2210.03629, 2022[\[9\]](https://www.promptingguide.ai/techniques/react%23:~:text=Yao%2520et%2520al,actions%2520in%2520an%2520interleaved%2520manner)[\[10\]](https://www.promptingguide.ai/techniques/react%23:~:text=ReAct%2520is%2520a%2520general%2520paradigm,involved%2520to%2520perform%2520question%2520answering).  
5. Fu et al. *PreAct: Prediction Enhances Agent’s Planning Ability*. COLING 2025[\[15\]](https://arxiv.org/abs/2402.11534%23:~:text=reflection,%2520thus%2520promoting%2520accurate%2520planning,with%2520other%2520memory%2520or%2520selection).  
6. Min et al. *AmbigQA: Answering Ambiguous Open-domain Questions*. EMNLP 2020[\[23\]](https://www.researchgate.net/publication/347233146_AmbigQA_Answering_Ambiguous_Open-domain_Questions%23:~:text=They%2520introduced%2520AmbigQA,%2520a%2520dataset,of%2520ambiguity%2520such%2520as).  
7. Chang et al. *ASQA: Factoid Questions Meet Long-Form Answers*. arXiv:2204.06092, 2022 (提出ASQA数据集)[\[48\]](https://aclanthology.org/2023.findings-emnlp.772.pdf%23:~:text=Ming,Yang%2520Liu,%2520Yichong%2520Xu,%2520Chenguang).  
8. Krasheninnikov et al. *ClarifyingQA: A Four-Turn Clarification Questions Dataset*. 2023 (MIT License)[\[24\]](https://github.com/krasheninnikov/clarifyingqa%23:~:text=ClarifyingQA%2520dataset%2520from%2520the%2520paper,Assistance%2520with%2520large%2520language%2520models)[\[26\]](https://github.com/krasheninnikov/clarifyingqa%23:~:text=,is%2520labeled%2520by%2520our%2520annotators).  
9. Kim et al. *Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented LLMs*. EMNLP 2023[\[67\]](https://openreview.net/forum?id=vDvFT7IX4O&noteId=aTDttbspvZ%23:~:text=TL;DR:%2520we%2520introduce%2520Tree%2520of,form%2520answer)[\[68\]](https://openreview.net/forum?id=vDvFT7IX4O&noteId=aTDttbspvZ%23:~:text=propose%2520a%2520novel%2520framework,%2520Tree,clarifications).  
10. Zhang et al. *Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying Questions*. ICLR 2025 (提出基于未来回合偏好学习的澄清策略)[\[40\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=TL;DR:%2520We%2520RLHF%2520train%2520LLMs,expected%2520outcomes%2520in%2520future%2520turns)[\[42\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=ask%2520clarifying%2520questions%2520when%2520it,can%2520be%2520used%2520to%2520train).  
11. Yu et al. *CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning*. arXiv:2507.23751, 2025[\[18\]](https://arxiv.org/html/2507.23751v1%23:~:text=We%2520propose%2520CoT,Hard)[\[33\]](https://arxiv.org/html/2507.23751v1%23:~:text=in%2520both%2520cases%2520using%2520Chain,Self).  
12. Choi et al. *CAC-CoT: Connector-Aware Compact Chain-of-Thought*. arXiv:2508.18743, 2025 (介绍用Gemini生成紧凑思维链)[\[69\]](https://arxiv.org/html/2508.18743v1%23:~:text=\(CAC,efficiency%2520without%2520loss%2520of%2520accuracy)[\[20\]](https://arxiv.org/html/2508.18743v1%23:~:text=To%2520build%2520a%2520high,CoT%2520corpora%2520or%2520complex%2520pipelines).  
13. Tang et al. *Clarifying Ambiguities: Ambiguity Type-Chain of Thought (AT-CoT)*. arXiv preprint, 2025 (提出按歧义类型澄清的提示方法)[\[70\]](https://www.researchgate.net/publication/347233146_AmbigQA_Answering_Ambiguous_Open-domain_Questions%23:~:text=understand%2520users'%2520information%2520needs%2520is,comprehensively%2520study%2520the%2520impact%2520of)[\[71\]](https://www.researchgate.net/publication/347233146_AmbigQA_Answering_Ambiguous_Open-domain_Questions%23:~:text=is%2520to%2520enhance%2520the%2520reasoning,clarifications%2520under%2520various%2520IR%2520scenarios).  
14. Aliannejadi et al. *Conversational Query Understanding and Clarification (ClariQ)*. TREC 2020 (信息检索澄清问挑战)[\[55\]](https://www.microsoft.com/en-us/research/wp-content/uploads/2020/01/webconf-2020-camera-zamani-et-al.pdf%23:~:text=Microsoft%2520www,based%2520model%2520to%2520generate%2520them)[\[56\]](https://www.microsoft.com/en-us/research/publication/generating-clarifying-questions-for-information-retrieval/%23:~:text=Generating%2520Clarifying%2520Questions%2520for%2520Information,learned%2520from%2520weak%2520supervision%2520data).  
15. Weng, Lilian. *Prompt Engineering Techniques*. 2023 (综述了Self-Ask、ReAct等提示策略)[\[8\]](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/%23:~:text=*%2520Self,it%2520back%2520into%2520the%2520context)[\[32\]](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/%23:~:text=Zhang%2520et%2520al,a%2520diverse%2520set%2520of%2520examples).  
16. PromptingGuide.ai. *ReAct Prompting*. (详细解析了ReAct框架与示例)[\[14\]](https://www.promptingguide.ai/techniques/react%23:~:text=additional%2520information%2520that%2520leads%2520to,more%2520reliable%2520and%2520factual%2520responses)[\[13\]](https://www.promptingguide.ai/techniques/react%23:~:text=Chain,fact%2520hallucination%2520and%2520error%2520propagation).

\[1\] \[7\] \[19\] \[28\] \[29\] \[60\] \[61\] \[64\] \[66\] 模型推理加速方案.txt

file://file-BGgnksm2GWvXYZoKztY8qw

[\[2\]](https://arxiv.org/abs/2210.03350%23:~:text=,questions,%2520which%2520additionally%2520improves%2520accuracy) \[2210.03350\] Measuring and Narrowing the Compositionality Gap in Language Models

[https://arxiv.org/abs/2210.03350](https://arxiv.org/abs/2210.03350)

[\[3\]](https://techcrunch.com/2025/03/25/google-unveils-a-next-gen-ai-reasoning-model/%23:~:text=On%2520Tuesday,%2520Google%2520unveiled%2520Gemini,%25E2%2580%259Cthink%25E2%2580%259D%2520before%2520answering%2520a%2520question) [\[4\]](https://techcrunch.com/2025/03/25/google-unveils-a-next-gen-ai-reasoning-model/%23:~:text=Moving%2520forward,%2520Google%2520says%2520all,have%2520reasoning%2520capabilities%2520baked%2520in) [\[5\]](https://techcrunch.com/2025/03/25/google-unveils-a-next-gen-ai-reasoning-model/%23:~:text=Reasoning%2520techniques%2520have%2520helped%2520AI,models%2520are%2520also%2520more%2520expensive) Google unveils a next-gen family of AI reasoning models | TechCrunch

[https://techcrunch.com/2025/03/25/google-unveils-a-next-gen-ai-reasoning-model/](https://techcrunch.com/2025/03/25/google-unveils-a-next-gen-ai-reasoning-model/)

[\[6\]](https://www.promptingguide.ai/techniques/react%23:~:text=The%2520ReAct%2520framework%2520can%2520allow,more%2520reliable%2520and%2520factual%2520responses) [\[9\]](https://www.promptingguide.ai/techniques/react%23:~:text=Yao%2520et%2520al,actions%2520in%2520an%2520interleaved%2520manner) [\[10\]](https://www.promptingguide.ai/techniques/react%23:~:text=ReAct%2520is%2520a%2520general%2520paradigm,involved%2520to%2520perform%2520question%2520answering) [\[11\]](https://www.promptingguide.ai/techniques/react%23:~:text=propagation) [\[12\]](https://www.promptingguide.ai/techniques/react%23:~:text=Image%2520Source:%2520Yao%2520et%2520al,opens%2520in%2520a%2520new%2520tab) [\[13\]](https://www.promptingguide.ai/techniques/react%23:~:text=Chain,fact%2520hallucination%2520and%2520error%2520propagation) [\[14\]](https://www.promptingguide.ai/techniques/react%23:~:text=additional%2520information%2520that%2520leads%2520to,more%2520reliable%2520and%2520factual%2520responses) [\[16\]](https://www.promptingguide.ai/techniques/react%23:~:text=Results%2520show%2520that%2520ReAct%2520can,external%2520information%2520obtained%2520during%2520reasoning) [\[17\]](https://www.promptingguide.ai/techniques/react%23:~:text=interpretability%2520and%2520trustworthiness%2520of%2520LLMs,external%2520information%2520obtained%2520during%2520reasoning) ReAct Prompting | Prompt Engineering Guide 

[https://www.promptingguide.ai/techniques/react](https://www.promptingguide.ai/techniques/react)

[\[8\]](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/%23:~:text=*%2520Self,it%2520back%2520into%2520the%2520context) [\[32\]](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/%23:~:text=Zhang%2520et%2520al,a%2520diverse%2520set%2520of%2520examples) Prompt Engineering | Lil'Log

[https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)

[\[15\]](https://arxiv.org/abs/2402.11534%23:~:text=reflection,%2520thus%2520promoting%2520accurate%2520planning,with%2520other%2520memory%2520or%2520selection) \[2402.11534\] PreAct: Prediction Enhances Agent's Planning Ability

[https://arxiv.org/abs/2402.11534](https://arxiv.org/abs/2402.11534)

[\[18\]](https://arxiv.org/html/2507.23751v1%23:~:text=We%2520propose%2520CoT,Hard) [\[33\]](https://arxiv.org/html/2507.23751v1%23:~:text=in%2520both%2520cases%2520using%2520Chain,Self) CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks

[https://arxiv.org/html/2507.23751v1](https://arxiv.org/html/2507.23751v1)

[\[20\]](https://arxiv.org/html/2508.18743v1%23:~:text=To%2520build%2520a%2520high,CoT%2520corpora%2520or%2520complex%2520pipelines) [\[69\]](https://arxiv.org/html/2508.18743v1%23:~:text=\(CAC,efficiency%2520without%2520loss%2520of%2520accuracy) CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks

[https://arxiv.org/html/2508.18743v1](https://arxiv.org/html/2508.18743v1)

[\[21\]](https://ghli.org/camel.pdf%23:~:text=,and%2520capabilities%2520of%2520chat%2520agents) \[PDF\] CAMEL: Communicative Agents for “Mind” Exploration of Large ...

[https://ghli.org/camel.pdf](https://ghli.org/camel.pdf)

[\[22\]](https://huggingface.co/camel-ai/CAMEL-13B-Role-Playing-Data%23:~:text=CAMEL,) camel-ai/CAMEL-13B-Role-Playing-Data \- Hugging Face

[https://huggingface.co/camel-ai/CAMEL-13B-Role-Playing-Data](https://huggingface.co/camel-ai/CAMEL-13B-Role-Playing-Data)

[\[23\]](https://www.researchgate.net/publication/347233146_AmbigQA_Answering_Ambiguous_Open-domain_Questions%23:~:text=They%2520introduced%2520AmbigQA,%2520a%2520dataset,of%2520ambiguity%2520such%2520as) [\[46\]](https://www.researchgate.net/publication/347233146_AmbigQA_Answering_Ambiguous_Open-domain_Questions%23:~:text=,) [\[47\]](https://www.researchgate.net/publication/347233146_AmbigQA_Answering_Ambiguous_Open-domain_Questions%23:~:text=purposes%2520and%2520contain%2520very%2520specific,) [\[70\]](https://www.researchgate.net/publication/347233146_AmbigQA_Answering_Ambiguous_Open-domain_Questions%23:~:text=understand%2520users'%2520information%2520needs%2520is,comprehensively%2520study%2520the%2520impact%2520of) [\[71\]](https://www.researchgate.net/publication/347233146_AmbigQA_Answering_Ambiguous_Open-domain_Questions%23:~:text=is%2520to%2520enhance%2520the%2520reasoning,clarifications%2520under%2520various%2520IR%2520scenarios) AmbigQA: Answering Ambiguous Open-domain Questions

[https://www.researchgate.net/publication/347233146\_AmbigQA\_Answering\_Ambiguous\_Open-domain\_Questions](https://www.researchgate.net/publication/347233146_AmbigQA_Answering_Ambiguous_Open-domain_Questions)

[\[24\]](https://github.com/krasheninnikov/clarifyingqa%23:~:text=ClarifyingQA%2520dataset%2520from%2520the%2520paper,Assistance%2520with%2520large%2520language%2520models) [\[25\]](https://github.com/krasheninnikov/clarifyingqa%23:~:text=A%2520subset%2520of%2520AmbigQA%2520,ask%2520a%2520human%2520labeler%2520to) [\[26\]](https://github.com/krasheninnikov/clarifyingqa%23:~:text=,is%2520labeled%2520by%2520our%2520annotators) GitHub \- krasheninnikov/clarifyingqa: ClarifyingQA dataset from the paper "assistance with large language models"

[https://github.com/krasheninnikov/clarifyingqa](https://github.com/krasheninnikov/clarifyingqa)

[\[27\]](https://aclanthology.org/2023.findings-emnlp.772/%23:~:text=Asking%2520Clarification%2520Questions%2520to%2520Handle,define%2520a%2520pipeline%2520of) Asking Clarification Questions to Handle Ambiguity in Open-Domain ...

[https://aclanthology.org/2023.findings-emnlp.772/](https://aclanthology.org/2023.findings-emnlp.772/)

[\[30\]](https://aclanthology.org/2023.findings-emnlp.772.pdf%23:~:text=3,If) [\[48\]](https://aclanthology.org/2023.findings-emnlp.772.pdf%23:~:text=Ming,Yang%2520Liu,%2520Yichong%2520Xu,%2520Chenguang) [\[51\]](https://aclanthology.org/2023.findings-emnlp.772.pdf%23:~:text=Figure%25202:%2520Overview%2520of%2520our,a%2520third%2520option,%2520evaluation%2520methods) [\[52\]](https://aclanthology.org/2023.findings-emnlp.772.pdf%23:~:text=3%2520Task%2520Overview%2520We%2520propose,question%2520and%2520relevant%2520passages,%2520the) [\[53\]](https://aclanthology.org/2023.findings-emnlp.772.pdf%23:~:text=%25E2%2580%259CWhich%2520,Also) [\[54\]](https://aclanthology.org/2023.findings-emnlp.772.pdf%23:~:text=There%2520should%2520be%2520an%2520,grained%2520evaluation) aclanthology.org

[https://aclanthology.org/2023.findings-emnlp.772.pdf](https://aclanthology.org/2023.findings-emnlp.772.pdf)

[\[31\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L13-L16) [\[34\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L18-L21) [\[35\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L26-L33) [\[36\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L13-L21) [\[37\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L18-L26) [\[44\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L26-L34) [\[45\]](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md%23L28-L33) GEMINI\_ORCHESTRATOR\_PROMPT.md

[https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI\_ORCHESTRATOR\_PROMPT.md](https://github.com/Atomheart-Father/llm-active-questioning/blob/433bb9348517147e83d26e16da24f14445d42121/prompt/GEMINI_ORCHESTRATOR_PROMPT.md)

\[38\] \[39\] \[63\] 技术方案设计.txt

file://file-7Q2pEJunQToSk47mAA1HYJ

[\[40\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=TL;DR:%2520We%2520RLHF%2520train%2520LLMs,expected%2520outcomes%2520in%2520future%2520turns) [\[41\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=only%2520on%2520their%2520prior%2520contexts,against%2520the%2520answer%2520set%2520from) [\[42\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=ask%2520clarifying%2520questions%2520when%2520it,can%2520be%2520used%2520to%2520train) [\[43\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=conversation%2520turns,in%2520accuracy%2520of%2520such%2520judgments) [\[57\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=users%2520who%2520intended%2520a%2520different,our%2520proposed%2520preference%2520labeling%2520methods) [\[58\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=multiple%2520annotations,%2520we%2520evaluate%2520systems,In%2520our%2520experiments,%2520we) [\[59\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=Our%2520method%2520achieves%2520a%25205,in%2520accuracy%2520of%2520such%2520judgments) [\[65\]](https://openreview.net/forum?id=cwuSAR7EKd%23:~:text=question%2520to%2520elicit%2520more%2520information,on%2520their%2520ability%2520to%2520ask) Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying Questions | OpenReview

[https://openreview.net/forum?id=cwuSAR7EKd](https://openreview.net/forum?id=cwuSAR7EKd)

[\[49\]](https://openreview.net/forum?id=vDvFT7IX4O&noteId=aTDttbspvZ%23:~:text=Abstract:%2520Questions%2520in%2520open,shot%2520prompting) [\[50\]](https://openreview.net/forum?id=vDvFT7IX4O&noteId=aTDttbspvZ%23:~:text=propose%2520a%2520novel%2520framework,%2520Tree,clarifications) [\[67\]](https://openreview.net/forum?id=vDvFT7IX4O&noteId=aTDttbspvZ%23:~:text=TL;DR:%2520we%2520introduce%2520Tree%2520of,form%2520answer) [\[68\]](https://openreview.net/forum?id=vDvFT7IX4O&noteId=aTDttbspvZ%23:~:text=propose%2520a%2520novel%2520framework,%2520Tree,clarifications) Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models | OpenReview

[https://openreview.net/forum?id=vDvFT7IX4O\&noteId=aTDttbspvZ](https://openreview.net/forum?id=vDvFT7IX4O&noteId=aTDttbspvZ)

[\[55\]](https://www.microsoft.com/en-us/research/wp-content/uploads/2020/01/webconf-2020-camera-zamani-et-al.pdf%23:~:text=Microsoft%2520www,based%2520model%2520to%2520generate%2520them) \[PDF\] Generating Clarifying Questions for Information Retrieval \- Microsoft

[https://www.microsoft.com/en-us/research/wp-content/uploads/2020/01/webconf-2020-camera-zamani-et-al.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2020/01/webconf-2020-camera-zamani-et-al.pdf)

[\[56\]](https://www.microsoft.com/en-us/research/publication/generating-clarifying-questions-for-information-retrieval/%23:~:text=Generating%2520Clarifying%2520Questions%2520for%2520Information,learned%2520from%2520weak%2520supervision%2520data) Generating Clarifying Questions for Information Retrieval \- Microsoft

[https://www.microsoft.com/en-us/research/publication/generating-clarifying-questions-for-information-retrieval/](https://www.microsoft.com/en-us/research/publication/generating-clarifying-questions-for-information-retrieval/)

[\[62\]](https://www.kaggle.com/datasets/konradb/chain-of-thought-collection%23:~:text=Chain,CoT%2520rationales%2520across%25201,060%2520tasks) Chain-of-Thought collection \- Kaggle

[https://www.kaggle.com/datasets/konradb/chain-of-thought-collection](https://www.kaggle.com/datasets/konradb/chain-of-thought-collection)

