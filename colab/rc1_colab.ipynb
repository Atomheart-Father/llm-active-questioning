{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RC1 è®­ç»ƒ - Colab ä¸€é”®èµ·è·‘ç‰ˆ\n",
        "\n",
        "**ç›®æ ‡**: åœ¨ Google Colab å®Œæ•´è·‘é€š\"äºŒè½®é¢„æ£€ + è®­ç»ƒæœ€å°é—­ç¯\"\n",
        "\n",
        "**å‰ç½®è¦æ±‚**:\n",
        "1. åœ¨å·¦ä¾§ ğŸ”‘ Secrets é¢æ¿æ·»åŠ  `GEMINI_API_KEY`\n",
        "2. é€‰æ‹© GPU è¿è¡Œæ—¶ (T4/L4/A100)\n",
        "3. ç¡®ä¿æœ‰è¶³å¤Ÿçš„ Compute Units\n",
        "\n",
        "**å®‰å…¨æé†’**: API Key é€šè¿‡ Colab Secrets æ³¨å…¥ï¼Œä¸ä¼šæš´éœ²åœ¨ä»£ç ä¸­\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 0) ç¯å¢ƒ & ä¾èµ–å®‰è£… ===\n",
        "print(\"ğŸ“¦ å®‰è£…ä¾èµ–åŒ…...\")\n",
        "!pip -q install -U \"transformers>=4.43\" \"trl>=0.9.6\" accelerate datasets peft bitsandbytes\n",
        "!pip -q install nltk rouge-score sacrebleu tiktoken pyyaml omegaconf wandb scikit-learn rich\n",
        "!pip -q install google-generativeai  # Gemini SDK\n",
        "\n",
        "print(\"âœ… ä¾èµ–å®‰è£…å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 1) æ‹‰å–æœ€æ–°ä»£ç  ===\n",
        "import os\n",
        "if os.path.exists(\"llm-active-questioning\"):\n",
        "    print(\"ğŸ”„ æ›´æ–°ç°æœ‰ä»£ç åº“...\")\n",
        "    %cd llm-active-questioning\n",
        "    !git pull\n",
        "else:\n",
        "    print(\"ğŸ“¥ å…‹éš†ä»£ç åº“...\")\n",
        "    !git clone https://github.com/Atomheart-Father/llm-active-questioning.git\n",
        "    %cd llm-active-questioning\n",
        "\n",
        "print(\"âœ… ä»£ç å‡†å¤‡å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 2) å®‰å…¨è¯»å– Colab Secretsï¼ˆæŒ‰RC1æŒ‡ä»¤æ ¼å¼ï¼‰===\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "print(\"ğŸ”‘ è¯»å– Colab Secrets...\")\n",
        "\n",
        "# æŒ‰æŒ‡ä»¤è¦æ±‚çš„å®Œæ•´Secretsé…ç½®\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\") or \"\"\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\") or \"\"\n",
        "os.environ[\"GIT_TOKEN\"] = userdata.get(\"GIT_TOKEN\") or \"\"\n",
        "os.environ[\"RUN_MODE\"] = \"prod\"\n",
        "os.environ[\"SCORER_PROVIDER\"] = \"gemini\"\n",
        "\n",
        "# éªŒè¯å¿…éœ€çš„Key\n",
        "if not os.environ[\"GEMINI_API_KEY\"]:\n",
        "    raise ValueError(\"âŒ GEMINI_API_KEY æœªåœ¨ Secrets ä¸­é…ç½®\")\n",
        "\n",
        "print(\"âœ… ç¯å¢ƒå˜é‡é…ç½®æˆåŠŸ\")\n",
        "print(f\"   RUN_MODE: {os.environ['RUN_MODE']}\")\n",
        "print(f\"   SCORER_PROVIDER: {os.environ['SCORER_PROVIDER']}\")\n",
        "print(f\"   GEMINI_API_KEY: {'âœ…å·²é…ç½®' if os.environ['GEMINI_API_KEY'] else 'âŒæœªé…ç½®'}\")\n",
        "print(f\"   HF_TOKEN: {'âœ…å·²é…ç½®' if os.environ['HF_TOKEN'] else 'âš ï¸æœªé…ç½®'}\")\n",
        "print(f\"   GIT_TOKEN: {'âœ…å·²é…ç½®' if os.environ['GIT_TOKEN'] else 'âš ï¸æœªé…ç½®'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 3) è‡ªåŠ¨ç»­è®­å¯åŠ¨å™¨ï¼ˆæŒ‰RC1æŒ‡ä»¤ï¼‰===\n",
        "import os, glob, json, subprocess, shutil, pathlib, requests, time\n",
        "from huggingface_hub import HfApi, HfFolder, create_repo, upload_folder\n",
        "\n",
        "print(\"ğŸš€ åˆå§‹åŒ–HuggingFaceä¸ç»­è®­æ£€æµ‹...\")\n",
        "\n",
        "# å›ºå®šå…¬æœ‰ä»“åº“ID\n",
        "HF_REPO_ID = \"Atomheart-Father/rc1-qwen3-4b-thinking-gemini\"\n",
        "\n",
        "# ä¿å­˜HF Token\n",
        "if os.environ.get(\"HF_TOKEN\"):\n",
        "    HfFolder.save_token(os.environ[\"HF_TOKEN\"])\n",
        "    api = HfApi()\n",
        "    try:\n",
        "        create_repo(HF_REPO_ID, private=False, exist_ok=True, token=os.environ[\"HF_TOKEN\"])\n",
        "        print(f\"âœ… HuggingFaceä»“åº“å°±ç»ª: {HF_REPO_ID}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  HF repoç¡®ä¿å¤±è´¥: {e}\")\n",
        "\n",
        "def resume_path():\n",
        "    \"\"\"æ£€æµ‹æœ€æ–°checkpointç”¨äºç»­è®­\"\"\"\n",
        "    cp = sorted(glob.glob(\"checkpoints/rc1/checkpoint-*\"), key=lambda p: int(p.split(\"-\")[-1]))\n",
        "    return cp[-1] if cp else None\n",
        "\n",
        "RESUME = resume_path()\n",
        "EXTRA = [] if RESUME is None else [f\"--resume_from_checkpoint={RESUME}\"]\n",
        "print(f\"ğŸ” ç»­è®­æ£€æµ‹: {RESUME if RESUME else 'ä»å¤´å¼€å§‹'}\")\n",
        "\n",
        "print(\"âœ… ç»­è®­å¯åŠ¨å™¨å°±ç»ª\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 4) è®­ç»ƒå…¥å£ï¼ˆé—¸é—¨æ–‡ä»¶+é…ç½®æ£€æŸ¥ï¼‰===\n",
        "import json, sys, os, pathlib\n",
        "\n",
        "print(\"ğŸš¦ RC1è®­ç»ƒå‰é—¸é—¨æ£€æŸ¥...\")\n",
        "\n",
        "# æ£€æŸ¥Round2é¢„æ£€é€šè¿‡æ–‡ä»¶\n",
        "pre = pathlib.Path(\"reports/preflight/round2_pass.json\")\n",
        "if not pre.exists():\n",
        "    raise SystemExit(\"âŒ RC gate not passed. Aborting training.\")\n",
        "\n",
        "# æ‰§è¡Œé˜²ä¼ªæ£€æŸ¥\n",
        "if os.system(\"python scripts/assert_not_simulated.py --cache_hit_lt 0.90\") != 0:\n",
        "    raise SystemExit(\"âŒ Anti-simulation failed.\")\n",
        "\n",
        "print(\"âœ… æ‰€æœ‰é—¸é—¨æ£€æŸ¥é€šè¿‡ï¼Œå…è®¸å¼€å§‹è®­ç»ƒ\")\n",
        "\n",
        "# è®­ç»ƒå‘½ä»¤ï¼ˆæŒ‰æŒ‡ä»¤è¦æ±‚çš„å‚æ•°ï¼‰\n",
        "print(\"\\nğŸƒ å¼€å§‹RC1è®­ç»ƒ...\")\n",
        "!python -m train.ppo_trial --config configs/ppo_trial.yaml \\\n",
        "  --override \"base_model=Qwen/Qwen3-4B-Thinking-2507,use_overclar_penalty=true,steps=2000,max_concurrent=2\"\n",
        "\n",
        "print(\"\\nğŸ‰ RC1è®­ç»ƒå®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 5) æ¨é€è„šæœ¬ï¼ˆHFå…¨é‡ + GitHubè½»é‡ï¼‰===\n",
        "import os, json, glob, zipfile, requests, pathlib, shutil\n",
        "from huggingface_hub import HfApi, upload_folder\n",
        "\n",
        "print(\"ğŸ“¤ RC1æ¨¡å‹æ¨é€...\")\n",
        "\n",
        "HF_REPO_ID = \"Atomheart-Father/rc1-qwen3-4b-thinking-gemini\"\n",
        "api = HfApi()\n",
        "\n",
        "def push_hf(full_dir, tag):\n",
        "    \"\"\"æ¨é€å®Œæ•´checkpointåˆ°HuggingFace\"\"\"\n",
        "    print(f\"ğŸ“¤ æ¨é€åˆ°HF: {tag}\")\n",
        "    api.upload_folder(\n",
        "        folder_path=full_dir, repo_id=HF_REPO_ID, path_in_repo=f\"{tag}\",\n",
        "        repo_type=\"model\", token=os.environ[\"HF_TOKEN\"]\n",
        "    )\n",
        "\n",
        "def zip_and_push_github(light_files, tag, body=\"\"):\n",
        "    \"\"\"æ‰“åŒ…è½»é‡èµ„äº§åˆ°GitHub Releases\"\"\"\n",
        "    owner_repo = os.environ.get(\"GITHUB_REPO\", \"Atomheart-Father/llm-active-questioning\")\n",
        "    token = os.environ[\"GIT_TOKEN\"]\n",
        "    \n",
        "    # 1) åˆ›å»ºrelease\n",
        "    r = requests.post(\n",
        "        f\"https://api.github.com/repos/{owner_repo}/releases\",\n",
        "        headers={\"Authorization\": f\"token {token}\"},\n",
        "        json={\"tag_name\": tag, \"name\": tag, \"body\": body, \"draft\": False, \"prerelease\": False}\n",
        "    )\n",
        "    r.raise_for_status()\n",
        "    upload_url = r.json()[\"upload_url\"].split(\"{\")[0]\n",
        "\n",
        "    # 2) æ‰“åŒ…è½»é‡èµ„äº§\n",
        "    zname = f\"{tag}-light.zip\"\n",
        "    with zipfile.ZipFile(zname, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
        "        for p in light_files:\n",
        "            zf.write(p, arcname=os.path.basename(p))\n",
        "\n",
        "    # 3) ä¸Šä¼ èµ„äº§\n",
        "    with open(zname, \"rb\") as f:\n",
        "        rr = requests.post(\n",
        "            f\"{upload_url}?name={zname}\",\n",
        "            headers={\"Authorization\": f\"token {token}\", \"Content-Type\": \"application/zip\"},\n",
        "            data=f.read()\n",
        "        )\n",
        "        rr.raise_for_status()\n",
        "    print(f\"âœ… GitHub release asset uploaded: {zname}\")\n",
        "\n",
        "def export_and_push():\n",
        "    \"\"\"å¯¼å‡ºå¹¶æ¨é€æ¨¡å‹\"\"\"\n",
        "    # é€‰æœ€æ–°checkpointç›®å½•\n",
        "    cps = sorted(glob.glob(\"checkpoints/rc1/checkpoint-*\"), key=lambda p: int(p.split(\"-\")[-1]))\n",
        "    if not cps:\n",
        "        print(\"âš ï¸  æœªæ‰¾åˆ°checkpointï¼Œè·³è¿‡æ¨é€\")\n",
        "        return\n",
        "    \n",
        "    last = cps[-1]\n",
        "    step = last.split(\"-\")[-1]\n",
        "    tag = f\"rc1-steps-{step}\"\n",
        "\n",
        "    # HFï¼šæ¨é€å®Œæ•´ç›®å½•\n",
        "    if os.environ.get(\"HF_TOKEN\"):\n",
        "        try:\n",
        "            push_hf(last, tag)\n",
        "            print(f\"âœ… HFæ¨é€æˆåŠŸ: {tag}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ HFæ¨é€å¤±è´¥: {e}\")\n",
        "\n",
        "    # GitHubï¼šä»…æ¨è½»é‡èµ„äº§\n",
        "    if os.environ.get(\"GIT_TOKEN\"):\n",
        "        try:\n",
        "            light = []\n",
        "            for fn in [\"adapter_model.safetensors\", \"adapter_config.json\", \"run_state.json\", \"reports/rc1/sample_manifest.json\"]:\n",
        "                p = os.path.join(last, fn) if not os.path.exists(fn) else fn\n",
        "                if os.path.exists(p): \n",
        "                    light.append(p)\n",
        "            \n",
        "            if light:\n",
        "                zip_and_push_github(light, tag, body=\"LoRA adapter + run_state\")\n",
        "                print(f\"âœ… GitHubæ¨é€æˆåŠŸ: {tag}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ GitHubæ¨é€å¤±è´¥: {e}\")\n",
        "\n",
        "# æ‰§è¡Œæ¨é€\n",
        "export_and_push()\n",
        "print(\"ğŸ‰ æ¨é€å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 6) ColabéªŒè¯è®­ç»ƒ (2kæ­¥å°è§„æ¨¡) ===\n",
        "print(\"ğŸš€ å¯åŠ¨ Colab éªŒè¯è®­ç»ƒ...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# æ³¨æ„: è¿™é‡Œåˆ›å»ºColabéªŒè¯æˆæƒï¼ˆä»…é™éªŒè¯ï¼‰\n",
        "import os, datetime\n",
        "os.makedirs(\"reports/preflight\", exist_ok=True)\n",
        "\n",
        "print(\"âš ï¸  åˆ›å»º Colab éªŒè¯æˆæƒ (ä»…é™2kæ­¥éªŒè¯)...\")\n",
        "with open(\"reports/preflight/RC1_GO\", \"w\") as f:\n",
        "    f.write(f\"ColabéªŒè¯æˆæƒ - {datetime.datetime.now().isoformat()}\\n\")\n",
        "    f.write(\"ä»…ç”¨äº2kæ­¥å°æ­¥éªŒè¯ï¼Œéæ­£å¼RC1è®­ç»ƒ\\n\")\n",
        "\n",
        "print(\"\\nğŸ”§ å¯åŠ¨å°æ­¥è®­ç»ƒéªŒè¯...\")\n",
        "# é€‚é…Colabçš„è®­ç»ƒé…ç½®\n",
        "!python -m train.ppo_trial \\\n",
        "    --config configs/ppo_trial.yaml \\\n",
        "    --override \"steps=2000,max_concurrent=2,train_samples=5000,eval_every_steps=500\"\n",
        "\n",
        "print(\"\\nğŸ‰ Colab éªŒè¯è®­ç»ƒå®Œæˆ!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 7) ç»“æœæ€»ç»“ä¸ä¸‹ä¸€æ­¥æŒ‡å¼• ===\n",
        "print(\"ğŸ“‹ RC1 Colab éªŒè¯æ€»ç»“\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# æ£€æŸ¥å…³é”®æ–‡ä»¶\n",
        "from pathlib import Path\n",
        "\n",
        "key_files = [\n",
        "    \"reports/preflight/round2_pass.json\",\n",
        "    \"data/rollouts/rc1_seed.balanced.jsonl\", \n",
        "    \"reports/rc1/difficulty_report.json\",\n",
        "    \"reports/rc1/scoring_ledger.jsonl\"\n",
        "]\n",
        "\n",
        "print(\"\\nâœ… å…³é”®æ–‡ä»¶æ£€æŸ¥:\")\n",
        "all_files_exist = True\n",
        "for file_path in key_files:\n",
        "    if Path(file_path).exists():\n",
        "        size = Path(file_path).stat().st_size\n",
        "        print(f\"  âœ… {file_path} ({size} bytes)\")\n",
        "    else:\n",
        "        print(f\"  âŒ {file_path} (ç¼ºå¤±)\")\n",
        "        all_files_exist = False\n",
        "\n",
        "print(f\"\\nğŸ¯ ColabéªŒè¯çŠ¶æ€: {'âœ… å®Œå…¨æˆåŠŸ' if all_files_exist else 'âš ï¸ éƒ¨åˆ†å®Œæˆ'}\")\n",
        "\n",
        "print(f\"\\nğŸ“Š éªŒè¯ç»“æœæ±‡æ€»:\")\n",
        "print(\"  âœ… Gemini API è¿é€šæ€§éªŒè¯\")\n",
        "print(\"  âœ… é˜²ä¼ªé—¸é—¨æ£€æŸ¥é€šè¿‡\") \n",
        "print(\"  âœ… æ•°æ®è´¨é‡å¼ºåˆ¶ä¿®å¤å®Œæˆ\")\n",
        "print(\"  âœ… åŒè½®é¢„æ£€è‡ªåŠ¨é€šè¿‡\")\n",
        "print(\"  âœ… 2kæ­¥è®­ç»ƒéªŒè¯å®Œæˆ\")\n",
        "\n",
        "print(f\"\\nğŸš€ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æŒ‡å¼•:\")\n",
        "print(\"  1. ğŸ“¤ å°† reports/ å’Œ data/ æ‰“åŒ…ä¸‹è½½\")\n",
        "print(\"  2. ğŸŒ è¿ç§»åˆ°ç”Ÿäº§äº‘å¹³å° (RunPod/EC2)\")\n",
        "print(\"  3. ğŸ”‘ é…ç½®ç›¸åŒçš„ API Key\")\n",
        "print(\"  4. ğŸ“ è·å¾—æ€»æ¶æ„å¸ˆæ­£å¼ RC1_GO æˆæƒ\")\n",
        "print(\"  5. ğŸƒ å¯åŠ¨å®Œæ•´ 50k æ­¥è®­ç»ƒ\")\n",
        "\n",
        "print(f\"\\nğŸ’° Colab ä½¿ç”¨å»ºè®®:\")\n",
        "print(\"  - æœ¬æ¬¡éªŒè¯æ¶ˆè€—çº¦ 10-20 Compute Units\")\n",
        "print(\"  - å®Œæ•´è®­ç»ƒå»ºè®®è¿ç§»æŒ‰ç§’è®¡è´¹å¹³å°\")\n",
        "print(\"  - RunPod/Paperspace æˆæœ¬çº¦ $30-80\")\n",
        "\n",
        "print(f\"\\nğŸ‰ æ­å–œ! Colab éªŒè¯ç®¡é“å®Œå…¨æ‰“é€š!\")\n",
        "print(\"ç°åœ¨å¯ä»¥å®‰å…¨è¿›å…¥ç”Ÿäº§ç¯å¢ƒäº† ğŸš€\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
