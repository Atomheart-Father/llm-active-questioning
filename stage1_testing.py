#!/usr/bin/env python3
"""
ç¬¬ä¸€é˜¶æ®µæµ‹è¯•è„šæœ¬ï¼šæ ¸å¿ƒæ¦‚å¿µéªŒè¯
ä¸»è¦ä»»åŠ¡ï¼š
1. éªŒè¯Qwen3-4B-Thinkingæ¨¡å‹åŠ è½½
2. å®ç°åŸºç¡€çš„ä¸»åŠ¨æé—®æœºåˆ¶
3. æ„å»ºMVPæµ‹è¯•æ•°æ®
4. è¿›è¡Œåˆæ­¥å¯¹æ¯”å®éªŒ
"""

import sys
import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
from typing import Dict, List, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.utils.config import get_config
from src.utils.logging import get_logger


class Stage1Tester:
    """ç¬¬ä¸€é˜¶æ®µæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.config = get_config()
        self.logger = get_logger("stage1_test")
        self.tokenizer = None
        self.model = None
        
        # MVPæµ‹è¯•æ•°æ®
        self.test_cases = self._create_mvp_test_cases()
        self.user_responses = self._create_user_response_mapping()
        
        self.logger.info("ç¬¬ä¸€é˜¶æ®µæµ‹è¯•å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _create_mvp_test_cases(self) -> List[Dict]:
        """åˆ›å»ºMVPæµ‹è¯•æ¡ˆä¾‹ - 5-10ä¸ªç²¾å¿ƒè®¾è®¡çš„åœºæ™¯"""
        return [
            {
                "id": 1,
                "type": "æ­§ä¹‰å‹",
                "question": "ä»–æ˜¯å“ªå¹´å»ä¸–çš„ï¼Ÿ",
                "context": "",
                "expected_behavior": "æé—®æ¾„æ¸…æŒ‡ä»£å¯¹è±¡",
                "correct_answer": "éœ€è¦çŸ¥é“å…·ä½“æŒ‡è°æ‰èƒ½å›ç­”"
            },
            {
                "id": 2,
                "type": "ä¸Šä¸‹æ–‡ä¸è¶³å‹", 
                "question": "é‚£å®¶é¤å…çš„è¥ä¸šæ—¶é—´æ˜¯ä»€ä¹ˆï¼Ÿ",
                "context": "",
                "expected_behavior": "æé—®å…·ä½“é¤å…åç§°",
                "correct_answer": "éœ€è¦çŸ¥é“å…·ä½“é¤å…åç§°"
            },
            {
                "id": 3,
                "type": "æ¨¡ç³Šè¡¨è¿°å‹",
                "question": "å¥¹ç°åœ¨ä½åœ¨å“ªé‡Œï¼Ÿ",
                "context": "",
                "expected_behavior": "æé—®æŒ‡ä»£çš„å…·ä½“äººå‘˜",
                "correct_answer": "éœ€è¦æ˜ç¡®æ˜¯å“ªä½å¥³æ€§"
            },
            {
                "id": 4,
                "type": "æ­§ä¹‰å‹",
                "question": "è¿™ä¸ªä¼šè®®ä»€ä¹ˆæ—¶å€™å¼€å§‹ï¼Ÿ",
                "context": "",
                "expected_behavior": "æé—®å…·ä½“ä¼šè®®",
                "correct_answer": "éœ€è¦çŸ¥é“æ˜¯å“ªä¸ªä¼šè®®"
            },
            {
                "id": 5,
                "type": "ä¸Šä¸‹æ–‡ä¸è¶³å‹",
                "question": "ç¥¨ä»·æ˜¯å¤šå°‘ï¼Ÿ",
                "context": "",
                "expected_behavior": "æé—®ä»€ä¹ˆç¥¨/å“ªé‡Œçš„ç¥¨",
                "correct_answer": "éœ€è¦çŸ¥é“å…·ä½“ç¥¨åŠ¡ä¿¡æ¯"
            },
            {
                "id": 6,
                "type": "æ¨¡ç³Šè¡¨è¿°å‹",
                "question": "é‚£ä¸ªé¡¹ç›®è¿›å±•å¦‚ä½•ï¼Ÿ",
                "context": "",
                "expected_behavior": "æé—®å…·ä½“é¡¹ç›®åç§°",
                "correct_answer": "éœ€è¦æ˜ç¡®æ˜¯å“ªä¸ªé¡¹ç›®"
            },
            {
                "id": 7,
                "type": "æ­§ä¹‰å‹",
                "question": "å®ƒçš„é‡é‡æ˜¯å¤šå°‘ï¼Ÿ",
                "context": "",
                "expected_behavior": "æé—®æŒ‡ä»£ç‰©å“",
                "correct_answer": "éœ€è¦çŸ¥é“æŒ‡çš„æ˜¯ä»€ä¹ˆç‰©å“"
            },
            {
                "id": 8,
                "type": "ä¸Šä¸‹æ–‡ä¸è¶³å‹",
                "question": "å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
                "context": "",
                "expected_behavior": "æé—®å…·ä½“æ—¶é—´åœ°ç‚¹",
                "correct_answer": "éœ€è¦çŸ¥é“å“ªé‡Œã€ä»€ä¹ˆæ—¶å€™çš„å¤©æ°”"
            }
        ]
    
    def _create_user_response_mapping(self) -> Dict[int, str]:
        """åˆ›å»ºç”¨æˆ·æ¾„æ¸…å›ç­”æ˜ å°„"""
        return {
            1: "æˆ‘æ˜¯æŒ‡çˆ±å› æ–¯å¦ï¼Œé˜¿å°”ä¼¯ç‰¹Â·çˆ±å› æ–¯å¦ã€‚",
            2: "æˆ‘è¯´çš„æ˜¯åŒ—äº¬ä¸‰é‡Œå±¯çš„æµ·åº•æç«é”…åº—ã€‚",
            3: "æˆ‘æ˜¯æŒ‡æˆ‘çš„åŒäº‹æå°æ˜çš„å¦»å­ç‹å°çº¢ã€‚",
            4: "æˆ‘æ˜¯æŒ‡ä¸‹å‘¨ä¸€çš„é¡¹ç›®è¯„å®¡ä¼šè®®ã€‚",
            5: "æˆ‘æƒ³é—®çš„æ˜¯ä»åŒ—äº¬åˆ°ä¸Šæµ·çš„é«˜é“ç¥¨ä»·ã€‚",
            6: "æˆ‘è¯´çš„æ˜¯æˆ‘ä»¬å…¬å¸çš„AIèŠå¤©æœºå™¨äººé¡¹ç›®ã€‚",
            7: "æˆ‘æ˜¯æŒ‡åˆšä¹°çš„æ–°æ¬¾MacBook Proã€‚",
            8: "æˆ‘æƒ³çŸ¥é“æ˜å¤©åŒ—äº¬çš„å¤©æ°”æƒ…å†µã€‚"
        }
    
    def load_model(self) -> bool:
        """åŠ è½½Qwen3-4B-Thinkingæ¨¡å‹"""
        try:
            model_name = self.config.get("model.name", "Qwen/Qwen3-4B-Thinking-2507")
            self.logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
            
            # åŠ è½½åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
                padding_side="left"
            )
            
            # è®¾ç½®pad token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                device_map="auto" if torch.cuda.is_available() else "cpu",
                trust_remote_code=True,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
            )
            
            self.logger.info("æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def create_prompts(self, question: str, mode: str = "with_question") -> str:
        """
        åˆ›å»ºä¸åŒæ¨¡å¼çš„æç¤ºè¯
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            mode: "with_question" æˆ– "direct_answer"
        """
        if mode == "with_question":
            # å¸¦ä¸»åŠ¨æé—®æœºåˆ¶çš„prompt
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªèªæ˜çš„AIåŠ©æ‰‹ã€‚å½“ä½ å‘ç°é—®é¢˜ä¿¡æ¯ä¸å®Œæ•´ã€å­˜åœ¨æ­§ä¹‰æˆ–æ— æ³•å‡†ç¡®å›ç­”æ—¶ï¼Œåº”è¯¥ä¸»åŠ¨å‘ç”¨æˆ·æé—®æ¾„æ¸…ï¼Œè€Œä¸æ˜¯çŒœæµ‹æˆ–è‡†æ–­ã€‚

å¦‚æœä½ éœ€è¦æ›´å¤šä¿¡æ¯æ‰èƒ½å‡†ç¡®å›ç­”ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š
<QUESTION>ä½ çš„æ¾„æ¸…é—®é¢˜</QUESTION>

å¦‚æœä¿¡æ¯å……è¶³å¯ä»¥ç›´æ¥å›ç­”ï¼Œå°±æ­£å¸¸å›ç­”å³å¯ã€‚"""
        else:
            # ç›´æ¥å›ç­”æ¨¡å¼çš„prompt  
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å°½åŠ›ç»™å‡ºå›ç­”ã€‚"""
        
        prompt = f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant
"""
        return prompt
    
    def generate_response(self, prompt: str) -> str:
        """ç”Ÿæˆæ¨¡å‹å›ç­”"""
        if not self.model or not self.tokenizer:
            return "æ¨¡å‹æœªåŠ è½½"
        
        try:
            # åˆ†è¯
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=1024
            )
            
            # ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
            if torch.cuda.is_available():
                inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=256,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç å›ç­”ï¼ˆå»é™¤è¾“å…¥éƒ¨åˆ†ï¼‰
            input_length = inputs["input_ids"].shape[1]
            response_tokens = outputs[0][input_length:]
            response = self.tokenizer.decode(response_tokens, skip_special_tokens=True)
            
            return response.strip()
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
            return f"ç”Ÿæˆå¤±è´¥: {e}"
    
    def extract_question(self, response: str) -> Tuple[bool, str]:
        """
        ä»å›ç­”ä¸­æå–æé—®
        
        Returns:
            (æ˜¯å¦åŒ…å«æé—®, æå–çš„é—®é¢˜å†…å®¹)
        """
        if "<QUESTION>" in response and "</QUESTION>" in response:
            start = response.find("<QUESTION>") + len("<QUESTION>")
            end = response.find("</QUESTION>")
            question = response[start:end].strip()
            return True, question
        return False, ""
    
    def simulate_user_clarification(self, case_id: int, model_question: str) -> str:
        """æ¨¡æ‹Ÿç”¨æˆ·æ¾„æ¸…å›ç­”"""
        if case_id in self.user_responses:
            return self.user_responses[case_id]
        else:
            return "æˆ‘ä¸ç¡®å®šä½ åœ¨é—®ä»€ä¹ˆã€‚"
    
    def run_single_test(self, test_case: Dict, mode: str) -> Dict:
        """è¿è¡Œå•ä¸ªæµ‹è¯•æ¡ˆä¾‹"""
        case_id = test_case["id"]
        question = test_case["question"]
        
        # åˆ›å»ºæç¤ºè¯
        prompt = self.create_prompts(question, mode)
        
        # ç”Ÿæˆç¬¬ä¸€è½®å›ç­”
        response = self.generate_response(prompt)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æé—®
        has_question, extracted_question = self.extract_question(response)
        
        result = {
            "case_id": case_id,
            "mode": mode,
            "original_question": question,
            "first_response": response,
            "has_question": has_question,
            "extracted_question": extracted_question,
            "final_answer": "",
            "success": False,
            "conversation_turns": 1
        }
        
        if mode == "with_question" and has_question:
            # æ¨¡æ‹Ÿç”¨æˆ·æ¾„æ¸…
            user_clarification = self.simulate_user_clarification(case_id, extracted_question)
            
            # åˆ›å»ºç¬¬äºŒè½®å¯¹è¯
            second_prompt = f"""<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªèªæ˜çš„AIåŠ©æ‰‹ã€‚<|im_end|>
<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant
<QUESTION>{extracted_question}</QUESTION><|im_end|>
<|im_start|>user
{user_clarification}<|im_end|>
<|im_start|>assistant
"""
            
            # ç”Ÿæˆæœ€ç»ˆå›ç­”
            final_response = self.generate_response(second_prompt)
            result["final_answer"] = final_response
            result["user_clarification"] = user_clarification
            result["conversation_turns"] = 2
            
            # ç®€å•åˆ¤æ–­æˆåŠŸï¼ˆåŒ…å«å…·ä½“ä¿¡æ¯ï¼‰
            result["success"] = len(final_response) > 10 and "ä¸ç¡®å®š" not in final_response
        
        elif mode == "direct_answer":
            result["final_answer"] = response
            # ç›´æ¥å›ç­”æ¨¡å¼çš„æˆåŠŸåˆ¤æ–­ï¼ˆè¾ƒå®½æ¾ï¼‰
            result["success"] = len(response) > 5
        
        return result
    
    def run_comparison_experiment(self) -> Dict:
        """è¿è¡Œå¯¹æ¯”å®éªŒ"""
        self.logger.info("å¼€å§‹è¿è¡Œå¯¹æ¯”å®éªŒ...")
        
        results = {
            "with_question_results": [],
            "direct_answer_results": [],
            "summary": {}
        }
        
        # æµ‹è¯•ä¸»åŠ¨æé—®æ¨¡å¼
        self.logger.info("æµ‹è¯•ä¸»åŠ¨æé—®æ¨¡å¼...")
        for test_case in self.test_cases:
            result = self.run_single_test(test_case, "with_question")
            results["with_question_results"].append(result)
            
            self.logger.info(f"æ¡ˆä¾‹ {test_case['id']}: {'æé—®' if result['has_question'] else 'ç›´ç­”'}")
        
        # æµ‹è¯•ç›´æ¥å›ç­”æ¨¡å¼
        self.logger.info("æµ‹è¯•ç›´æ¥å›ç­”æ¨¡å¼...")
        for test_case in self.test_cases:
            result = self.run_single_test(test_case, "direct_answer")
            results["direct_answer_results"].append(result)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        results["summary"] = self._calculate_summary(results)
        
        return results
    
    def _calculate_summary(self, results: Dict) -> Dict:
        """è®¡ç®—å®éªŒæ€»ç»“ç»Ÿè®¡"""
        with_q = results["with_question_results"]
        direct = results["direct_answer_results"]
        
        summary = {
            "æ€»æµ‹è¯•æ¡ˆä¾‹æ•°": len(self.test_cases),
            "ä¸»åŠ¨æé—®æ¨¡å¼": {
                "æé—®ç‡": sum(1 for r in with_q if r["has_question"]) / len(with_q),
                "æˆåŠŸç‡": sum(1 for r in with_q if r["success"]) / len(with_q),
                "å¹³å‡å¯¹è¯è½®æ¬¡": sum(r["conversation_turns"] for r in with_q) / len(with_q)
            },
            "ç›´æ¥å›ç­”æ¨¡å¼": {
                "æˆåŠŸç‡": sum(1 for r in direct if r["success"]) / len(direct),
                "å¹³å‡å¯¹è¯è½®æ¬¡": 1.0
            }
        }
        
        # è®¡ç®—æå‡
        success_improvement = (summary["ä¸»åŠ¨æé—®æ¨¡å¼"]["æˆåŠŸç‡"] - 
                             summary["ç›´æ¥å›ç­”æ¨¡å¼"]["æˆåŠŸç‡"])
        summary["æˆåŠŸç‡æå‡"] = success_improvement
        
        return summary
    
    def save_results(self, results: Dict, output_file: str = "stage1_results.json"):
        """ä¿å­˜å®éªŒç»“æœ"""
        output_path = Path(output_file)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"å®éªŒç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    def print_summary_report(self, results: Dict):
        """æ‰“å°æ€»ç»“æŠ¥å‘Š"""
        summary = results["summary"]
        
        print("\n" + "="*50)
        print("ç¬¬ä¸€é˜¶æ®µå®éªŒç»“æœæ€»ç»“")
        print("="*50)
        
        print(f"ğŸ“Š æµ‹è¯•æ¡ˆä¾‹æ•°: {summary['æ€»æµ‹è¯•æ¡ˆä¾‹æ•°']}")
        print()
        
        print("ğŸ¤– ä¸»åŠ¨æé—®æ¨¡å¼:")
        print(f"  â”œâ”€ æé—®ç‡: {summary['ä¸»åŠ¨æé—®æ¨¡å¼']['æé—®ç‡']:.1%}")
        print(f"  â”œâ”€ æˆåŠŸç‡: {summary['ä¸»åŠ¨æé—®æ¨¡å¼']['æˆåŠŸç‡']:.1%}")
        print(f"  â””â”€ å¹³å‡è½®æ¬¡: {summary['ä¸»åŠ¨æé—®æ¨¡å¼']['å¹³å‡å¯¹è¯è½®æ¬¡']:.1f}")
        print()
        
        print("ğŸ“ ç›´æ¥å›ç­”æ¨¡å¼:")
        print(f"  â”œâ”€ æˆåŠŸç‡: {summary['ç›´æ¥å›ç­”æ¨¡å¼']['æˆåŠŸç‡']:.1%}")
        print(f"  â””â”€ å¹³å‡è½®æ¬¡: {summary['ç›´æ¥å›ç­”æ¨¡å¼']['å¹³å‡å¯¹è¯è½®æ¬¡']:.1f}")
        print()
        
        improvement = summary['æˆåŠŸç‡æå‡']
        if improvement > 0:
            print(f"âœ… æˆåŠŸç‡æå‡: +{improvement:.1%}")
            print("ğŸ‰ ä¸»åŠ¨æé—®æœºåˆ¶æ˜¾ç¤ºæ­£é¢æ•ˆæœï¼")
        else:
            print(f"âŒ æˆåŠŸç‡å˜åŒ–: {improvement:.1%}")
            print("âš ï¸ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–æé—®æœºåˆ¶")
        
        print("="*50)


def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹ç¬¬ä¸€é˜¶æ®µæ ¸å¿ƒæ¦‚å¿µéªŒè¯å®éªŒ")
    print("="*50)
    
    # åˆå§‹åŒ–æµ‹è¯•å™¨
    tester = Stage1Tester()
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ”„ æ­£åœ¨åŠ è½½Qwen3-4B-Thinkingæ¨¡å‹...")
    if not tester.load_model():
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
        return
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # è¿è¡Œå¯¹æ¯”å®éªŒ
    print("\nğŸ§ª å¼€å§‹è¿è¡ŒMVPå¯¹æ¯”å®éªŒ...")
    results = tester.run_comparison_experiment()
    
    # ä¿å­˜ç»“æœ
    tester.save_results(results)
    
    # æ‰“å°æŠ¥å‘Š
    tester.print_summary_report(results)
    
    print("\nğŸ¯ ç¬¬ä¸€é˜¶æ®µå®éªŒå®Œæˆï¼")
    print("ğŸ“‹ ä¸‹ä¸€æ­¥: æ ¹æ®ç»“æœåˆ†æå¹¶å‡†å¤‡ç¬¬äºŒé˜¶æ®µå¼€å‘")


if __name__ == "__main__":
    main()
