#!/usr/bin/env python3
"""
ç¬¬ä¸€é˜¶æ®µä¼˜åŒ–ç‰ˆæœ¬ï¼šåŸºäºGPT5æ¶æ„å¸ˆæŒ‡å¯¼çš„æ”¹è¿›
ä¸»è¦æ”¹è¿›ï¼š
1. å¯ç”¨MPSåŠ é€Ÿ
2. ä½¿ç”¨Few-shotå­¦ä¹ å’Œè‡ªç„¶è¯­è¨€æé—®
3. æ”¹è¿›promptè®¾è®¡
4. æ›´çµæ´»çš„æé—®æ£€æµ‹æœºåˆ¶
"""

import sys
import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
import re
from typing import Dict, List, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.utils.config import get_config
from src.utils.logging import get_logger


class OptimizedStage1Tester:
    """ä¼˜åŒ–çš„ç¬¬ä¸€é˜¶æ®µæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.config = get_config()
        self.logger = get_logger("stage1_optimized")
        self.tokenizer = None
        self.model = None
        
        # è®¾ç½®è®¾å¤‡ï¼ˆä¼˜å…ˆä½¿ç”¨MPSï¼‰
        self.device = self._setup_device()
        
        # MVPæµ‹è¯•æ•°æ®
        self.test_cases = self._create_optimized_test_cases()
        self.user_responses = self._create_user_response_mapping()
        
        self.logger.info("ä¼˜åŒ–çš„ç¬¬ä¸€é˜¶æ®µæµ‹è¯•å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_device(self) -> torch.device:
        """è®¾ç½®æœ€ä¼˜è®¾å¤‡ï¼ˆæŒ‰GPT5å»ºè®®ä¼˜å…ˆä½¿ç”¨MPSï¼‰"""
        if torch.backends.mps.is_available():
            device = torch.device("mps")
            self.logger.info("ä½¿ç”¨Apple Silicon MPSåŠ é€Ÿ")
        elif torch.cuda.is_available():
            device = torch.device("cuda")
            self.logger.info("ä½¿ç”¨CUDA GPU")
        else:
            device = torch.device("cpu")
            self.logger.info("ä½¿ç”¨CPUï¼ˆæ€§èƒ½å¯èƒ½è¾ƒæ…¢ï¼‰")
        
        return device
    
    def _create_optimized_test_cases(self) -> List[Dict]:
        """åˆ›å»ºä¼˜åŒ–çš„æµ‹è¯•æ¡ˆä¾‹ï¼ˆåŒ…å«æ§åˆ¶æ¡ˆä¾‹ï¼‰"""
        # éœ€è¦æé—®çš„æ¡ˆä¾‹
        ambiguous_cases = [
            {
                "id": 1,
                "type": "ä»£è¯æ­§ä¹‰",
                "question": "ä»–æ˜¯å“ªå¹´å»ä¸–çš„ï¼Ÿ",
                "should_ask": True,
                "expected_question_topic": "å…·ä½“æŒ‡ä»£äººç‰©",
                "complete_info": "çˆ±å› æ–¯å¦æ˜¯1955å¹´å»ä¸–çš„"
            },
            {
                "id": 2,
                "type": "æŒ‡ä»£ä¸æ¸…",
                "question": "é‚£å®¶é¤å…çš„è¥ä¸šæ—¶é—´æ˜¯ä»€ä¹ˆï¼Ÿ",
                "should_ask": True,
                "expected_question_topic": "é¤å…åç§°",
                "complete_info": "æµ·åº•æç«é”…åº—çš„è¥ä¸šæ—¶é—´æ˜¯11:00-22:00"
            },
            {
                "id": 3,
                "type": "ç¼ºå°‘å‚æ•°",
                "question": "è®¢ä¸€å¼ ç¥¨",
                "should_ask": True,
                "expected_question_topic": "ç¥¨çš„ç±»å‹/ç›®çš„åœ°",
                "complete_info": "åŒ—äº¬åˆ°ä¸Šæµ·çš„é«˜é“ç¥¨"
            },
            {
                "id": 4,
                "type": "ä¸Šä¸‹æ–‡ç¼ºå¤±",
                "question": "è¿™ä¸ªä¼šè®®ä»€ä¹ˆæ—¶å€™å¼€å§‹ï¼Ÿ",
                "should_ask": True,
                "expected_question_topic": "å…·ä½“ä¼šè®®",
                "complete_info": "é¡¹ç›®è¯„å®¡ä¼šè®®ä¸‹å‘¨ä¸€ä¸Šåˆ9ç‚¹å¼€å§‹"
            },
            {
                "id": 5,
                "type": "æ¨¡ç³ŠæŒ‡ä»£",
                "question": "å¥¹ç°åœ¨ä½åœ¨å“ªé‡Œï¼Ÿ",
                "should_ask": True,
                "expected_question_topic": "å…·ä½“äººå‘˜",
                "complete_info": "ç‹å°çº¢ç°åœ¨ä½åœ¨åŒ—äº¬æœé˜³åŒº"
            }
        ]
        
        # æ— éœ€æé—®çš„æ§åˆ¶æ¡ˆä¾‹
        control_cases = [
            {
                "id": 6,
                "type": "å®Œæ•´é—®é¢˜",
                "question": "æ³•å›½çš„é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ",
                "should_ask": False,
                "expected_answer": "å·´é»",
                "complete_info": "æ³•å›½çš„é¦–éƒ½æ˜¯å·´é»"
            },
            {
                "id": 7,
                "type": "æ˜ç¡®è®¡ç®—",
                "question": "15ä¹˜ä»¥8ç­‰äºå¤šå°‘ï¼Ÿ",
                "should_ask": False,
                "expected_answer": "120",
                "complete_info": "15ä¹˜ä»¥8ç­‰äº120"
            },
            {
                "id": 8,
                "type": "å…·ä½“æŸ¥è¯¢",
                "question": "åŒ—äº¬ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
                "should_ask": False,
                "expected_answer": "éœ€è¦æŸ¥è¯¢å®æ—¶å¤©æ°”",
                "complete_info": "åŒ—äº¬ä»Šå¤©å¤šäº‘ï¼Œæ°”æ¸©15-22åº¦"
            }
        ]
        
        return ambiguous_cases + control_cases
    
    def _create_user_response_mapping(self) -> Dict[int, str]:
        """åˆ›å»ºç”¨æˆ·æ¾„æ¸…å›ç­”æ˜ å°„"""
        return {
            1: "æˆ‘æ˜¯æŒ‡çˆ±å› æ–¯å¦ã€‚",
            2: "æˆ‘è¯´çš„æ˜¯åŒ—äº¬ä¸‰é‡Œå±¯çš„æµ·åº•æç«é”…åº—ã€‚",
            3: "æˆ‘æƒ³è®¢ä»åŒ—äº¬åˆ°ä¸Šæµ·çš„é«˜é“ç¥¨ã€‚",
            4: "æˆ‘é—®çš„æ˜¯ä¸‹å‘¨ä¸€çš„é¡¹ç›®è¯„å®¡ä¼šè®®ã€‚",
            5: "æˆ‘æ˜¯æŒ‡æˆ‘åŒäº‹ç‹å°çº¢ã€‚"
        }
    
    def load_model(self) -> bool:
        """åŠ è½½æ¨¡å‹å¹¶ä¼˜åŒ–è®¾ç½®"""
        try:
            model_name = self.config.get("model.name", "Qwen/Qwen3-4B-Thinking-2507")
            self.logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
            
            # åŠ è½½åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
                padding_side="left"
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½æ¨¡å‹ï¼ˆæ ¹æ®è®¾å¤‡ä¼˜åŒ–ï¼‰
            if self.device.type == "mps":
                # MPSä¼˜åŒ–è®¾ç½®
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16,
                    trust_remote_code=True
                ).to(self.device)
            else:
                # é€šç”¨è®¾ç½®
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    device_map="auto" if torch.cuda.is_available() else None,
                    trust_remote_code=True,
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
                )
            
            self.logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
            return True
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def create_few_shot_prompt(self, question: str, mode: str = "with_question") -> str:
        """
        åˆ›å»ºåŒ…å«Few-shotç¤ºä¾‹çš„æç¤ºè¯
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            mode: "with_question" æˆ– "direct_answer"
        """
        if mode == "with_question":
            # åŒ…å«Few-shotå­¦ä¹ çš„ä¸»åŠ¨æé—®æ¨¡å¼
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½AIåŠ©æ‰‹ã€‚å½“é‡åˆ°ä¿¡æ¯ä¸å®Œæ•´æˆ–æ¨¡ç³Šçš„é—®é¢˜æ—¶ï¼Œä½ åº”è¯¥ä¸»åŠ¨å‘ç”¨æˆ·æé—®æ¾„æ¸…ï¼Œè€Œä¸æ˜¯çŒœæµ‹ã€‚

ä»¥ä¸‹æ˜¯ä¸€äº›æ­£ç¡®å¤„ç†æ¨¡ç³Šé—®é¢˜çš„ç¤ºä¾‹ï¼š

ç¤ºä¾‹1ï¼š
ç”¨æˆ·ï¼šå®‰æ’ä¸€ä¸ªä¼šè®®
åŠ©æ‰‹ï¼šå¥½çš„ï¼è¯·é—®æ‚¨å¸Œæœ›ä»€ä¹ˆæ—¶å€™å®‰æ’ä¼šè®®ï¼Ÿå‚ä¸äººå‘˜æœ‰å“ªäº›ï¼Ÿ

ç¤ºä¾‹2ï¼š
ç”¨æˆ·ï¼šå¸®æˆ‘æŸ¥ä¸€ä¸‹ä»·æ ¼
åŠ©æ‰‹ï¼šè¯·é—®æ‚¨æƒ³æŸ¥è¯¢ä»€ä¹ˆäº§å“çš„ä»·æ ¼å‘¢ï¼Ÿ

ç°åœ¨è¯·æŒ‰ç…§åŒæ ·çš„æ–¹å¼å¤„ç†ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¿¡æ¯å……è¶³å°±ç›´æ¥å›ç­”ï¼Œå¦‚æœä¿¡æ¯ä¸è¶³å°±ä¸»åŠ¨æé—®æ¾„æ¸…ã€‚"""

        else:
            # ç›´æ¥å›ç­”æ¨¡å¼
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å°½åŠ›ç»™å‡ºå›ç­”ã€‚"""
        
        prompt = f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant
"""
        return prompt
    
    def generate_response(self, prompt: str) -> str:
        """ç”Ÿæˆæ¨¡å‹å›ç­”ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        if not self.model or not self.tokenizer:
            return "æ¨¡å‹æœªåŠ è½½"
        
        try:
            # åˆ†è¯
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=1024
            ).to(self.device)
            
            # ç”Ÿæˆå‚æ•°ä¼˜åŒ–
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=200,  # å‡å°‘é•¿åº¦ï¼Œæé«˜é€Ÿåº¦
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç å›ç­”
            input_length = inputs["input_ids"].shape[1]
            response_tokens = outputs[0][input_length:]
            response = self.tokenizer.decode(response_tokens, skip_special_tokens=True)
            
            return response.strip()
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
            return f"ç”Ÿæˆå¤±è´¥: {e}"
    
    def detect_question(self, response: str) -> Tuple[bool, str]:
        """
        æ£€æµ‹å›ç­”ä¸­æ˜¯å¦åŒ…å«æé—®ï¼ˆæ”¹è¿›çš„æ£€æµ‹é€»è¾‘ï¼‰
        
        Returns:
            (æ˜¯å¦åŒ…å«æé—®, æå–çš„é—®é¢˜å†…å®¹)
        """
        # æ–¹æ³•1: æ£€æŸ¥é—®å·
        if 'ï¼Ÿ' in response or '?' in response:
            # æå–åŒ…å«é—®å·çš„å¥å­
            sentences = re.split(r'[ã€‚ï¼!.]', response)
            for sentence in sentences:
                if 'ï¼Ÿ' in sentence or '?' in sentence:
                    return True, sentence.strip()
        
        # æ–¹æ³•2: æ£€æŸ¥æé—®å…³é”®è¯
        question_patterns = [
            r'è¯·é—®.*?[ï¼Ÿ?]',
            r'èƒ½å¦.*?[ï¼Ÿ?]',
            r'æ‚¨.*?[ï¼Ÿ?]',
            r'ä»€ä¹ˆ.*?[ï¼Ÿ?]',
            r'å“ª.*?[ï¼Ÿ?]',
            r'å¦‚ä½•.*?[ï¼Ÿ?]',
            r'æ˜¯å¦.*?[ï¼Ÿ?]'
        ]
        
        for pattern in question_patterns:
            match = re.search(pattern, response)
            if match:
                return True, match.group(0)
        
        # æ–¹æ³•3: æ£€æŸ¥åŸæœ‰çš„<QUESTION>æ ‡ç­¾ï¼ˆå…¼å®¹æ€§ï¼‰
        if "<QUESTION>" in response and "</QUESTION>" in response:
            start = response.find("<QUESTION>") + len("<QUESTION>")
            end = response.find("</QUESTION>")
            question = response[start:end].strip()
            return True, question
        
        return False, ""
    
    def simulate_user_clarification(self, case_id: int, model_question: str) -> str:
        """æ¨¡æ‹Ÿç”¨æˆ·æ¾„æ¸…å›ç­”ï¼ˆç¡®ä¿åä½œæ€§ï¼‰"""
        if case_id in self.user_responses:
            return self.user_responses[case_id]
        else:
            return "è¯·æ‚¨å…·ä½“è¯´æ˜ä¸€ä¸‹ã€‚"
    
    def run_single_test(self, test_case: Dict, mode: str) -> Dict:
        """è¿è¡Œå•ä¸ªæµ‹è¯•æ¡ˆä¾‹ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        case_id = test_case["id"]
        question = test_case["question"]
        should_ask = test_case.get("should_ask", False)
        
        # åˆ›å»ºFew-shotæç¤ºè¯
        prompt = self.create_few_shot_prompt(question, mode)
        
        # ç”Ÿæˆç¬¬ä¸€è½®å›ç­”
        response = self.generate_response(prompt)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æé—®
        has_question, extracted_question = self.detect_question(response)
        
        result = {
            "case_id": case_id,
            "mode": mode,
            "original_question": question,
            "should_ask": should_ask,
            "first_response": response,
            "has_question": has_question,
            "extracted_question": extracted_question,
            "final_answer": "",
            "success": False,
            "conversation_turns": 1,
            "correct_behavior": False
        }
        
        # è¯„ä¼°è¡Œä¸ºæ­£ç¡®æ€§
        if should_ask:
            # åº”è¯¥æé—®çš„æ¡ˆä¾‹
            result["correct_behavior"] = has_question
            
            if has_question:
                # æ¨¡æ‹Ÿç”¨æˆ·æ¾„æ¸…
                user_clarification = self.simulate_user_clarification(case_id, extracted_question)
                
                # åˆ›å»ºç¬¬äºŒè½®å¯¹è¯
                second_prompt = f"""{prompt.rstrip()}{response}<|im_end|>
<|im_start|>user
{user_clarification}<|im_end|>
<|im_start|>assistant
"""
                
                # ç”Ÿæˆæœ€ç»ˆå›ç­”
                final_response = self.generate_response(second_prompt)
                result["final_answer"] = final_response
                result["user_clarification"] = user_clarification
                result["conversation_turns"] = 2
                
                # åˆ¤æ–­æœ€ç»ˆæˆåŠŸï¼ˆåŒ…å«å…·ä½“ä¿¡æ¯ä¸”åˆç†ï¼‰
                result["success"] = len(final_response) > 10 and "ä¸ç¡®å®š" not in final_response
        else:
            # ä¸åº”è¯¥æé—®çš„æ¡ˆä¾‹
            result["correct_behavior"] = not has_question
            result["final_answer"] = response
            result["success"] = len(response) > 5 and not has_question
        
        return result
    
    def run_comparison_experiment(self) -> Dict:
        """è¿è¡Œä¼˜åŒ–çš„å¯¹æ¯”å®éªŒ"""
        self.logger.info("å¼€å§‹è¿è¡Œä¼˜åŒ–çš„å¯¹æ¯”å®éªŒ...")
        
        results = {
            "with_question_results": [],
            "direct_answer_results": [],
            "summary": {}
        }
        
        # æµ‹è¯•ä¸»åŠ¨æé—®æ¨¡å¼
        self.logger.info("æµ‹è¯•ä¸»åŠ¨æé—®æ¨¡å¼ï¼ˆåŒ…å«Few-shotå­¦ä¹ ï¼‰...")
        for test_case in self.test_cases:
            result = self.run_single_test(test_case, "with_question")
            results["with_question_results"].append(result)
            
            behavior = "æ­£ç¡®" if result["correct_behavior"] else "é”™è¯¯"
            action = "æé—®" if result["has_question"] else "ç›´ç­”"
            self.logger.info(f"æ¡ˆä¾‹ {test_case['id']}: {action} - è¡Œä¸º{behavior}")
        
        # æµ‹è¯•ç›´æ¥å›ç­”æ¨¡å¼
        self.logger.info("æµ‹è¯•ç›´æ¥å›ç­”æ¨¡å¼...")
        for test_case in self.test_cases:
            result = self.run_single_test(test_case, "direct_answer")
            results["direct_answer_results"].append(result)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        results["summary"] = self._calculate_optimized_summary(results)
        
        return results
    
    def _calculate_optimized_summary(self, results: Dict) -> Dict:
        """è®¡ç®—ä¼˜åŒ–å®éªŒçš„æ€»ç»“ç»Ÿè®¡"""
        with_q = results["with_question_results"]
        direct = results["direct_answer_results"]
        
        # æŒ‰æ˜¯å¦åº”è¯¥æé—®åˆ†ç»„
        should_ask_cases = [r for r in with_q if r["should_ask"]]
        should_not_ask_cases = [r for r in with_q if not r["should_ask"]]
        
        summary = {
            "æ€»æµ‹è¯•æ¡ˆä¾‹æ•°": len(self.test_cases),
            "åº”è¯¥æé—®çš„æ¡ˆä¾‹æ•°": len(should_ask_cases),
            "ä¸åº”è¯¥æé—®çš„æ¡ˆä¾‹æ•°": len(should_not_ask_cases),
            
            "ä¸»åŠ¨æé—®æ¨¡å¼": {
                # æ•´ä½“ç»Ÿè®¡
                "æ€»ä½“è¡Œä¸ºæ­£ç¡®ç‡": sum(1 for r in with_q if r["correct_behavior"]) / len(with_q),
                "æ€»ä½“æˆåŠŸç‡": sum(1 for r in with_q if r["success"]) / len(with_q),
                
                # åº”è¯¥æé—®çš„æ¡ˆä¾‹
                "éœ€è¦æé—®-å®é™…æé—®ç‡": sum(1 for r in should_ask_cases if r["has_question"]) / len(should_ask_cases) if should_ask_cases else 0,
                "éœ€è¦æé—®-æœ€ç»ˆæˆåŠŸç‡": sum(1 for r in should_ask_cases if r["success"]) / len(should_ask_cases) if should_ask_cases else 0,
                
                # ä¸åº”è¯¥æé—®çš„æ¡ˆä¾‹
                "æ— éœ€æé—®-å®é™…æœªæé—®ç‡": sum(1 for r in should_not_ask_cases if not r["has_question"]) / len(should_not_ask_cases) if should_not_ask_cases else 0,
                
                "å¹³å‡å¯¹è¯è½®æ¬¡": sum(r["conversation_turns"] for r in with_q) / len(with_q)
            },
            
            "ç›´æ¥å›ç­”æ¨¡å¼": {
                "æˆåŠŸç‡": sum(1 for r in direct if r["success"]) / len(direct),
                "å¹³å‡å¯¹è¯è½®æ¬¡": 1.0
            }
        }
        
        # è®¡ç®—æå‡
        success_improvement = (summary["ä¸»åŠ¨æé—®æ¨¡å¼"]["æ€»ä½“æˆåŠŸç‡"] - 
                             summary["ç›´æ¥å›ç­”æ¨¡å¼"]["æˆåŠŸç‡"])
        summary["æˆåŠŸç‡æå‡"] = success_improvement
        
        return summary
    
    def print_optimized_report(self, results: Dict):
        """æ‰“å°ä¼˜åŒ–å®éªŒæŠ¥å‘Š"""
        summary = results["summary"]
        
        print("\n" + "="*60)
        print("ç¬¬ä¸€é˜¶æ®µä¼˜åŒ–å®éªŒç»“æœæ€»ç»“")
        print("="*60)
        
        print(f"ğŸ“Š æµ‹è¯•æ¡ˆä¾‹: {summary['æ€»æµ‹è¯•æ¡ˆä¾‹æ•°']} (éœ€è¦æé—®: {summary['åº”è¯¥æé—®çš„æ¡ˆä¾‹æ•°']}, æ— éœ€æé—®: {summary['ä¸åº”è¯¥æé—®çš„æ¡ˆä¾‹æ•°']})")
        print()
        
        print("ğŸ¤– ä¸»åŠ¨æé—®æ¨¡å¼ï¼ˆFew-shotä¼˜åŒ–ï¼‰:")
        print(f"  â”œâ”€ æ€»ä½“è¡Œä¸ºæ­£ç¡®ç‡: {summary['ä¸»åŠ¨æé—®æ¨¡å¼']['æ€»ä½“è¡Œä¸ºæ­£ç¡®ç‡']:.1%}")
        print(f"  â”œâ”€ æ€»ä½“æˆåŠŸç‡: {summary['ä¸»åŠ¨æé—®æ¨¡å¼']['æ€»ä½“æˆåŠŸç‡']:.1%}")
        print(f"  â”œâ”€ éœ€è¦æé—®-å®é™…æé—®ç‡: {summary['ä¸»åŠ¨æé—®æ¨¡å¼']['éœ€è¦æé—®-å®é™…æé—®ç‡']:.1%}")
        print(f"  â”œâ”€ éœ€è¦æé—®-æœ€ç»ˆæˆåŠŸç‡: {summary['ä¸»åŠ¨æé—®æ¨¡å¼']['éœ€è¦æé—®-æœ€ç»ˆæˆåŠŸç‡']:.1%}")
        print(f"  â”œâ”€ æ— éœ€æé—®-å®é™…æœªæé—®ç‡: {summary['ä¸»åŠ¨æé—®æ¨¡å¼']['æ— éœ€æé—®-å®é™…æœªæé—®ç‡']:.1%}")
        print(f"  â””â”€ å¹³å‡è½®æ¬¡: {summary['ä¸»åŠ¨æé—®æ¨¡å¼']['å¹³å‡å¯¹è¯è½®æ¬¡']:.1f}")
        print()
        
        print("ğŸ“ ç›´æ¥å›ç­”æ¨¡å¼:")
        print(f"  â”œâ”€ æˆåŠŸç‡: {summary['ç›´æ¥å›ç­”æ¨¡å¼']['æˆåŠŸç‡']:.1%}")
        print(f"  â””â”€ å¹³å‡è½®æ¬¡: {summary['ç›´æ¥å›ç­”æ¨¡å¼']['å¹³å‡å¯¹è¯è½®æ¬¡']:.1f}")
        print()
        
        improvement = summary['æˆåŠŸç‡æå‡']
        if improvement > 0:
            print(f"âœ… æˆåŠŸç‡æå‡: +{improvement:.1%}")
            print("ğŸ‰ ä¸»åŠ¨æé—®æœºåˆ¶æ˜¾ç¤ºæ­£é¢æ•ˆæœï¼")
        else:
            print(f"âŒ æˆåŠŸç‡å˜åŒ–: {improvement:.1%}")
            
        behavior_rate = summary['ä¸»åŠ¨æé—®æ¨¡å¼']['æ€»ä½“è¡Œä¸ºæ­£ç¡®ç‡']
        if behavior_rate > 0.5:
            print("âœ… æ¨¡å‹è¡Œä¸ºæ­£ç¡®ç‡è‰¯å¥½ï¼")
        else:
            print("âš ï¸ æ¨¡å‹è¡Œä¸ºéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        
        print("="*60)
    
    def save_results(self, results: Dict, output_file: str = "stage1_optimized_results.json"):
        """ä¿å­˜ä¼˜åŒ–å®éªŒç»“æœ"""
        output_path = Path(output_file)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ä¼˜åŒ–å®éªŒç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹ç¬¬ä¸€é˜¶æ®µä¼˜åŒ–å®éªŒ")
    print("åŸºäºGPT5æ¶æ„å¸ˆæŒ‡å¯¼çš„æ”¹è¿›æ–¹æ¡ˆ")
    print("="*60)
    
    # åˆå§‹åŒ–ä¼˜åŒ–æµ‹è¯•å™¨
    tester = OptimizedStage1Tester()
    
    # åŠ è½½æ¨¡å‹ï¼ˆå¯ç”¨MPSåŠ é€Ÿï¼‰
    print("ğŸ”„ æ­£åœ¨åŠ è½½Qwen3-4B-Thinkingæ¨¡å‹ï¼ˆå¯ç”¨åŠ é€Ÿï¼‰...")
    if not tester.load_model():
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
        return
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå¼€å§‹æ€§èƒ½æµ‹è¯•")
    
    # è¿è¡Œä¼˜åŒ–çš„å¯¹æ¯”å®éªŒ
    print("\nğŸ§ª å¼€å§‹è¿è¡Œä¼˜åŒ–çš„MVPå®éªŒ...")
    print("æ”¹è¿›é¡¹: Few-shotå­¦ä¹  + è‡ªç„¶è¯­è¨€æé—® + MPSåŠ é€Ÿ")
    
    results = tester.run_comparison_experiment()
    
    # ä¿å­˜å’ŒæŠ¥å‘Šç»“æœ
    tester.save_results(results)
    tester.print_optimized_report(results)
    
    print("\nğŸ¯ ç¬¬ä¸€é˜¶æ®µä¼˜åŒ–å®éªŒå®Œæˆï¼")
    print("ğŸ“‹ å‡†å¤‡å‘äº§å“ç»ç†å’ŒGPT5æ¶æ„å¸ˆæ±‡æŠ¥ç»“æœ")


if __name__ == "__main__":
    main()
