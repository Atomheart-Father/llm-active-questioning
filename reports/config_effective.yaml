callbacks:
- logging
- checkpoint
- eval_shadow
data:
  batch_size: 1
  eval_path: data/eval_min.jsonl
  max_length: 1024
  padding_side: right
  train_path: data/train_min.jsonl
engine:
  clip_coef: 0.2
  lr: 1.0e-05
  max_grad_norm: 1.0
  name: trl_ppo
  target_kl: 0.03
model:
  base: Qwen/Qwen3-4B-Thinking-2507
  gradient_checkpointing: true
  lora:
    alpha: 16
    dropout: 0.05
    enable: true
    r: 8
    target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
  trust_remote_code: true
  use_cache: false
  use_flash_attention_2: false
run:
  device_pref:
  - mps
  - cpu
  eval_steps: 1000
  gradient_accumulation: 2
  log_file: logs/train.log
  max_steps: 2000
  mixed_precision: fp16
  output_dir: checkpoints/local
  save_steps: 500
  seed: 20250823
strategy:
  name: ppo
