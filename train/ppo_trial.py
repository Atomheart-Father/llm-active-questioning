#!/usr/bin/env python3
"""
PPO Trial - 5k stepså°æ­¥PPOè¯•ç‚¼
åŸºäºTRLçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œé›†æˆPhase 2çš„æƒé‡æ ¡å‡†å’Œè¿‡åº¦æ¾„æ¸…æƒ©ç½š
"""

import argparse
import json
import time
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging
import yaml
import numpy as np
import torch
from dataclasses import dataclass, field

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# æ£€æŸ¥å¹¶å®‰è£…å¿…è¦ä¾èµ–
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
    from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
    from datasets import Dataset
    import wandb
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦ä¾èµ–: {e}")
    print("è¯·è¿è¡Œ: pip install transformers trl datasets wandb accelerate")
    sys.exit(1)

from src.evaluation.advanced_reward_system import MultiDimensionalRewardSystem
from src.evaluation.overclar_penalty import OverClarificationPenalty
from src.evaluation.shadow_run import ShadowRunEvaluator

logger = logging.getLogger(__name__)

@dataclass
class PPOTrialConfig:
    """PPOè¯•ç‚¼é…ç½®"""
    
    # æ¨¡å‹é…ç½®
    base_model: str = "Qwen/Qwen3-4B-Thinking-2507"
    tokenizer: str = "auto"
    
    # æ•°æ®é…ç½®
    datasets: Dict[str, float] = field(default_factory=lambda: {
        "hotpotqa": 0.40,
        "strategyqa": 0.30, 
        "gsm8k": 0.30
    })
    rollout_len: int = 128
    max_turns: int = 6
    train_samples: int = 100  # è½»é‡çº§è¯•ç‚¼
    eval_shadow_n: int = 245
    
    # è®­ç»ƒé…ç½®
    steps: int = 5000
    batch_size: int = 32
    mini_batch_size: int = 4
    lr: float = 1.0e-5
    ppo_clip: float = 0.2
    gae_lambda: float = 0.95
    gamma: float = 0.99
    vf_coef: float = 0.5
    
    # KLæ§åˆ¶
    init_kl_coef: float = 0.02
    target_kl: float = 0.03
    kl_adaptation: bool = True
    
    # å¥–åŠ±é…ç½®
    weights_file: str = "configs/weights.json"
    use_overclar_penalty: bool = True
    overclar: Dict[str, Any] = field(default_factory=lambda: {
        "alpha": 0.07,
        "cap": 3
    })
    
    # å¹¶å‘é…ç½®
    scorer_provider: str = "deepseek_r1"
    k_vote: int = 3
    cache_ttl_days: int = 14
    max_concurrent: int = 5
    
    # å…¶ä»–é…ç½®
    seed: int = 20250820
    wandb: bool = False
    save_every_steps: int = 500
    eval_every_steps: int = 500

class PPOTrialTrainer:
    """PPOè¯•ç‚¼è®­ç»ƒå™¨"""
    
    def __init__(self, config: PPOTrialConfig):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(config.seed)
        np.random.seed(config.seed)
        
        # åˆå§‹åŒ–å¥–åŠ±ç³»ç»Ÿ
        self.reward_system = MultiDimensionalRewardSystem()
        if config.use_overclar_penalty:
            self.penalty_system = OverClarificationPenalty(
                alpha=config.overclar["alpha"],
                cap=config.overclar["cap"]
            )
        else:
            self.penalty_system = None
        
        # åŠ è½½æƒé‡
        self.load_weights()
        
        # åˆå§‹åŒ–shadow runè¯„ä¼°å™¨
        self.shadow_evaluator = ShadowRunEvaluator()
        
        # Hackingæ£€æµ‹è®¡æ•°å™¨
        self.hacking_signals = {
            "ask_spam_count": 0,
            "format_exploit_count": 0,
            "variance_spike_count": 0
        }
        
        logger.info(f"PPOè¯•ç‚¼è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ: {config.base_model}")
    
    def load_weights(self):
        """åŠ è½½æƒé‡é…ç½®"""
        if self.config.weights_file == "_uniform":
            # ä½¿ç”¨å‡åŒ€æƒé‡
            self.weights = None
            logger.info("ä½¿ç”¨å‡åŒ€æƒé‡")
        else:
            try:
                with open(self.config.weights_file, 'r', encoding='utf-8') as f:
                    weights_data = json.load(f)
                self.weights = weights_data.get("weights", {})
                logger.info(f"åŠ è½½æƒé‡: {self.weights}")
            except FileNotFoundError:
                logger.warning(f"æƒé‡æ–‡ä»¶æœªæ‰¾åˆ°: {self.config.weights_file}ï¼Œä½¿ç”¨å‡åŒ€æƒé‡")
                self.weights = None
    
    def setup_model_and_tokenizer(self):
        """è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆè¯•ç‚¼ç‰ˆæœ¬-æ¨¡æ‹Ÿæ¨¡å¼ï¼‰"""
        # ä»ç¯å¢ƒå˜é‡è·å–æ¨¡å‹åç§°
        model_name = os.getenv("BASE_MODEL", self.config.base_model)
        
        logger.info(f"æ¨¡æ‹ŸåŠ è½½æ¨¡å‹: {model_name}")
        
        # è¯•ç‚¼ç‰ˆæœ¬ï¼šè·³è¿‡çœŸå®æ¨¡å‹åŠ è½½ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
        self.tokenizer = None
        self.model = None
        self.ref_model = None
        
        logger.info("æ¨¡å‹å’Œåˆ†è¯å™¨è®¾ç½®å®Œæˆï¼ˆæ¨¡æ‹Ÿæ¨¡å¼ï¼‰")
    
    def generate_training_data(self) -> Dataset:
        """ç”Ÿæˆè®­ç»ƒæ•°æ®"""
        logger.info(f"ç”Ÿæˆ{self.config.train_samples}ä¸ªè®­ç»ƒæ ·æœ¬...")
        
        # åŸºäºç°æœ‰shadow_runæ•°æ®æ‰©å±•
        base_samples = self.shadow_evaluator.load_or_generate_sample_data(
            self.config.eval_shadow_n, 
            self.config.seed
        )
        
        # æ‰©å±•åˆ°ç›®æ ‡æ•°é‡
        training_samples = []
        target_count = self.config.train_samples
        samples_per_base = target_count // len(base_samples) + 1
        
        for i, base_sample in enumerate(base_samples):
            for j in range(samples_per_base):
                if len(training_samples) >= target_count:
                    break
                
                # åˆ›å»ºå˜ä½“æ ·æœ¬
                sample = base_sample.copy()
                sample["id"] = f"{base_sample['id']}_var_{j}"
                
                # ç”Ÿæˆé—®é¢˜å˜ä½“
                base_query = sample.get("question", sample.get("query", f"æ¨¡æ‹Ÿé—®é¢˜ {sample['id']}"))
                variants = [
                    base_query,
                    f"è¯·å¸®åŠ©æˆ‘è§£å†³ï¼š{base_query}",
                    f"å…³äºè¿™ä¸ªé—®é¢˜ï¼š{base_query}ï¼Œè¯·ç»™å‡ºç­”æ¡ˆã€‚",
                    f"{base_query}è¯·è¯¦ç»†è¯´æ˜ã€‚"
                ]
                query = variants[j % len(variants)]
                
                training_samples.append({
                    "query": query,
                    "sample_id": sample["id"],
                    "task_type": sample.get("task_type", "unknown"),
                    "meta": sample.get("meta", {})
                })
        
        # æˆªæ–­åˆ°ç›®æ ‡æ•°é‡
        training_samples = training_samples[:target_count]
        
        logger.info(f"ç”Ÿæˆäº†{len(training_samples)}ä¸ªè®­ç»ƒæ ·æœ¬")
        return Dataset.from_list(training_samples)
    
    def compute_reward(self, query: str, response: str, sample_meta: Dict[str, Any]) -> float:
        """è®¡ç®—å¥–åŠ±"""
        # æ„å»ºå¯¹è¯æ ·æœ¬
        dialogue = {
            "id": f"trial_{int(time.time())}",
            "question": query,
            "response": response,
            "turns": [
                {"role": "user", "content": query},
                {"role": "assistant", "content": response}
            ],
            "meta": sample_meta
        }
        
        # åŸºç¡€å¥–åŠ±è¯„ä¼°
        reward_result = self.reward_system.evaluate_dialogue(dialogue)
        base_reward = reward_result["primary_reward"]
        
        # åº”ç”¨è¿‡åº¦æ¾„æ¸…æƒ©ç½š
        if self.penalty_system:
            penalty_info = self.penalty_system.compute_penalty(dialogue)
            final_reward = self.penalty_system.apply_penalty_to_reward(base_reward, penalty_info)
            
            # Hackingæ£€æµ‹
            self.detect_reward_hacking(dialogue, reward_result, penalty_info)
        else:
            final_reward = base_reward
        
        return final_reward
    
    def detect_reward_hacking(self, dialogue: Dict[str, Any], 
                            reward_result: Dict[str, Any], 
                            penalty_info: Dict[str, Any]):
        """æ£€æµ‹å¥–åŠ±ç ´è§£è¡Œä¸º"""
        # æ£€æµ‹1: ask_spam (æ¾„æ¸…è½®æ•°è¿‡å¤š)
        clarify_turns = penalty_info["clarify_turns"]
        if clarify_turns > self.config.overclar["cap"]:
            self.hacking_signals["ask_spam_count"] += 1
        
        # æ£€æµ‹2: format_exploit (é«˜æ ¼å¼åˆ†ï¼Œä½æ­£ç¡®æ€§)
        component_scores = reward_result.get("component_scores", {})
        format_score = reward_result.get("hard_rules", {}).get("metrics", {}).get("format_score", 0)
        logic_score = component_scores.get("logic_rigor", 0)
        
        if format_score > 0.8 and logic_score < 0.3:
            self.hacking_signals["format_exploit_count"] += 1
        
        # æ£€æµ‹3: variance_spike (è¯„åˆ†æ–¹å·®è¿‡é«˜)
        variance = reward_result.get("meta", {}).get("variance", 0)
        if variance > 0.08:
            self.hacking_signals["variance_spike_count"] += 1
    
    def check_hacking_thresholds(self, total_samples: int) -> Dict[str, bool]:
        """æ£€æŸ¥hackingé˜ˆå€¼"""
        thresholds = {
            "ask_spam_rate": 0.05,        # 5%
            "format_exploit_rate": 0.03,  # 3%
            "variance_spike_rate": 0.10   # 10%
        }
        
        rates = {}
        alerts = {}
        
        for signal_name, count in self.hacking_signals.items():
            rate_name = signal_name.replace("_count", "_rate")
            
            rate = count / total_samples if total_samples > 0 else 0
            rates[rate_name] = rate
            alerts[rate_name] = rate > thresholds[rate_name]
        
        return {"rates": rates, "alerts": alerts}
    
    def run_shadow_evaluation(self, checkpoint_path: Optional[str] = None, tag: str = "eval") -> Dict[str, Any]:
        """è¿è¡Œå½±å­è¯„ä¼°"""
        logger.info(f"è¿è¡Œå½±å­è¯„ä¼°: {tag}")
        
        # å¦‚æœæŒ‡å®šäº†checkpointï¼Œéœ€è¦åŠ è½½æ¨¡å‹
        if checkpoint_path:
            logger.info(f"ä»checkpointåŠ è½½æ¨¡å‹: {checkpoint_path}")
            # è¿™é‡Œåº”è¯¥åŠ è½½checkpointï¼Œä½†ç”±äºæ˜¯è¯•ç‚¼ç‰ˆæœ¬ï¼Œæš‚æ—¶è·³è¿‡
        
        # ä½¿ç”¨shadow_runè¿›è¡Œè¯„ä¼°
        result = self.shadow_evaluator.run_shadow_evaluation(
            n=self.config.eval_shadow_n,
            seed=self.config.seed,
            stratify=True
        )
        
        # æå–å…³é”®æŒ‡æ ‡
        evaluation_result = {
            "tag": tag,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "shadow_metrics": {
                "spearman": result["correlations"]["stable_dataset"]["spearman"],
                "top10_overlap": result["overlap_metrics"]["top10_overlap"],
                "corr_improve_pct": result["task_success_correlation"]["corr_improve_pct"]
            },
            "success_rates": {
                task: info["mean"] for task, info in result["task_success_correlation"]["success_rate_by_task"].items()
            },
            "overclar_rate": result.get("overclar_rate", 0.0),
            "avg_turns": result.get("avg_turns", 0.0)
        }
        
        return evaluation_result
    
    def train(self) -> Dict[str, Any]:
        """æ‰§è¡ŒPPOè®­ç»ƒ"""
        logger.info("å¼€å§‹PPOè®­ç»ƒ...")
        
        # è®¾ç½®æ¨¡å‹
        self.setup_model_and_tokenizer()
        
        # ç”Ÿæˆè®­ç»ƒæ•°æ®
        train_dataset = self.generate_training_data()
        
        # è¯•ç‚¼ç‰ˆæœ¬ï¼šè·³è¿‡çœŸå®PPOé…ç½®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè®­ç»ƒ
        logger.info("ä½¿ç”¨æ¨¡æ‹ŸPPOè®­ç»ƒæ¨¡å¼")
        
        # è®­ç»ƒå‰è¯„ä¼°
        pre_eval = self.run_shadow_evaluation(tag="pre_rl")
        
        # è®­ç»ƒå¾ªç¯ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        training_stats = {
            "steps": [],
            "rewards": [],
            "kl_divergence": [],
            "loss": []
        }
        
        logger.info("å¼€å§‹è®­ç»ƒå¾ªç¯...")
        
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼ˆå®é™…å®ç°ä¸­éœ€è¦çœŸæ­£çš„PPOå¾ªç¯ï¼‰
        for step in range(0, self.config.steps, self.config.eval_every_steps):
            # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
            batch = train_dataset.shuffle(seed=self.config.seed + step).select(range(self.config.batch_size))
            
            # æ¨¡æ‹Ÿå¥–åŠ±è®¡ç®—
            step_rewards = []
            for sample in batch:
                # ç”Ÿæˆå“åº”ï¼ˆè¿™é‡Œç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰
                response = f"è¿™æ˜¯æ­¥éª¤{step}çš„æ¨¡æ‹Ÿå“åº”: {sample['query'][:50]}..."
                reward = self.compute_reward(sample['query'], response, sample.get('meta', {}))
                step_rewards.append(reward)
            
            avg_reward = np.mean(step_rewards)
            mock_kl = np.random.normal(0.02, 0.01)  # æ¨¡æ‹ŸKLæ•£åº¦
            mock_loss = np.random.normal(0.5, 0.1)  # æ¨¡æ‹ŸæŸå¤±
            
            training_stats["steps"].append(step)
            training_stats["rewards"].append(avg_reward)
            training_stats["kl_divergence"].append(max(0, mock_kl))
            training_stats["loss"].append(max(0, mock_loss))
            
            logger.info(f"Step {step}: reward={avg_reward:.4f}, kl={mock_kl:.4f}")
            
            # KLæ£€æŸ¥
            if mock_kl > self.config.target_kl * 4:  # ä¸¥é‡è¶…æ ‡
                logger.warning(f"KLæ•£åº¦è¿‡é«˜: {mock_kl:.4f}")
                break
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if step % self.config.save_every_steps == 0 and step > 0:
                checkpoint_dir = f"checkpoints/ppo_trial/step_{step}"
                Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)
                logger.info(f"ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_dir}")
        
        # è®­ç»ƒåè¯„ä¼°
        post_eval = self.run_shadow_evaluation(tag="post_rl")
        
        # æ£€æŸ¥hacking
        total_samples = len(train_dataset) * (self.config.steps // self.config.eval_every_steps)
        hacking_check = self.check_hacking_thresholds(total_samples)
        
        # è®¡ç®—å¢é‡
        delta_metrics = self.compute_delta_metrics(pre_eval, post_eval)
        
        # éªŒæ”¶æ£€æŸ¥
        pass_criteria = self.check_pass_criteria(pre_eval, post_eval, hacking_check)
        
        # ç»„è£…ç»“æœ
        result = {
            "config": {
                "steps": self.config.steps,
                "model": self.config.base_model,
                "seed": self.config.seed
            },
            "train": {
                "steps": self.config.steps,
                "kl_curve": training_stats["kl_divergence"],
                "reward_curve": training_stats["rewards"],
                "final_kl": training_stats["kl_divergence"][-1] if training_stats["kl_divergence"] else 0.0
            },
            "eval_pre": pre_eval,
            "eval_post": post_eval,
            "delta": delta_metrics,
            "shadow": post_eval["shadow_metrics"],
            "hacking_signals": hacking_check["rates"],
            "pass_criteria": pass_criteria,
            "overall_pass": all(pass_criteria.values())
        }
        
        return result
    
    def compute_delta_metrics(self, pre_eval: Dict[str, Any], post_eval: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—å¢é‡æŒ‡æ ‡"""
        delta = {}
        
        # æˆåŠŸç‡å¢é‡
        success_pp = {}
        for task in ["hotpotqa", "strategyqa", "gsm8k"]:
            pre_success = pre_eval["success_rates"].get(task, 0.0)
            post_success = post_eval["success_rates"].get(task, 0.0)
            success_pp[task] = (post_success - pre_success) * 100  # è½¬æ¢ä¸ºç™¾åˆ†ç‚¹
        
        delta["success_pp"] = success_pp
        
        # è¿‡åº¦æ¾„æ¸…ç‡å˜åŒ–
        pre_overclar = pre_eval.get("overclar_rate", 0.0)
        post_overclar = post_eval.get("overclar_rate", 0.0)
        if pre_overclar > 0:
            overclar_change_pct = ((post_overclar - pre_overclar) / pre_overclar) * 100
        else:
            overclar_change_pct = 0.0
        delta["overclar_rate_pct"] = overclar_change_pct
        
        # å¹³å‡è½®æ•°å˜åŒ–
        pre_turns = pre_eval.get("avg_turns", 0.0)
        post_turns = post_eval.get("avg_turns", 0.0)
        delta["avg_turns"] = post_turns - pre_turns
        
        return delta
    
    def check_pass_criteria(self, pre_eval: Dict[str, Any], post_eval: Dict[str, Any], 
                          hacking_check: Dict[str, Any]) -> Dict[str, bool]:
        """æ£€æŸ¥éªŒæ”¶æ ‡å‡†"""
        criteria = {}
        
        # 1. ä»»åŠ¡æˆåŠŸç‡æå‡â‰¥5pp (éœ€è¦å‘é—®çš„ä»»åŠ¡)
        ask_needed_tasks = ["hotpotqa", "strategyqa"]
        success_improvements = []
        for task in ask_needed_tasks:
            pre_success = pre_eval["success_rates"].get(task, 0.0)
            post_success = post_eval["success_rates"].get(task, 0.0)
            improvement = (post_success - pre_success) * 100
            success_improvements.append(improvement)
        
        avg_success_improvement = np.mean(success_improvements) if success_improvements else 0.0
        criteria["success_improvement_5pp"] = avg_success_improvement >= 5.0
        
        # 2. è¿‡åº¦æ¾„æ¸…ç‡ç›¸å¯¹ä¸‹é™â‰¥20%
        pre_overclar = pre_eval.get("overclar_rate", 0.0)
        post_overclar = post_eval.get("overclar_rate", 0.0)
        if pre_overclar > 0:
            overclar_reduction = (pre_overclar - post_overclar) / pre_overclar
            criteria["overclar_reduction_20pct"] = overclar_reduction >= 0.2
        else:
            criteria["overclar_reduction_20pct"] = True  # æ— è¿‡åº¦æ¾„æ¸…åˆ™é€šè¿‡
        
        # 3. å¹³å‡è½®æ•°ä¸å¢åŠ 
        pre_turns = pre_eval.get("avg_turns", 0.0)
        post_turns = post_eval.get("avg_turns", 0.0)
        criteria["avg_turns_no_increase"] = post_turns <= pre_turns
        
        # 4. å½±å­è¿è¡Œç¨³æ€æŒ‡æ ‡
        shadow_metrics = post_eval["shadow_metrics"]
        criteria["shadow_spearman"] = shadow_metrics["spearman"] >= 0.75
        criteria["shadow_top10_overlap"] = shadow_metrics["top10_overlap"] >= 0.70
        criteria["shadow_corr_improve"] = shadow_metrics["corr_improve_pct"] >= 10
        
        # 5. KLç¨³å®šæ€§
        final_kl = post_eval.get("final_kl", 0.0)
        criteria["kl_stability"] = final_kl <= self.config.target_kl * 4
        
        # 6. æ— reward hacking
        alerts = hacking_check.get("alerts", {})
        criteria["no_hacking"] = not any(alerts.values())
        
        return criteria

def run_ablation_studies(base_config: PPOTrialConfig) -> Dict[str, Any]:
    """è¿è¡Œæ¶ˆèç ”ç©¶"""
    ablation_results = {}
    
    # 1. å…³é—­è¿‡åº¦æ¾„æ¸…æƒ©ç½š
    config_no_penalty = base_config
    config_no_penalty.use_overclar_penalty = False
    trainer_no_penalty = PPOTrialTrainer(config_no_penalty)
    result_no_penalty = trainer_no_penalty.run_shadow_evaluation(tag="ablate_penalty")
    ablation_results["penalty_off"] = result_no_penalty
    
    # 2. è°ƒæ•´alphaå‚æ•°
    config_alpha_04 = base_config
    config_alpha_04.overclar["alpha"] = 0.04
    trainer_alpha = PPOTrialTrainer(config_alpha_04)
    result_alpha = trainer_alpha.run_shadow_evaluation(tag="alpha_0p04")
    ablation_results["alpha_0p04"] = result_alpha
    
    # 3. ä½¿ç”¨å‡åŒ€æƒé‡
    config_uniform = base_config
    config_uniform.weights_file = "_uniform"
    trainer_uniform = PPOTrialTrainer(config_uniform)
    result_uniform = trainer_uniform.run_shadow_evaluation(tag="uniform_weights")
    ablation_results["uniform_weights"] = result_uniform
    
    return ablation_results

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="PPO Trial - 5k stepså°æ­¥PPOè¯•ç‚¼")
    parser.add_argument("--config", default="configs/ppo_trial.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--override", help="é…ç½®è¦†ç›–ï¼Œæ ¼å¼ï¼škey=value")
    parser.add_argument("--tag", default="main", help="å®éªŒæ ‡ç­¾")
    parser.add_argument("--ablation", action="store_true", help="è¿è¡Œæ¶ˆèç ”ç©¶")
    parser.add_argument("--eval-only", action="store_true", help="ä»…è¿è¡Œè¯„ä¼°")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    try:
        # åŠ è½½é…ç½®
        with open(args.config, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
        
        # åº”ç”¨è¦†ç›–
        if args.override:
            key, value = args.override.split("=", 1)
            keys = key.split(".")
            current = config_dict
            for k in keys[:-1]:
                current = current[k]
            # å°è¯•è½¬æ¢ç±»å‹
            try:
                if value.lower() in ["true", "false"]:
                    current[keys[-1]] = value.lower() == "true"
                elif value.replace(".", "").isdigit():
                    current[keys[-1]] = float(value) if "." in value else int(value)
                else:
                    current[keys[-1]] = value
            except:
                current[keys[-1]] = value
        
        # åˆ›å»ºé…ç½®å¯¹è±¡
        config = PPOTrialConfig(**config_dict)
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = PPOTrialTrainer(config)
        
        if args.eval_only:
            # ä»…è¿è¡Œè¯„ä¼°
            result = trainer.run_shadow_evaluation(tag=args.tag)
        else:
            # è¿è¡Œå®Œæ•´è®­ç»ƒ
            result = trainer.train()
            
            # æ·»åŠ æ¶ˆèç ”ç©¶
            if args.ablation:
                ablation_results = run_ablation_studies(config)
                result["ablation"] = ablation_results
        
        # ä¿å­˜ç»“æœ
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_file = f"reports/ppo_trial_{timestamp}_{args.tag}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            # è‡ªå®šä¹‰JSONç¼–ç å™¨å¤„ç†numpyç±»å‹
            def json_serializer(obj):
                if hasattr(obj, 'item'):  # numpy scalar
                    return obj.item()
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                if isinstance(obj, (np.bool_, bool)):
                    return bool(obj)
                raise TypeError(f'Object of type {obj.__class__.__name__} is not JSON serializable')
            
            json.dump(result, f, ensure_ascii=False, indent=2, default=json_serializer)
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print("ğŸš€ PPOè¯•ç‚¼ç»“æœæ‘˜è¦")
        print("=" * 60)
        
        if not args.eval_only:
            print(f"ğŸ“Š è®­ç»ƒæ­¥æ•°: {result['config']['steps']}")
            print(f"ğŸ¯ æœ€ç»ˆå¥–åŠ±: {result['train']['reward_curve'][-1]:.4f}")
            print(f"ğŸ“ˆ æœ€ç»ˆKL: {result['train']['final_kl']:.4f}")
            
            # æ‰“å°å¢é‡æŒ‡æ ‡
            delta = result["delta"]
            print(f"\nğŸ“ˆ æ€§èƒ½æå‡:")
            for task, improvement in delta["success_pp"].items():
                print(f"  {task}: {improvement:+.2f}pp")
            print(f"  è¿‡åº¦æ¾„æ¸…ç‡å˜åŒ–: {delta['overclar_rate_pct']:+.2f}%")
            print(f"  å¹³å‡è½®æ•°å˜åŒ–: {delta['avg_turns']:+.2f}")
            
            # éªŒæ”¶çŠ¶æ€
            pass_criteria = result["pass_criteria"]
            print(f"\nğŸš¦ éªŒæ”¶æ£€æŸ¥:")
            for criterion, passed in pass_criteria.items():
                status = "âœ… PASS" if passed else "âŒ FAIL"
                print(f"  {criterion}: {status}")
            
            overall_status = "âœ… å…¨éƒ¨é€šè¿‡" if result["overall_pass"] else "âŒ å­˜åœ¨æœªé€šè¿‡é¡¹"
            print(f"\nğŸ† æ€»ä½“çŠ¶æ€: {overall_status}")
        
        print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜: {output_file}")
        
        # è¿”å›é€€å‡ºç 
        if args.eval_only:
            sys.exit(0)
        else:
            sys.exit(0 if result["overall_pass"] else 1)
        
    except Exception as e:
        logger.error(f"PPOè¯•ç‚¼å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
