---

# **ã€ç»™ Cursorã€‘RC1 é‡å¯å¯åŠ¨åŒ…ï¼ˆæœ€å°ä¸Šä¸‹æ–‡ç‰ˆï¼‰**

ç›®æ ‡ï¼šåœ¨ä¸Šä¸‹æ–‡è¢«æ¸…ç©º/é™æ™ºçš„æƒ…å†µä¸‹ï¼Œä»é›¶æ¢å¤**çœŸå®HFå½±å­é›†â†’å®¡è®¡â†’è¯„æµ‹**ï¼Œå½»åº•ç¦æ­¢â€œæ¨¡æ¿è¦†ç›–HFæ•°æ®â€ã€‚  
çº¦æŸï¼šä¸¥ç¦â€œæ¼”ç¤ºå¼ç»§ç»­è·‘â€ï¼›å¤±è´¥å°±åœå¹¶å›æŠ¥æ—¥å¿—ä¸æ–‡ä»¶ç‰‡æ®µã€‚

## **0\) ç¯å¢ƒä¸€æ¬¡æ€§å°±ä½**

set \-euo pipefail  
export PYTHONUNBUFFERED=1  
export RUN\_MODE=prod  
export SCORER\_PROVIDER=gemini  
export GEMINI\_MODEL="${GEMINI\_MODEL:-gemini-2.5-flash}"  
export SCORING\_CACHE\_DISABLE=1  
mkdir \-p data/rollouts reports/rc1 logs scripts  
source .env || true

\# éœ€è¦çš„ä¾èµ–ï¼ˆdatasets \+ jqï¼‰  
python \-m pip \-q install \-U datasets jq || pip \-q install \-U datasets jq

## **1\) æ¸…ç©ºå¯ç–‘äº§ç‰©ï¼ˆä¿è¯â€œä»é›¶å¼€å§‹â€ï¼‰**

echo "ğŸ§¹ purgeâ€¦" \\  
&& rm \-f data/shadow\_eval\_245.jsonl reports/rc1/sample\_manifest.json \\  
&& rm \-f reports/rc1/shadow\_data\_audit.json \\  
&& rm \-f data/rollouts/rc1\_seed.jsonl data/rollouts/rc1\_seed.\*jsonl \\  
&& rm \-f reports/rc1/diversity\_report.txt reports/rc1/difficulty\_report.json \\  
&& find . \-name "\_\_pycache\_\_" \-type d \-exec rm \-rf {} \+ || true \\  
&& echo "âœ… purge ok"

## **2\) å†™å…¥ä¸‰æ”¯è„šæœ¬ï¼ˆç›´æ¥è¦†ç›–/æ–°å»ºï¼‰**

### **2.1 `scripts/rebuild_shadow_from_hf.py`**

\# scripts/rebuild\_shadow\_from\_hf.py  
import argparse, json, random, os  
from datasets import load\_dataset

ALLOWED \= {  
  "hotpotqa": dict(ds="hotpot\_qa", split="validation",  
                   map=lambda r: (r\["question"\], r\["answer"\])),  
  "strategyqa": dict(ds="strategy\_qa", split="train",  
                     map=lambda r: (r\["question"\], "yes" if bool(r\["answer"\]) else "no")),  
  "gsm8k": dict(ds="gsm8k", config="main", split="train",  
                map=lambda r: (r\["question"\], str(r\["answer"\]).strip())),  
}

def take(ds\_name, n, seed):  
  cfg \= ALLOWED\[ds\_name\]  
  name \= cfg.get("config")  
  d \= load\_dataset(cfg\["ds"\], name, split=cfg\["split"\])  
  idx \= list(range(len(d)))  
  random.Random(seed).shuffle(idx)  
  out=\[\]  
  for k,i in enumerate(idx\[:n\]):  
    q, a \= cfg\["map"\](d\[i\])  
    out.append(dict(  
      id=f"{ds\_name}\_{k}",  
      task=ds\_name,  
      question=str(q).strip(),  
      answer=str(a).strip(),  
      source="hf",  
      hf\_dataset=cfg\["ds"\],  
      hf\_config=name,  
      hf\_split=cfg\["split"\],  
      hf\_fingerprint=getattr(d, "\_fingerprint", None),  
      hf\_num\_rows=len(d),  
    ))  
  return out

def main():  
  ap=argparse.ArgumentParser()  
  ap.add\_argument("--n", type=int, default=245)  
  ap.add\_argument("--seed", type=int, default=20250821)  
  ap.add\_argument("--out", default="data/shadow\_eval\_245.jsonl")  
  ap.add\_argument("--manifest", default="reports/rc1/sample\_manifest.json")  
  args=ap.parse\_args()  
  n1=args.n//3; n2=args.n//3; n3=args.n-n1-n2  
  hot=take("hotpotqa", n1, args.seed+1)  
  sqa=take("strategyqa", n2, args.seed+2)  
  gsm=take("gsm8k", n3, args.seed+3)  
  all\_s=hot+sqa+gsm  
  os.makedirs(os.path.dirname(args.out), exist\_ok=True)  
  with open(args.out,"w") as f:  
    for r in all\_s: f.write(json.dumps(r, ensure\_ascii=False)+"\\n")  
  manifest=dict(samples=\[{"id":s\["id"\],"task":s\["task"\],"question":s\["question"\]} for s in all\_s\])  
  os.makedirs(os.path.dirname(args.manifest), exist\_ok=True)  
  json.dump(manifest, open(args.manifest,"w"), indent=2, ensure\_ascii=False)  
  print("OK rebuild:", len(all\_s), "-\>", args.out, args.manifest)

if \_\_name\_\_=="\_\_main\_\_":  
  main()

### **2.2 `scripts/audit_shadow_data.py`**

\# scripts/audit\_shadow\_data.py  
import argparse, json, re, random, statistics, sys  
from collections import Counter

def mask\_digits(s): return re.sub(r"\\d+", "\#", s.lower())  
def jaccard\_5gram(a,b):  
  A=set(\[a\[i:i+5\] for i in range(max(0,len(a)-4))\])  
  B=set(\[b\[i:i+5\] for i in range(max(0,len(b)-4))\])  
  return (len(A\&B)/len(A|B)) if A and B else 0.0

def main():  
  ap=argparse.ArgumentParser()  
  ap.add\_argument("infile")  
  ap.add\_argument("--report", default="reports/rc1/shadow\_data\_audit.json")  
  args=ap.parse\_args()  
  rows=\[json.loads(l) for l in open(args.infile)\]  
  N=len(rows)  
  \# 1\) çœŸä¼ªä¸å­—æ®µ  
  assert all(r.get("source")=="hf" for r in rows), "non-HF source detected"  
  assert all(r.get("task") in {"hotpotqa","strategyqa","gsm8k"} for r in rows), "illegal task"  
  assert all(r.get("question") for r in rows), "empty question exists"  
  \# 2\) æ©ç å”¯ä¸€ç‡ & æœ€é¢‘ç¹æ©ç å æ¯”  
  masks=\[mask\_digits(r\["question"\]) for r in rows\]  
  uniq \= len(set(masks))/N  
  top\_ratio \= Counter(masks).most\_common(1)\[0\]\[1\]/N  
  \# 3\) éšæœº 2000 å¯¹ 5-gram Jaccard  
  rnd=random.Random(20250821)  
  sims=\[\]  
  M=min(2000, max(0, N\*(N-1)//2))  
  seen=set()  
  while len(sims)\<M and N\>1:  
    i,j=rnd.randrange(N), rnd.randrange(N)  
    if i==j or (i,j) in seen: continue  
    seen.add((i,j))  
    sims.append(jaccard\_5gram(masks\[i\], masks\[j\]))  
  hi \= sum(1 for x in sims if x\>=0.9)/max(1,len(sims))  
  \# 4\) é•¿åº¦ä¸é‡å¤  
  lens=\[len(r\["question"\]) for r in rows\]  
  dup\_ratio \= 1 \- len(set((r\["task"\],r\["question"\]) for r in rows))/N  
  mean\_len \= statistics.mean(lens)  
  stdev\_len \= statistics.pstdev(lens)  
  \# ç¡¬é—¨æ§›  
  assert uniq \>= 0.60, f"mask uniqueness {uniq:.2f}\<0.60"  
  assert top\_ratio \<= 0.10, f"top mask ratio {top\_ratio:.2f}\>0.10"  
  assert hi \<= 0.01, f"jaccard\>=0.9 pairs {hi:.2%}\>1%"  
  assert 30 \<= mean\_len \<= 300, f"mean len {mean\_len:.1f} out of range"  
  assert stdev\_len \>= 15, f"std len {stdev\_len:.1f}\<15"  
  assert dup\_ratio \<= 0.01, f"dup ratio {dup\_ratio:.2%}\>1%"  
  rep=dict(N=N, mask\_uniqueness=uniq, top\_mask\_ratio=top\_ratio, jaccard\_hi\_ratio=hi,  
           mean\_len=mean\_len, std\_len=stdev\_len, dup\_ratio=dup\_ratio)  
  import os; os.makedirs("reports/rc1", exist\_ok=True)  
  json.dump(rep, open(args.report,"w"), indent=2)  
  print("AUDIT OK:", rep)

if \_\_name\_\_=="\_\_main\_\_":  
  main()

### **2.3 ç»™ `scripts/pre_run_check.py` åŠ â€œå®¡è®¡ \+ åˆ†å¸ƒå¥åº·â€é—¸é—¨**

è‹¥æ–‡ä»¶å·²å­˜åœ¨ï¼š**åœ¨å¼€å¤´æ’å…¥**ä»¥ä¸‹å‡½æ•°ï¼Œå¹¶åœ¨ä¸»æµç¨‹æ‹¿åˆ° `scores` åè°ƒç”¨ `assert_distribution_health(scores)`ï¼›è‹¥ä¸å­˜åœ¨å°±æ–°å»ºå¹¶åœ¨æœ€å `print("pre_run_check hook installed")`ã€‚

\# ç‰‡æ®µï¼šç²˜åˆ° scripts/pre\_run\_check.py é¡¶éƒ¨  
import json, numpy as np, os, sys  
def assert\_distribution\_health(scores, std\_min=0.08, iqr\_min=0.12):  
  arr=np.array(\[float(s) for s in scores\], dtype=float)  
  std=float(arr.std()); q75,q25=np.percentile(arr,75),np.percentile(arr,25); iqr=float(q75-q25)  
  assert std\>=std\_min and iqr\>=iqr\_min, f"score distribution too narrow: std={std:.3f}, iqr={iqr:.3f}"  
def assert\_shadow\_audit\_present():  
  p="reports/rc1/shadow\_data\_audit.json"  
  assert os.path.exists(p), f"missing audit report: {p}"  
  rep=json.load(open(p)); keys=\["mask\_uniqueness","top\_mask\_ratio","jaccard\_hi\_ratio","mean\_len","std\_len","dup\_ratio"\]  
  assert all(k in rep for k in keys), "incomplete audit report"  
  print("\[audit\] ok:", {k:rep\[k\] for k in keys})  
\# åœ¨ä¸»æµç¨‹æœ€å¼€å§‹è°ƒç”¨ï¼š  
\#   assert\_shadow\_audit\_present()  
\# åœ¨å¾—åˆ° scores ä¹‹åè°ƒç”¨ï¼š  
\#   assert\_distribution\_health(scores)

## **3\) ä¿®å¤ `src/evaluation/shadow_run.py` çš„ä¸¤ä¸ªå‘ï¼ˆé¿å…è¦†ç›–HFæ•°æ® & é”™æ˜ å°„ï¼‰**

ä¸æ”¹æ•´ä¸ªæ–‡ä»¶ï¼Œåªæ‰“ä¸¤ä¸ªâ€œæœ€å°è¡¥ä¸â€ã€‚è‹¥ä»£ç ä½ç½®ä¸åŒï¼ŒæŒ‰å…³é”®å­—å°±è¿‘æ’å…¥ã€‚

**è¡¥ä¸ Aï¼šä¼˜å…ˆä½¿ç”¨å·²ç‰©åŒ–çš„ HF æ•°æ®**

python \- \<\<'PY'  
import io,sys,os,re  
p="src/evaluation/shadow\_run.py"  
s=open(p,"r",encoding="utf-8").read()  
if "using existing HF materialized file" not in s:  
  s=s.replace("def load\_or\_generate\_sample\_data", "import json, os\\n\\ndef load\_or\_generate\_sample\_data")  
  k="def load\_or\_generate\_sample\_data"  
  i=s.find(k); assert i\>=0  
  j=s.find(":", i); assert j\>=0  
  head=s\[:j+1\]  
  rest=s\[j+1:\]  
  inject='''  
    \# \--- HF materialized short-circuit \---  
    if materialize\_path and os.path.exists(materialize\_path):  
        try:  
            with open(materialize\_path, "r", encoding="utf-8") as \_f:  
                \_first \= json.loads(next(iter(\_f)))  
            if \_first.get("source") \== "hf":  
                print("\[shadow\] using existing HF materialized file:", materialize\_path)  
                return \[json.loads(l) for l in open(materialize\_path, "r", encoding="utf-8")\]  
        except StopIteration:  
            pass  
    \# \--- end short-circuit \---  
'''  
  s=head+inject+rest  
  open(p,"w",encoding="utf-8").write(s)  
  print("âœ… patched A")  
else:  
  print("âœ… already patched A")  
PY

**è¡¥ä¸ Bï¼šmanifest å­—æ®µæ˜ å°„ï¼ˆtask/questionï¼‰**

python \- \<\<'PY'  
import re, os, json  
p="src/evaluation/shadow\_run.py"  
s=open(p,"r",encoding="utf-8").read()  
if '"task\_type","unknown"' in s or "turns" in s and "content" in s:  
  s=s.replace(  
    '{"task": s.get("task\_type","unknown"), "question": s.get("turns",\[{}\])\[0\].get("content","")}',  
    '({"task": (s.get("task") or s.get("task\_type") or "unknown"), "question": (s.get("question") or s.get("turns",\[{}\])\[0\].get("content","")), "id": s.get("id")})'  
  )  
  open(p,"w",encoding="utf-8").write(s); print("âœ… patched B")  
else:  
  print("â„¹ï¸ manifest mapping seems fine; skip B")  
PY

**éªŒæ”¶ï¼ˆè„šæœ¬å¯¼å…¥ä¸æŠ¥é”™ï¼‰ï¼š**

python \-c "import scripts.rebuild\_shadow\_from\_hf; print('âœ… rebuild\_shadow\_from\_hf ok')"  
python \-c "import scripts.audit\_shadow\_data; print('âœ… audit\_shadow\_data ok')"  
python \-c "import importlib; importlib.import\_module('src.evaluation.shadow\_run'); print('âœ… shadow\_run import ok')"

## **4\) ä¸‰è¿ï¼šé‡å»ºâ†’å®¡è®¡â†’manifestä½“æ£€ï¼ˆå¿…é¡»ä¸€æ¬¡é€šè¿‡ï¼‰**

python scripts/rebuild\_shadow\_from\_hf.py \\  
  \--n 245 \--seed 20250821 \\  
  \--out data/shadow\_eval\_245.jsonl \\  
  \--manifest reports/rc1/sample\_manifest.json

python scripts/audit\_shadow\_data.py data/shadow\_eval\_245.jsonl \\  
  \--report reports/rc1/shadow\_data\_audit.json

python \- \<\<'PY'  
import json,collections  
j=json.load(open("reports/rc1/sample\_manifest.json"))  
bad=\[x for x in j\["samples"\] if (not x.get("task")) or x\["task"\]=="unknown" or (not x.get("question"))\]  
print("by\_task:", collections.Counter(\[s\["task"\] for s in j\["samples"\]\]))  
assert not bad, f"bad samples exist: {bad\[:3\]}"  
print("âœ… manifest ok")  
PY

## **5\) å½±å­è¯„ä¼° \+ é¢„æ£€ï¼ˆå¸¦åˆ†å¸ƒå¥åº·é—¸é—¨ï¼‰**

å…ˆç¡®ä¿è¯„åˆ†å™¨ç”¨**æ¸©åº¦0+JSON** rubricï¼ˆå·²åœ¨ä¹‹å‰ä¿®è¿‡ï¼‰ï¼›å¦‚æœæ²¡æœ‰ï¼Œè¯·ä½ åœ¨ `src/scoring/providers/gemini.py` é‡ŒæŒ‰ä»¥ä¸‹ç‰‡æ®µæ”¹å¥½ï¼ˆç•¥ï¼‰ã€‚

stdbuf \-oL \-eL python \-m src.evaluation.shadow\_run \\  
  \--n 245 \--seed 20250821 \--stratify \\  
  \--materialize data/shadow\_eval\_245.jsonl \\  
  \--dump-manifest reports/rc1/sample\_manifest.json 2\>&1 | tee logs/shadow\_run.log

python scripts/pre\_run\_check.py \--shadow data/shadow\_eval\_245.jsonl \\  
  \--spearman-min 0.55 \--top10-min 0.60

**ç¡¬é—¨æ§›**ï¼š`spearmanâ‰¥0.55`ã€`top10â‰¥0.60`ï¼Œä¸” `stdâ‰¥0.08 & IQRâ‰¥0.12`ã€‚  
**å¤±è´¥å°±åœ**ï¼šå›ä¼  `logs/shadow_run.log` æœ«200è¡Œ \+ `reports/rc1/sample_manifest.json` å‰60è¡Œã€‚

## **6\) ï¼ˆé€šè¿‡åï¼‰ç»§ç»­åŸæµç¨‹**

* ç§å­æ± æ„å»ºâ†’å¤šæ ·æ€§/éš¾åº¦æ ¡éªŒ  
* RC é—¸é—¨ï¼ˆè‡ªåŠ¨åˆ¤å®šï¼‰  
* 2kæ­¥å°è·‘â†’HF/GitHub æ¨é€

ï¼ˆè¿™äº›å‘½ä»¤ä½ å·²ç»æœ‰äº†ï¼Œæ­¤å¤„ä¸èµ˜è¿°ï¼›è‹¥éœ€è¦æˆ‘å†å‘â€œé™æ™ºç‰ˆâ€ï¼Œå¼ä¸€å£°ã€‚ï¼‰

---

## **æ•…éšœæ’æŸ¥ï¼ˆé‡å¯åå¸¸è§é—®é¢˜ï¼‰**

* **å¯¼å…¥å¤±è´¥**ï¼šç¡®è®¤ä½ å·²æ–°å»ºä¸‰æ”¯è„šæœ¬ï¼›`python -c "import scripts.rebuild_shadow_from_hf"` ä¸æŠ¥é”™å³é€šè¿‡ã€‚  
* **HF æ‹‰å–å¤±è´¥**ï¼šæœ¬æœºéœ€èƒ½è®¿é—® Hugging Faceï¼›å¿…è¦æ—¶ `huggingface-cli login` æˆ–è®¾ç½® `HF_HUB_DISABLE_TELEMETRY=1`ã€‚  
* **manifest ä»å…¨ unknown/ç©º**ï¼šè¯´æ˜è¡¥ä¸æœªç”Ÿæ•ˆæˆ– shadow\_run åˆè¦†ç›–äº†ç‰©åŒ–æ–‡ä»¶ï¼›é‡è·‘â€œè¡¥ä¸A/Bâ€ï¼Œå†æ‰§è¡Œâ€œä¸‰è¿â€ã€‚  
* **Spearman / top10 ä»ä¸è¾¾æ ‡**ï¼šæŠŠ `logs/shadow_run.log` æœ«200è¡Œä¸ä»»æ„5æ¡æ ·æœ¬åŸæ–‡è´´å›ï¼Œå¹¶**ä¸è¦**è¿ç»­é‡è¯•ã€‚

---

å¦‚æœè¦å†ç¨³ä¸€ç‚¹ï¼Œæˆ‘ä¹Ÿå‡†å¤‡äº†**ä¸€é”®è„šæœ¬**ï¼ˆæŠŠä¸Šé¢ä¸²èµ·æ¥ï¼‰ã€‚ä½ å¯ä»¥è®©ä»–ä¿å­˜ä¸º `scripts/bootstrap_reboot.sh`å¹¶æ‰§è¡Œï¼š

cat \> scripts/bootstrap\_reboot.sh \<\<'SH'  
\#\!/usr/bin/env bash  
set \-euo pipefail  
export PYTHONUNBUFFERED=1 RUN\_MODE=prod SCORER\_PROVIDER=gemini GEMINI\_MODEL="${GEMINI\_MODEL:-gemini-2.5-flash}" SCORING\_CACHE\_DISABLE=1  
mkdir \-p data/rollouts reports/rc1 logs scripts  
source .env || true  
python \-m pip \-q install \-U datasets jq || true  
\# purge  
rm \-f data/shadow\_eval\_245.jsonl reports/rc1/sample\_manifest.json reports/rc1/shadow\_data\_audit.json \\  
      data/rollouts/rc1\_seed.jsonl data/rollouts/rc1\_seed.\*jsonl reports/rc1/diversity\_report.txt reports/rc1/difficulty\_report.json || true  
find . \-name "\_\_pycache\_\_" \-type d \-exec rm \-rf {} \+ || true  
\# scripts exist?  
python \-c "import scripts.rebuild\_shadow\_from\_hf" 2\>/dev/null || { echo "missing rebuild\_shadow\_from\_hf.py"; exit 1; }  
python \-c "import scripts.audit\_shadow\_data" 2\>/dev/null || { echo "missing audit\_shadow\_data.py"; exit 1; }  
python \-c "import importlib; importlib.import\_module('src.evaluation.shadow\_run')" 2\>/dev/null || { echo "shadow\_run import fail"; exit 1; }  
\# build+audit+check  
python scripts/rebuild\_shadow\_from\_hf.py \--n 245 \--seed 20250821 \--out data/shadow\_eval\_245.jsonl \--manifest reports/rc1/sample\_manifest.json  
python scripts/audit\_shadow\_data.py data/shadow\_eval\_245.jsonl \--report reports/rc1/shadow\_data\_audit.json  
python \- \<\<'PY'  
import json,collections  
j=json.load(open("reports/rc1/sample\_manifest.json"))  
bad=\[x for x in j\["samples"\] if (not x.get("task")) or x\["task"\]=="unknown" or (not x.get("question"))\]  
print("by\_task:", collections.Counter(\[s\["task"\] for s in j\["samples"\]\]))  
assert not bad, f"bad samples exist: {bad\[:3\]}"  
print("manifest ok")  
PY  
\# evaluate  
stdbuf \-oL \-eL python \-m src.evaluation.shadow\_run \--n 245 \--seed 20250821 \--stratify \\  
  \--materialize data/shadow\_eval\_245.jsonl \--dump-manifest reports/rc1/sample\_manifest.json 2\>&1 | tee logs/shadow\_run.log  
python scripts/pre\_run\_check.py \--shadow data/shadow\_eval\_245.jsonl \--spearman-min 0.55 \--top10-min 0.60  
SH  
chmod \+x scripts/bootstrap\_reboot.sh  
./scripts/bootstrap\_reboot.sh

---

æŠŠè¿™ä»½â€œå¯åŠ¨åŒ…â€ä¸¢ç»™ä»–ï¼Œç­‰ä»–é‡å¯åç›´æ¥ç…§è·‘å°±èƒ½ç»­å‘½ã€‚å¦‚æœä»–å“ªä¸€æ­¥åˆå¡ä½äº†ï¼ŒæŠŠ**å¤±è´¥çš„å‘½ä»¤è¾“å‡º \+ æ—¥å¿—æœ«200è¡Œ \+ ç›¸å…³æ–‡ä»¶å‰/åè‹¥å¹²è¡Œ**è´´å›æ¥ï¼Œæˆ‘ç»§ç»­ç›¯ã€‚

