---

# **【给 Cursor】RC1 重启启动包（最小上下文版）**

目标：在上下文被清空/降智的情况下，从零恢复**真实HF影子集→审计→评测**，彻底禁止“模板覆盖HF数据”。  
约束：严禁“演示式继续跑”；失败就停并回报日志与文件片段。

## **0\) 环境一次性就位**

set \-euo pipefail  
export PYTHONUNBUFFERED=1  
export RUN\_MODE=prod  
export SCORER\_PROVIDER=gemini  
export GEMINI\_MODEL="${GEMINI\_MODEL:-gemini-2.5-flash}"  
export SCORING\_CACHE\_DISABLE=1  
mkdir \-p data/rollouts reports/rc1 logs scripts  
source .env || true

\# 需要的依赖（datasets \+ jq）  
python \-m pip \-q install \-U datasets jq || pip \-q install \-U datasets jq

## **1\) 清空可疑产物（保证“从零开始”）**

echo "🧹 purge…" \\  
&& rm \-f data/shadow\_eval\_245.jsonl reports/rc1/sample\_manifest.json \\  
&& rm \-f reports/rc1/shadow\_data\_audit.json \\  
&& rm \-f data/rollouts/rc1\_seed.jsonl data/rollouts/rc1\_seed.\*jsonl \\  
&& rm \-f reports/rc1/diversity\_report.txt reports/rc1/difficulty\_report.json \\  
&& find . \-name "\_\_pycache\_\_" \-type d \-exec rm \-rf {} \+ || true \\  
&& echo "✅ purge ok"

## **2\) 写入三支脚本（直接覆盖/新建）**

### **2.1 `scripts/rebuild_shadow_from_hf.py`**

\# scripts/rebuild\_shadow\_from\_hf.py  
import argparse, json, random, os  
from datasets import load\_dataset

ALLOWED \= {  
  "hotpotqa": dict(ds="hotpot\_qa", split="validation",  
                   map=lambda r: (r\["question"\], r\["answer"\])),  
  "strategyqa": dict(ds="strategy\_qa", split="train",  
                     map=lambda r: (r\["question"\], "yes" if bool(r\["answer"\]) else "no")),  
  "gsm8k": dict(ds="gsm8k", config="main", split="train",  
                map=lambda r: (r\["question"\], str(r\["answer"\]).strip())),  
}

def take(ds\_name, n, seed):  
  cfg \= ALLOWED\[ds\_name\]  
  name \= cfg.get("config")  
  d \= load\_dataset(cfg\["ds"\], name, split=cfg\["split"\])  
  idx \= list(range(len(d)))  
  random.Random(seed).shuffle(idx)  
  out=\[\]  
  for k,i in enumerate(idx\[:n\]):  
    q, a \= cfg\["map"\](d\[i\])  
    out.append(dict(  
      id=f"{ds\_name}\_{k}",  
      task=ds\_name,  
      question=str(q).strip(),  
      answer=str(a).strip(),  
      source="hf",  
      hf\_dataset=cfg\["ds"\],  
      hf\_config=name,  
      hf\_split=cfg\["split"\],  
      hf\_fingerprint=getattr(d, "\_fingerprint", None),  
      hf\_num\_rows=len(d),  
    ))  
  return out

def main():  
  ap=argparse.ArgumentParser()  
  ap.add\_argument("--n", type=int, default=245)  
  ap.add\_argument("--seed", type=int, default=20250821)  
  ap.add\_argument("--out", default="data/shadow\_eval\_245.jsonl")  
  ap.add\_argument("--manifest", default="reports/rc1/sample\_manifest.json")  
  args=ap.parse\_args()  
  n1=args.n//3; n2=args.n//3; n3=args.n-n1-n2  
  hot=take("hotpotqa", n1, args.seed+1)  
  sqa=take("strategyqa", n2, args.seed+2)  
  gsm=take("gsm8k", n3, args.seed+3)  
  all\_s=hot+sqa+gsm  
  os.makedirs(os.path.dirname(args.out), exist\_ok=True)  
  with open(args.out,"w") as f:  
    for r in all\_s: f.write(json.dumps(r, ensure\_ascii=False)+"\\n")  
  manifest=dict(samples=\[{"id":s\["id"\],"task":s\["task"\],"question":s\["question"\]} for s in all\_s\])  
  os.makedirs(os.path.dirname(args.manifest), exist\_ok=True)  
  json.dump(manifest, open(args.manifest,"w"), indent=2, ensure\_ascii=False)  
  print("OK rebuild:", len(all\_s), "-\>", args.out, args.manifest)

if \_\_name\_\_=="\_\_main\_\_":  
  main()

### **2.2 `scripts/audit_shadow_data.py`**

\# scripts/audit\_shadow\_data.py  
import argparse, json, re, random, statistics, sys  
from collections import Counter

def mask\_digits(s): return re.sub(r"\\d+", "\#", s.lower())  
def jaccard\_5gram(a,b):  
  A=set(\[a\[i:i+5\] for i in range(max(0,len(a)-4))\])  
  B=set(\[b\[i:i+5\] for i in range(max(0,len(b)-4))\])  
  return (len(A\&B)/len(A|B)) if A and B else 0.0

def main():  
  ap=argparse.ArgumentParser()  
  ap.add\_argument("infile")  
  ap.add\_argument("--report", default="reports/rc1/shadow\_data\_audit.json")  
  args=ap.parse\_args()  
  rows=\[json.loads(l) for l in open(args.infile)\]  
  N=len(rows)  
  \# 1\) 真伪与字段  
  assert all(r.get("source")=="hf" for r in rows), "non-HF source detected"  
  assert all(r.get("task") in {"hotpotqa","strategyqa","gsm8k"} for r in rows), "illegal task"  
  assert all(r.get("question") for r in rows), "empty question exists"  
  \# 2\) 掩码唯一率 & 最频繁掩码占比  
  masks=\[mask\_digits(r\["question"\]) for r in rows\]  
  uniq \= len(set(masks))/N  
  top\_ratio \= Counter(masks).most\_common(1)\[0\]\[1\]/N  
  \# 3\) 随机 2000 对 5-gram Jaccard  
  rnd=random.Random(20250821)  
  sims=\[\]  
  M=min(2000, max(0, N\*(N-1)//2))  
  seen=set()  
  while len(sims)\<M and N\>1:  
    i,j=rnd.randrange(N), rnd.randrange(N)  
    if i==j or (i,j) in seen: continue  
    seen.add((i,j))  
    sims.append(jaccard\_5gram(masks\[i\], masks\[j\]))  
  hi \= sum(1 for x in sims if x\>=0.9)/max(1,len(sims))  
  \# 4\) 长度与重复  
  lens=\[len(r\["question"\]) for r in rows\]  
  dup\_ratio \= 1 \- len(set((r\["task"\],r\["question"\]) for r in rows))/N  
  mean\_len \= statistics.mean(lens)  
  stdev\_len \= statistics.pstdev(lens)  
  \# 硬门槛  
  assert uniq \>= 0.60, f"mask uniqueness {uniq:.2f}\<0.60"  
  assert top\_ratio \<= 0.10, f"top mask ratio {top\_ratio:.2f}\>0.10"  
  assert hi \<= 0.01, f"jaccard\>=0.9 pairs {hi:.2%}\>1%"  
  assert 30 \<= mean\_len \<= 300, f"mean len {mean\_len:.1f} out of range"  
  assert stdev\_len \>= 15, f"std len {stdev\_len:.1f}\<15"  
  assert dup\_ratio \<= 0.01, f"dup ratio {dup\_ratio:.2%}\>1%"  
  rep=dict(N=N, mask\_uniqueness=uniq, top\_mask\_ratio=top\_ratio, jaccard\_hi\_ratio=hi,  
           mean\_len=mean\_len, std\_len=stdev\_len, dup\_ratio=dup\_ratio)  
  import os; os.makedirs("reports/rc1", exist\_ok=True)  
  json.dump(rep, open(args.report,"w"), indent=2)  
  print("AUDIT OK:", rep)

if \_\_name\_\_=="\_\_main\_\_":  
  main()

### **2.3 给 `scripts/pre_run_check.py` 加“审计 \+ 分布健康”闸门**

若文件已存在：**在开头插入**以下函数，并在主流程拿到 `scores` 后调用 `assert_distribution_health(scores)`；若不存在就新建并在最后 `print("pre_run_check hook installed")`。

\# 片段：粘到 scripts/pre\_run\_check.py 顶部  
import json, numpy as np, os, sys  
def assert\_distribution\_health(scores, std\_min=0.08, iqr\_min=0.12):  
  arr=np.array(\[float(s) for s in scores\], dtype=float)  
  std=float(arr.std()); q75,q25=np.percentile(arr,75),np.percentile(arr,25); iqr=float(q75-q25)  
  assert std\>=std\_min and iqr\>=iqr\_min, f"score distribution too narrow: std={std:.3f}, iqr={iqr:.3f}"  
def assert\_shadow\_audit\_present():  
  p="reports/rc1/shadow\_data\_audit.json"  
  assert os.path.exists(p), f"missing audit report: {p}"  
  rep=json.load(open(p)); keys=\["mask\_uniqueness","top\_mask\_ratio","jaccard\_hi\_ratio","mean\_len","std\_len","dup\_ratio"\]  
  assert all(k in rep for k in keys), "incomplete audit report"  
  print("\[audit\] ok:", {k:rep\[k\] for k in keys})  
\# 在主流程最开始调用：  
\#   assert\_shadow\_audit\_present()  
\# 在得到 scores 之后调用：  
\#   assert\_distribution\_health(scores)

## **3\) 修复 `src/evaluation/shadow_run.py` 的两个坑（避免覆盖HF数据 & 错映射）**

不改整个文件，只打两个“最小补丁”。若代码位置不同，按关键字就近插入。

**补丁 A：优先使用已物化的 HF 数据**

python \- \<\<'PY'  
import io,sys,os,re  
p="src/evaluation/shadow\_run.py"  
s=open(p,"r",encoding="utf-8").read()  
if "using existing HF materialized file" not in s:  
  s=s.replace("def load\_or\_generate\_sample\_data", "import json, os\\n\\ndef load\_or\_generate\_sample\_data")  
  k="def load\_or\_generate\_sample\_data"  
  i=s.find(k); assert i\>=0  
  j=s.find(":", i); assert j\>=0  
  head=s\[:j+1\]  
  rest=s\[j+1:\]  
  inject='''  
    \# \--- HF materialized short-circuit \---  
    if materialize\_path and os.path.exists(materialize\_path):  
        try:  
            with open(materialize\_path, "r", encoding="utf-8") as \_f:  
                \_first \= json.loads(next(iter(\_f)))  
            if \_first.get("source") \== "hf":  
                print("\[shadow\] using existing HF materialized file:", materialize\_path)  
                return \[json.loads(l) for l in open(materialize\_path, "r", encoding="utf-8")\]  
        except StopIteration:  
            pass  
    \# \--- end short-circuit \---  
'''  
  s=head+inject+rest  
  open(p,"w",encoding="utf-8").write(s)  
  print("✅ patched A")  
else:  
  print("✅ already patched A")  
PY

**补丁 B：manifest 字段映射（task/question）**

python \- \<\<'PY'  
import re, os, json  
p="src/evaluation/shadow\_run.py"  
s=open(p,"r",encoding="utf-8").read()  
if '"task\_type","unknown"' in s or "turns" in s and "content" in s:  
  s=s.replace(  
    '{"task": s.get("task\_type","unknown"), "question": s.get("turns",\[{}\])\[0\].get("content","")}',  
    '({"task": (s.get("task") or s.get("task\_type") or "unknown"), "question": (s.get("question") or s.get("turns",\[{}\])\[0\].get("content","")), "id": s.get("id")})'  
  )  
  open(p,"w",encoding="utf-8").write(s); print("✅ patched B")  
else:  
  print("ℹ️ manifest mapping seems fine; skip B")  
PY

**验收（脚本导入不报错）：**

python \-c "import scripts.rebuild\_shadow\_from\_hf; print('✅ rebuild\_shadow\_from\_hf ok')"  
python \-c "import scripts.audit\_shadow\_data; print('✅ audit\_shadow\_data ok')"  
python \-c "import importlib; importlib.import\_module('src.evaluation.shadow\_run'); print('✅ shadow\_run import ok')"

## **4\) 三连：重建→审计→manifest体检（必须一次通过）**

python scripts/rebuild\_shadow\_from\_hf.py \\  
  \--n 245 \--seed 20250821 \\  
  \--out data/shadow\_eval\_245.jsonl \\  
  \--manifest reports/rc1/sample\_manifest.json

python scripts/audit\_shadow\_data.py data/shadow\_eval\_245.jsonl \\  
  \--report reports/rc1/shadow\_data\_audit.json

python \- \<\<'PY'  
import json,collections  
j=json.load(open("reports/rc1/sample\_manifest.json"))  
bad=\[x for x in j\["samples"\] if (not x.get("task")) or x\["task"\]=="unknown" or (not x.get("question"))\]  
print("by\_task:", collections.Counter(\[s\["task"\] for s in j\["samples"\]\]))  
assert not bad, f"bad samples exist: {bad\[:3\]}"  
print("✅ manifest ok")  
PY

## **5\) 影子评估 \+ 预检（带分布健康闸门）**

先确保评分器用**温度0+JSON** rubric（已在之前修过）；如果没有，请你在 `src/scoring/providers/gemini.py` 里按以下片段改好（略）。

stdbuf \-oL \-eL python \-m src.evaluation.shadow\_run \\  
  \--n 245 \--seed 20250821 \--stratify \\  
  \--materialize data/shadow\_eval\_245.jsonl \\  
  \--dump-manifest reports/rc1/sample\_manifest.json 2\>&1 | tee logs/shadow\_run.log

python scripts/pre\_run\_check.py \--shadow data/shadow\_eval\_245.jsonl \\  
  \--spearman-min 0.55 \--top10-min 0.60

**硬门槛**：`spearman≥0.55`、`top10≥0.60`，且 `std≥0.08 & IQR≥0.12`。  
**失败就停**：回传 `logs/shadow_run.log` 末200行 \+ `reports/rc1/sample_manifest.json` 前60行。

## **6\) （通过后）继续原流程**

* 种子池构建→多样性/难度校验  
* RC 闸门（自动判定）  
* 2k步小跑→HF/GitHub 推送

（这些命令你已经有了，此处不赘述；若需要我再发“降智版”，吼一声。）

---

## **故障排查（重启后常见问题）**

* **导入失败**：确认你已新建三支脚本；`python -c "import scripts.rebuild_shadow_from_hf"` 不报错即通过。  
* **HF 拉取失败**：本机需能访问 Hugging Face；必要时 `huggingface-cli login` 或设置 `HF_HUB_DISABLE_TELEMETRY=1`。  
* **manifest 仍全 unknown/空**：说明补丁未生效或 shadow\_run 又覆盖了物化文件；重跑“补丁A/B”，再执行“三连”。  
* **Spearman / top10 仍不达标**：把 `logs/shadow_run.log` 末200行与任意5条样本原文贴回，并**不要**连续重试。

---

如果要再稳一点，我也准备了**一键脚本**（把上面串起来）。你可以让他保存为 `scripts/bootstrap_reboot.sh`并执行：

cat \> scripts/bootstrap\_reboot.sh \<\<'SH'  
\#\!/usr/bin/env bash  
set \-euo pipefail  
export PYTHONUNBUFFERED=1 RUN\_MODE=prod SCORER\_PROVIDER=gemini GEMINI\_MODEL="${GEMINI\_MODEL:-gemini-2.5-flash}" SCORING\_CACHE\_DISABLE=1  
mkdir \-p data/rollouts reports/rc1 logs scripts  
source .env || true  
python \-m pip \-q install \-U datasets jq || true  
\# purge  
rm \-f data/shadow\_eval\_245.jsonl reports/rc1/sample\_manifest.json reports/rc1/shadow\_data\_audit.json \\  
      data/rollouts/rc1\_seed.jsonl data/rollouts/rc1\_seed.\*jsonl reports/rc1/diversity\_report.txt reports/rc1/difficulty\_report.json || true  
find . \-name "\_\_pycache\_\_" \-type d \-exec rm \-rf {} \+ || true  
\# scripts exist?  
python \-c "import scripts.rebuild\_shadow\_from\_hf" 2\>/dev/null || { echo "missing rebuild\_shadow\_from\_hf.py"; exit 1; }  
python \-c "import scripts.audit\_shadow\_data" 2\>/dev/null || { echo "missing audit\_shadow\_data.py"; exit 1; }  
python \-c "import importlib; importlib.import\_module('src.evaluation.shadow\_run')" 2\>/dev/null || { echo "shadow\_run import fail"; exit 1; }  
\# build+audit+check  
python scripts/rebuild\_shadow\_from\_hf.py \--n 245 \--seed 20250821 \--out data/shadow\_eval\_245.jsonl \--manifest reports/rc1/sample\_manifest.json  
python scripts/audit\_shadow\_data.py data/shadow\_eval\_245.jsonl \--report reports/rc1/shadow\_data\_audit.json  
python \- \<\<'PY'  
import json,collections  
j=json.load(open("reports/rc1/sample\_manifest.json"))  
bad=\[x for x in j\["samples"\] if (not x.get("task")) or x\["task"\]=="unknown" or (not x.get("question"))\]  
print("by\_task:", collections.Counter(\[s\["task"\] for s in j\["samples"\]\]))  
assert not bad, f"bad samples exist: {bad\[:3\]}"  
print("manifest ok")  
PY  
\# evaluate  
stdbuf \-oL \-eL python \-m src.evaluation.shadow\_run \--n 245 \--seed 20250821 \--stratify \\  
  \--materialize data/shadow\_eval\_245.jsonl \--dump-manifest reports/rc1/sample\_manifest.json 2\>&1 | tee logs/shadow\_run.log  
python scripts/pre\_run\_check.py \--shadow data/shadow\_eval\_245.jsonl \--spearman-min 0.55 \--top10-min 0.60  
SH  
chmod \+x scripts/bootstrap\_reboot.sh  
./scripts/bootstrap\_reboot.sh

---

把这份“启动包”丢给他，等他重启后直接照跑就能续命。如果他哪一步又卡住了，把**失败的命令输出 \+ 日志末200行 \+ 相关文件前/后若干行**贴回来，我继续盯。

