干得漂亮，流水线和断点续训都打通了 👍。基于你最新日志，我这边做了体检&对齐我们既有路线图，下一步直接进入“模板多样性 \+ 真实评分 \+ 影子对比 \+ 权重校准 \+ 过度澄清惩罚”的连续三相工单。所有门槛与命令都给全了，照抄即可。

---

# **光速工单（按顺序执行，任何 FAIL 就地修）**

## **Phase 1：模板多样性 \+ 真实评分落地（门槛不过不准扩量）**

1. **Template Pack v2**：每类任务≥6个模板（数学/多跳/澄清），覆盖至少4种角色、3种语体，落盘到 `templates/pack_v2/*.json`。验收：distinct-2≥0.60、与v1的3-gram KL≥0.15、角色≥4、语体≥3，生成 `diversity_report.json`。  
2. **开启真实评分 live\_mode**：打分走K=3、取median并记录variance；variance\>0.08 的样本标红写入 `unstable_samples.jsonl`，训练阶段降权或重评；缓存要有TTL（默认14天）和版本失效。  
3. **缺口补丁（立刻加代码）**  
* `REWARD_LIVE_MODE` 开关（stub vs live）  
* `scorecache-cli`：`stat / invalidate --spec / replay-bad`  
* 限流：令牌桶+自适应并发，监控里写 `rps/429_rate/avg_latency`  
* `over_clarification_penalty()` 模块（先占位，本相接入在 Phase 2）  
* `calibrate_weights()`（非负最小二乘+先验；先占位）  
* `diversity_report.py` 指标：distinct-1/2、TTR、3-gram KL、Zipf 斜率、角色/语体覆盖度  
* README 标注“训练=PyTorch/TRL；推理=llama.cpp，在线rollouts禁用llama.cpp”，并加 PyTorch→GGUF 占位脚本。

说明：你的日志里训练环节已可跑，但我们必须先用 **多样性与真实评分** 把数据闸门立起来，再许扩量与RL。

---

## **Phase 2：影子运行对比 \+ 权重校准 \+ 过度澄清惩罚（出报告）**

新增文件：  
`src/evaluation/shadow_run.py` / `weight_calib.py` / `overclar_penalty.py`，报告落 `reports/`；在 `configs/default.yaml` 追加门槛与参数（spearman、top10、alpha 等）。

A) **影子运行对比**（245条，三任务分层）：对同批样本同时跑“旧7维评分 vs 新奖励(含live聚合)”；输出 `spearman / kendall / top10/20 overlap / 与成功率的相关性`；门槛：spearman≥0.75、top10≥0.70、rank-corr相对旧系统提升≥10%。命令：

python \-m src.evaluation.shadow\_run \--n 245 \--seed 20250820 \--stratify

B) **权重校准**（非负最小二乘 \+ L2先验，5折CV+200次bootstrap）：落盘 `configs/weights.json` 与 `calibration_report_*.json`；门槛：rank-corr中位数相对旧权重提升≥8%、MAE下降≥5%、任一维权重≤0.5且全非负。命令：

python \-m src.evaluation.weight\_calib \--cv 5 \--boot 200 \--l2 0.1 \--seed 42

C) **过度澄清惩罚**：仅在 `needs_clarification=false` 样本上按 `penalty = alpha * min(c, cap)` 扣分（默认α=0.07，cap=3）；A/B 报告门槛：过度澄清率相对下降≥20%、平均轮数不升、成功率不劣化\>1pp。命令：

python \-m src.evaluation.overclar\_penalty \--alpha 0.07 \--cap 3 \--ablation

提醒：你之前的评测里 spearman 与 top-10 偏低，Phase 2 的对比与校准就是为了解决这个排序一致性问题。

---

## **Phase 3：小步 PPO 试炼 \+ 扩量开闸（严格 gating）**

1. **5k steps 本地小步PPO**（MPS不够就切3090云）：起始配置建议 `lr=1e-5, clip=0.2, kl_coef=0.02, bs=32, rollout_len=128`，评测HotpotQA/StrategyQA/GSM8K 的成功率、平均轮数、过度澄清率；门槛：成功率（需问类）≥+5pp、过度澄清率≤基线-20%、平均轮数不升。  
2. **扩量生成 gating**：只有 Phase 1/2 全 PASS 才能从1000→5000，并在每次扩量后复跑“多样性+稳定性”报告。

训练=PyTorch/TRL，推理/离线评测=llama.cpp+GGUF；不要把llama.cpp用于在线采样/RL回合（需要logits/grad）。

---

## **（可选但强烈建议）RC1 前“二轮预检 \+ 种子池固化 \+ 难度体检”**

把下面这套“一键预检”接进来，保你起跑稳：包含 **影子集物化、30k seed pool、泄漏/多样性体检、打分器反模拟闸门、权重文件校验、磁盘余量、Round1→Round2聚合**；`reports/preflight/round2_pass.json` 存在才允许启动训练。脚本与阈值一应俱全（这里不重复粘贴命令）。  
此外新增 **“数据难度核查与难度采样”**：计算 `len_tokens/turns/tool_hops/ops_numeric/connector_density/...`，按 Easy/Med/Hard 分桶，平衡到 **Hard 30–40%**、Medium 40–50%、Easy 20–30%，并把难度接入采样与奖励加权；RC1 报告需给出按难度拆分的成功率并要求 Hard 桶提升≥+5pp。

---

## **已知坑位提醒（针对你日志里的风险点）**

* **评分器“2ms/对话&100%命中”**：这是stub迹象，必须打开 `live_mode`，做K=3+median+variance，并实现TTL/版本失效与高方差重评。  
* **并发与限流**：固定并发=5只是起点，要按429比例自适应2↔8，并把限流指标写进监控。  
* **权重文件**：`weights.json` 必须非负、归一、单维≤0.5，否则阻断训练。  
* **评测一致性偏低**：优先修 `shadow_run` 脚本的数据读取与字段映射（优先物化数据，不覆盖HF），并校对HotpotQA/GSM8K/StrategyQA 的分层与成功标签定义。

---

## **资源与部署建议（便于你切云或本地加速）**

* 本地推理首选 **llama.cpp+GGUF**（跨平台、维护成本低）；训练仍用 PyTorch/TRL。  
* 需要速度时再上低价云（Runpod/Vast 等），4B级别单卡24GB足够。  
* 量化/MLX 路线作为补充优化（Mac本地推理吞吐显著高于PyTorch MPS）。

---

# **一句话裁决**

你的基础设施已达标，现在**直接按上面的 Phase 1→2→3 跑**。任何一个门槛没过，都**禁止扩量/开训**，先补齐：

* 先补 **多样性与真实评分**，  
* 再做 **影子对比+权重校准+过度澄清惩罚**，  
* 全部 PASS 后再开 **小步PPO与扩量**。

需要我把这些命令打成 `make` 目标或 `ops/*.sh` 一键脚本，也可以现在就给你生成。

