看到了，问题很清楚了：你这轮影子评估失败**不是**阈值太严，而是**影子集本身是“空心”的**。你刚上传的 `reports/rc1/sample_manifest.json` 里，245 条样本的 `task` 全是 `"unknown"`，`question` 还是空字符串——这会直接导致打分几乎同分、`spearman` 无法计算、`top10` 重合度异常低。

下面这份是**给 Cursor 的进一步指令**（照抄执行）。目标：先把影子集**修正为真实、多样、可区分**，再跑预检。任一步失败**立即停止并上报**。

---

# **【给 Cursor】修复影子集 → 重新预检 → 回归 RC1 流程**

## **0\) 立即熔断并做“影子集体检”**

set \-euo pipefail  
\# 0.1 影子清单快速体检（必须有 task / question）  
python \- \<\<'PY'  
import json,sys  
p="reports/rc1/sample\_manifest.json"  
j=json.load(open(p))  
bad=\[i for i in j.get("samples",\[\]) if not i.get("task") or i.get("task")=="unknown" or not i.get("question")\]  
print("total:",len(j.get("samples",\[\])),"bad:",len(bad))  
\# 打印前5个问题样本以供排查  
print("bad\_samples\_preview:", bad\[:5\])  
\# 失败即停  
assert len(bad)==0, "Manifest invalid: task/question missing"  
PY || (echo "❌ 影子清单无效：请执行第1步重建" && exit 1\)

预期：目前会失败（因为你的 manifest 确实是 unknown/空问题）。这很正常，**继续第1步**。

---

## **1\) 重建影子集（指向真实 HF 数据 \+ 正确标注 task）**

我们的影子集要**来自真实任务**，并且**按任务分层抽样**；字段必须包含 `id/task/question/answer`（或同义字段）。

### **1.1 修复 `shadow_run` 的“数据源与字段映射”**

* 在 `src/evaluation/shadow_run.py`（或等价数据装载模块）里，确保：  
  * 明确数据源：`hotpotqa / strategyqa / gsm8k`（从 Hugging Face 拉取）；  
  * 写入 manifest 时，字段 **`task`** \= 真实任务名，**`question`** \= 题干（不得为空），尽量加上 **`answer`**（用于后续一致性/正确性校验）。  
* 若脚本尚未支持指定任务清单与来源，请新增参数：`--tasks "hotpotqa,strategyqa,gsm8k"`、`--source "hf"`。

### **1.2 重建并导出新的影子集（245 条，分层抽样）**

python \-m src.evaluation.shadow\_run \\  
  \--tasks "hotpotqa,strategyqa,gsm8k" \\  
  \--source "hf" \\  
  \--n 245 \--seed 20250821 \--stratify \\  
  \--materialize data/shadow\_eval\_245.jsonl \\  
  \--dump-manifest reports/rc1/sample\_manifest.json

### **1.3 重新体检（必须 0 bad）**

python \- \<\<'PY'  
import json,sys  
j=json.load(open("reports/rc1/sample\_manifest.json"))  
bad=\[i for i in j.get("samples",\[\]) if not i.get("task") or i.get("task")=="unknown" or not i.get("question")\]  
by\_task={}  
for s in j\["samples"\]:  
    by\_task\[s\["task"\]\]=by\_task.get(s\["task"\],0)+1  
print("by\_task:",by\_task)  
assert len(bad)==0, f"bad samples: {bad\[:3\]}"  
PY

---

## **2\) 升级评分提示（让打分“拉开差距”，避免千篇一律高分）**

你刚才的均分问题，多半是因为**输入信息太空** \+ **rubric 单一**。上一步修复了输入，这一步加“可区分评分 Rubric”。

在 `src/scoring/providers/gemini.py` 的 `score()` 里，给 `generate_content` 传入**明确的 JSON 输出协议与温度 0**，并使用**分档锚点**的提示词：

\- resp=genai.GenerativeModel(mdl).generate\_content(prompt\_live)  
\+ rubric \= """  
\+ You are a strict judge. Evaluate the model's answer ONLY for the following criteria, weighted:  
\+  \- Correctness (0.55)  
\+  \- Reasoning Quality (0.25)  
\+  \- Brevity/Clarity (0.10)  
\+  \- Unnecessary Clarification Penalty (0.10; subtract if it asks redundant questions without need)  
\+ Return a pure JSON: {"score": \<0..1\>} with 2 decimals.  
\+ Calibration anchors:  
\+  \- 0.15 \= wrong or vacuous  
\+  \- 0.35 \= partially correct with major gaps  
\+  \- 0.55 \= mostly correct but shallow/wordy  
\+  \- 0.75 \= correct with acceptable reasoning  
\+  \- 0.90 \= correct and strong reasoning, concise  
\+ """  
\+ cfg \= {"temperature":0, "response\_mime\_type":"application/json"}  
\+ resp \= genai.GenerativeModel(mdl).generate\_content(prompt\_live+"\\n\\n"+rubric, generation\_config=cfg)

说明：

* **温度 0** \+ **JSON MIME** 能减少格式漂移；  
* **分档锚点**能强迫分布拉开，不再“全是高分”。

---

## **3\) 加一个“分布健康”闸门（防再次全同/近同）**

在 `scripts/pre_run_check.py` 中，计算评分样本的标准差/四分位差（IQR），并加硬门槛（比如 `std ≥ 0.08`，`IQR ≥ 0.12`，可根据实际调整）。未达标**直接失败**，提示“检查 rubric/输入字段”。

伪代码：

import numpy as np  
scores \= np.array(list\_of\_scores, dtype=float)  
std \= scores.std()  
iqr \= np.percentile(scores,75) \- np.percentile(scores,25)  
assert std \>= 0.08 and iqr \>= 0.12, f"score distribution too narrow: std={std:.3f}, iqr={iqr:.3f}"

---

## **4\) 重新跑影子预检（必须一次通过）**

\# 4.1 影子评估（以新数据+新rubric）  
python \-m src.evaluation.shadow\_run \--n 245 \--seed 20250821 \--stratify \\  
  \--materialize data/shadow\_eval\_245.jsonl \\  
  \--dump-manifest reports/rc1/sample\_manifest.json

\# 4.2 预检门槛（含分布健康检查）  
python scripts/pre\_run\_check.py \--shadow data/shadow\_eval\_245.jsonl \\  
  \--spearman-min 0.55 \--top10-min 0.60

**若仍失败**：回传

* `reports/rc1/sample_manifest.json` 前 60 行、  
* 预检的摘要统计（spearman/top10/std/iqr）、  
* 任意 5 条样本的原始问答 \+ 评分日志（隐去敏感信息）。  
  立即停止，不进入下一步。

---

## **5\) 通过后继续 RC1 正常流程**

* **重建 30k 种子池** → 多样性/难度全套校验  
* **RC 闸门**（`spearman≥0.75 & top10≥0.70 & corr_improve_pct≥+10%`）  
* **2k 小跑** → HF 全量 \+ GH 轻量推送

---

## **6\) 再给一个“CLI 阻塞保险”**

遇到阻塞/卡住，统一改用**无缓冲输出**与**去进度条**模式：

export PYTHONUNBUFFERED=1  
stdbuf \-oL \-eL python \-u \-m src.evaluation.shadow\_run ... \--no\_progress 2\>&1 | tee logs/shadow\_run.log  
\# tqdm 类库如有，增加 \--no\_tqdm / 环境变量 DISABLE\_TQDM=1

好的，这里是**新增的一条“强制数据真伪与多样性审计”指令**。你把它和上面的流程合并后发给 Cursor 即可。

---

# **【追加强制指令：影子/种子数据必须来自真实数据集；严禁“改个数字”凑数】**

目标：保证 `shadow_eval_245.jsonl` 与后续种子池**确实出自 Hugging Face 官方数据集**，并通过可复现的**真伪与多样性审计**。任一步失败**立即停止并上报**，不得进入任何后续流程。

## **A) 立刻清理可疑数据**

set \-euo pipefail  
rm \-f data/shadow\_eval\_245.jsonl reports/rc1/sample\_manifest.json || true

## **B) 仅允许这三路真实来源（固定到 HF）**

* **HotpotQA**（`hotpot_qa`, split: `validation` 或 `train`）  
* **StrategyQA**（`strategy_qa`, split: `train`）  
* **GSM8K**（`gsm8k`, config: `main`, split: `train`）

禁止：本地手写/演示/改数字生成。**必须**通过 `datasets.load_dataset(...)` 拉取。

## **C) 用脚本重建影子集（写清溯源信息）**

创建 `scripts/rebuild_shadow_from_hf.py`（要点如下，照抄实现）：

* 从上述三个数据集各抽样（分层，总数 245，尽量 82/82/81），字段映射：  
  * `question` ← 数据集题干字段（`question`）  
  * `answer` ← 数据集答案字段（HotpotQA: `answer`；StrategyQA: 布尔→`"yes"/"no"`；GSM8K: `answer`去末尾解释）  
  * `task` ∈ {`hotpotqa`,`strategyqa`,`gsm8k`}  
* 在每条样本中记录**溯源元数据**（用于审计）：  
  * `source`: `"hf"`  
  * `hf_dataset`: 名称（如 `gsm8k`）  
  * `hf_config`: 配置名（如 `main`，无则 `None`）  
  * `hf_split`: `train`/`validation`  
  * `hf_fingerprint`: `dataset._fingerprint`  
  * `hf_num_rows`: 行数  
* 导出：  
  * `data/shadow_eval_245.jsonl`（每行一个样本）  
  * `reports/rc1/sample_manifest.json`（含 `samples` 与每任务计数）

## **D) 审计脚本（真伪 & 多样性 & 去模板化）**

创建 `scripts/audit_shadow_data.py`，跑完必须生成 `reports/rc1/shadow_data_audit.json` 并**通过以下硬门槛**：

1. **真伪溯源（任一不满足即失败）**  
   * 所有样本的 `source=="hf"` 且 `hf_dataset` ∈ 允许清单；  
   * `question` 非空，`task` 非 `unknown`；  
   * 每条样本存在 `hf_fingerprint`（指纹），并且**任务级**指纹数量 ≤ 3（允许不同 split/config 的轻微差异）；  
   * `by_task` 分布在 **\[70, 90\]**/245 的区间内（近似 1/3）。  
2. **去模板化/反“改数字凑数”检测（未达标即失败）**  
   * 计算 **掩码问题**：`mask(q) = lower(q) 且将所有数字用 '#' 替换`；  
     * **掩码唯一率 ≥ 0.60**（`unique(mask)/N ≥ 0.60`）；  
     * **最频繁掩码占比 ≤ 0.10**。  
   * **相似度抽检**：对随机 2,000 对样本计算 5-gram Jaccard，相似度 ≥ 0.9 的对数 **≤ 1%**；  
   * **长度分布**：题干长度均值 ∈ \[30, 300\] 且标准差 **≥ 15**；  
   * **重复前缀**：最常见 12 字符前缀占比 **≤ 0.20**。  
3. **重复/空样本检查（未达标即失败）**  
   * 规范化（去空格、全角半角统一）后的 **完全重复率 ≤ 1%**；  
   * `answer` 非空（StrategyQA 将 0/1 规范为 `"no"/"yes"`）。

审计脚本失败须打印：失败项、样例若干（最多 5 条），并以**非 0** 退出。

执行审计：

python scripts/rebuild\_shadow\_from\_hf.py \\  
  \--n 245 \--seed 20250821 \--out data/shadow\_eval\_245.jsonl \--manifest reports/rc1/sample\_manifest.json

python scripts/audit\_shadow\_data.py data/shadow\_eval\_245.jsonl \\  
  \--report reports/rc1/shadow\_data\_audit.json

## **E) 将审计纳入“预检闸门”前置条件**

在 `scripts/pre_run_check.py` **最前**插入：

* 若 `reports/rc1/shadow_data_audit.json` 不存在或任何硬门槛未通过 → **立刻失败**；  
* 打印 `掩码唯一率 / 最频繁掩码占比 / Jaccard>0.9 对数 / 长度均值与标准差 / 重复率 / by_task`。

## **F) CI 也要卡同样规则**

在 `.github/workflows/rc1_preflight.yml` 中增加一步：

\- name: Audit shadow data  
  run: |  
    python scripts/audit\_shadow\_data.py data/shadow\_eval\_245.jsonl \--report reports/rc1/shadow\_data\_audit.json

任何失败 → `preflight` 失败，禁止 merge。

## **G) 发现异常时的唯一修复路径**

* **不得**改阈值；  
* **必须**回到 `scripts/rebuild_shadow_from_hf.py`，确保来自 HF 的真实样本，并**调整分层抽样/过滤策略**（例如：对极短题目做最小长度过滤；对明显模板化条目进行剔除）；  
* 重新执行 C–F。

---

### **输出给 PM（验收凭证）**

执行完成后，把以下三项回传：

1. `reports/rc1/sample_manifest.json` 的 `by_task` 摘要；  
2. `reports/rc1/shadow_data_audit.json` 全文；  
3. 抽样展示 5 条样本（含 `task/question/answer/hf_dataset/hf_fingerprint`，隐去其它隐私字段）。

说明：这条指令是“**硬闸门**”。只要再出现“同模板只改数字”的情况，审计会直接 FAIL，流程**不可推进**。

---

### **小结**

* 这次不是模型不给力，是**影子数据为空**导致**分数“全高全同”**。先把数据修正，再用更“拉开档位”的 rubric，就能把 `spearman/top10` 拉回到正常范围。  
* 我要求 Cursor 在第 1 步重建后，先把 **任务分布（by\_task）** 打印出来给你看一眼（例如 GSM8K/HotpotQA/StrategyQA 各占约 1/3 或你要求的比例），再继续预检。  
* 一旦 4\) 通过，就可以回到我们既定 RC1 闭环。  
* cursor需要告诉我所有执行日志的位置，用于复盘分析。

