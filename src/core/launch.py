#!/usr/bin/env python3
import argparse
import json
import time
import sys
import os
from pathlib import Path
from typing import Dict, Any
import torch

from src.config.loader import load_config
from src.engines.trl_ppo import TRLPPOEngine
from src.strategies.ppo import PPOStrategy
from src.data.readers.jsonl_reader import JSONLReader
from src.data.collate.default import DefaultCollator
from src.core.checkpoint import save_ckpt, load_ckpt
from src.runtime.device import get_device

def fatal_error(msg: str, logs_dir: str = "logs") -> None:
    """è‡´å‘½é”™è¯¯å¤„ç†"""
    print(f"FATAL: {msg}")
    
    # æ”¶é›†æœ€è¿‘200è¡Œæ—¥å¿—
    log_file = f"{logs_dir}/train.log"
    recent_logs = []
    if os.path.exists(log_file):
        with open(log_file, 'r') as f:
            lines = f.readlines()
            recent_logs = lines[-200:] if len(lines) > 200 else lines
    
    # å†™å…¥é”™è¯¯æŠ¥å‘Š
    error_report = {
        'error': msg,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'recent_logs': recent_logs
    }
    
    os.makedirs('reports', exist_ok=True)
    error_file = f"reports/ERROR_{int(time.time())}.json"
    with open(error_file, 'w') as f:
        json.dump(error_report, f, indent=2, ensure_ascii=False)
    
    print(f"âŒ é”™è¯¯æŠ¥å‘Šå·²å†™å…¥: {error_file}")
    sys.exit(1)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--resume', help='æ¢å¤è®­ç»ƒçš„checkpointè·¯å¾„')
    args = parser.parse_args()
    
    try:
        # åŠ è½½é…ç½®
        config = load_config(args.config)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(config['run']['output_dir'])
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        log_file = config['run']['log_file']
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        
        # è®¾ç½®è®¾å¤‡
        device = get_device()
        print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åˆå§‹åŒ–å¼•æ“å’Œç­–ç•¥
        engine = TRLPPOEngine()
        strategy = PPOStrategy()
        
        # è®¾ç½®å¼•æ“
        engine.setup(config)
        strategy.attach_engine(engine)
        
        # åŠ è½½æ•°æ®
        train_reader = JSONLReader(config['data']['train_path'])
        eval_reader = JSONLReader(config['data']['eval_path'])
        
        train_samples = train_reader.read_samples()
        eval_samples = eval_reader.read_samples()
        
        print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬: {len(train_samples)}, è¯„æµ‹æ ·æœ¬: {len(eval_samples)}")
        
        # åˆ›å»ºæ•°æ®æ•´ç†å™¨
        collator = DefaultCollator(
            engine.tokenizer, 
            config['data']['max_length']
        )
        
        # æ¢å¤è®­ç»ƒçŠ¶æ€
        start_step = 0
        if args.resume:
            print(f"ğŸ”„ ä»checkpointæ¢å¤: {args.resume}")
            state = load_ckpt(args.resume)
            start_step = state.get('step', 0)
            engine.load_state_dict(state)
            print(f"âœ… å·²æ¢å¤åˆ°æ­¥éª¤: {start_step}")
        
        # è®­ç»ƒå¾ªç¯
        max_steps = config['run']['max_steps']
        save_steps = config['run']['save_steps']
        eval_steps = config['run']['eval_steps']
        
        print(f"ğŸ¯ å¼€å§‹è®­ç»ƒï¼Œç›®æ ‡æ­¥æ•°: {max_steps}")
        
        for step in range(start_step, max_steps):
            try:
                # å‡†å¤‡batch
                batch_size = config['data']['batch_size']
                batch_samples = train_samples[step % len(train_samples):(step % len(train_samples)) + batch_size]
                if len(batch_samples) < batch_size:
                    batch_samples.extend(train_samples[:batch_size - len(batch_samples)])
                
                batch = collator(batch_samples)
                
                # è®­ç»ƒæ­¥éª¤
                metrics = strategy.on_batch(batch)
                
                # è®°å½•æŒ‡æ ‡
                metrics['step'] = step
                metrics['lr'] = metrics.get('lr', 0.0)
                metrics['mem_used_mb'] = torch.cuda.memory_allocated() / 1024 / 1024 if torch.cuda.is_available() else 0
                metrics['sec_per_step'] = time.time()
                
                # å†™å…¥metrics.jsonl
                os.makedirs('reports', exist_ok=True)
                with open('reports/metrics.jsonl', 'a') as f:
                    f.write(json.dumps(metrics) + '\n')
                
                # æ‰“å°è¿›åº¦
                if step % 10 == 0:
                    print(f"Step {step}/{max_steps}: loss={metrics.get('loss', 0):.4f}, kl={metrics.get('kl', 0):.4f}")
                
                # ä¿å­˜checkpoint
                if step > 0 and step % save_steps == 0:
                    ckpt_path = output_dir / f"step_{step:06d}"
                    # è¿‡æ»¤æ‰ä¸å¯åºåˆ—åŒ–çš„é…ç½®
                    safe_config = {}
                    for key, value in config.items():
                        try:
                            json.dumps(value)
                            safe_config[key] = value
                        except (TypeError, ValueError):
                            safe_config[key] = str(value)
                    
                    state = {
                        'step': step,
                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                        'config': safe_config,
                        **engine.state_dict()
                    }
                    save_ckpt(state, str(ckpt_path))
                    
                    # æ›´æ–°latesté“¾æ¥
                    latest_link = output_dir / "latest"
                    if latest_link.exists():
                        latest_link.unlink()
                    latest_link.symlink_to(ckpt_path.name)
                
                # è¯„ä¼°
                if step > 0 and step % eval_steps == 0:
                    print(f"ğŸ” æ­¥éª¤ {step} è¯„ä¼°ä¸­...")
                    eval_batch = collator(eval_samples[:batch_size])
                    eval_metrics = engine.eval_step(eval_batch)
                    print(f"ğŸ“Š è¯„ä¼°ç»“æœ: {eval_metrics}")
                
            except Exception as e:
                fatal_error(f"è®­ç»ƒæ­¥éª¤ {step} å¤±è´¥: {str(e)}", "logs")
        
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        
    except Exception as e:
        fatal_error(f"è®­ç»ƒå¯åŠ¨å¤±è´¥: {str(e)}", "logs")

if __name__ == "__main__":
    main()
