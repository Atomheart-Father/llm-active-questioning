#!/usr/bin/env python3
"""
Weight Calibration - æƒé‡æ ¡å‡†ç³»ç»Ÿ
éè´Ÿæœ€å°äºŒä¹˜ + L2å…ˆéªŒ + äº¤å‰éªŒè¯ + Bootstrap
"""

import argparse
import json
import time
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Tuple
import logging
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error
from scipy.stats import spearmanr
from scipy.optimize import nnls
import sys
import yaml

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.evaluation.advanced_reward_system import MultiDimensionalRewardSystem
from src.evaluation.shadow_run import ShadowRunEvaluator

logger = logging.getLogger(__name__)

class WeightCalibrator:
    """æƒé‡æ ¡å‡†å™¨
    
    å®ç°éè´Ÿæœ€å°äºŒä¹˜ + L2å…ˆéªŒæ­£åˆ™åŒ–çš„æƒé‡å­¦ä¹ 
    """
    
    def __init__(self, config_path: str = "configs/default_config.yaml"):
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.calib_config = self.config.get("calibration", {})
        self.lambda_reg = self.calib_config.get("l2_reg", 0.1)
        self.cv_folds = self.calib_config.get("cv_folds", 5)
        self.bootstraps = self.calib_config.get("bootstraps", 200)
        self.random_seed = self.calib_config.get("random_seed", 42)
        
        # è®¾ç½®éšæœºç§å­
        np.random.seed(self.random_seed)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path("reports").mkdir(exist_ok=True)
        Path("configs").mkdir(exist_ok=True)
        
        logger.info(f"æƒé‡æ ¡å‡†å™¨åˆå§‹åŒ–: Î»={self.lambda_reg}, CV={self.cv_folds}, Bootstrap={self.bootstraps}")
    
    def load_shadow_run_data(self, shadow_file: str = "reports/shadow_run_20250820.json") -> pd.DataFrame:
        """åŠ è½½shadow runæ•°æ®"""
        # è§£æshadowæ–‡ä»¶è·¯å¾„
        from glob import glob
        import os
        
        def resolve_shadow_file(p):
            if p and p != "latest":
                return p
            files = sorted(glob("reports/shadow_run_*.json"), key=os.path.getmtime)
            if not files:
                raise FileNotFoundError("No shadow_run_*.json under reports/")
            return files[-1]
        
        # ä½¿ç”¨è§£æåçš„è·¯å¾„
        resolved_file = resolve_shadow_file(shadow_file)
        with open(resolved_file, 'r', encoding='utf-8') as f:
            shadow_data = json.load(f)
        
        # ä»è¯Šæ–­ä¿¡æ¯ä¸­é‡å»ºDataFrame
        sample_manifest = shadow_data["diagnostics"]["sample_manifest"]["samples"]
        
        # é‡æ–°ç”Ÿæˆå’Œè¯„ä¼°æ ·æœ¬ä»¥è·å–å®Œæ•´ç‰¹å¾
        evaluator = ShadowRunEvaluator()
        samples = evaluator.load_or_generate_sample_data(
            len(sample_manifest), 
            shadow_data["metadata"]["seed"]
        )
        
        reward_system = MultiDimensionalRewardSystem()
        
        feature_data = []
        for i, sample in enumerate(samples):
            # ä»»åŠ¡æˆåŠŸæ ‡ç­¾ - å¼•å…¥ä¸€äº›å¤±è´¥æ ·æœ¬ç”¨äºæ ¡å‡†
            base_task_success = evaluator._compute_task_success(sample)
            
            # æ–°ç³»ç»Ÿè¯¦ç»†è¯„ä¼°
            new_result = reward_system.evaluate_dialogue(sample)
            
            # åŸºäºå¥–åŠ±åˆ†æ•°æ¨¡æ‹ŸæˆåŠŸç‡ (é¿å…å…¨ä¸º1çš„é—®é¢˜)
            primary_reward = new_result["primary_reward"]
            # ä½¿ç”¨sigmoidå‡½æ•°å°†å¥–åŠ±æ˜ å°„åˆ°æˆåŠŸæ¦‚ç‡
            success_prob = 1 / (1 + np.exp(-10 * (primary_reward - 0.7)))  # é˜ˆå€¼0.7
            
            # åŠ å…¥ä¸€äº›éšæœºæ€§
            np.random.seed(hash(sample["id"]) % 2**32)  # åŸºäºIDçš„ç¡®å®šæ€§éšæœº
            task_success = 1 if np.random.random() < success_prob else 0
            
            # ç¡®ä¿è‡³å°‘30%çš„æ ·æœ¬å¤±è´¥ï¼Œ70%æˆåŠŸ
            if i % 10 < 3:  # æ¯10ä¸ªæ ·æœ¬ä¸­æœ‰3ä¸ªå¤±è´¥
                task_success = 0
            elif i % 10 >= 7:  # æ¯10ä¸ªæ ·æœ¬ä¸­æœ‰3ä¸ªæˆåŠŸ
                task_success = 1
            
            # æå–ç‰¹å¾ (ä¸å«è¿‡åº¦æ¾„æ¸…æƒ©ç½š)
            component_scores = new_result["component_scores"]
            hard_rules = new_result["hard_rules"]
            
            features = {
                "sample_id": sample["id"],
                "task_type": sample.get("task_type", "unknown"),
                "task_success": task_success,
                "variance": new_result.get("meta", {}).get("variance", 0.0),
                # æ–°å¥–åŠ±å­é¡¹
                "logic_rigor": component_scores["logic_rigor"],
                "question_quality": component_scores["question_quality"], 
                "reasoning_completeness": component_scores["reasoning_completeness"],
                "natural_interaction": component_scores["natural_interaction"],
                "rules_score": hard_rules["rules_score"],
                "step_count": hard_rules["metrics"]["step_count"],
                "format_score": hard_rules["metrics"]["format_score"],
                "primary_reward": new_result["primary_reward"]
            }
            
            feature_data.append(features)
        
        df = pd.DataFrame(feature_data)
        logger.info(f"åŠ è½½äº†{len(df)}ä¸ªæ ·æœ¬çš„ç‰¹å¾æ•°æ®")
        
        return df
    
    def prepare_features_and_labels(self, df: pd.DataFrame, use_stable_only: bool = True) -> Tuple[np.ndarray, np.ndarray, List[str], pd.DataFrame]:
        """å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾"""
        # é€‰æ‹©ç¨³å®šæ ·æœ¬ (å¯é€‰)
        if use_stable_only:
            stable_mask = df['variance'] <= 0.08
            if stable_mask.sum() < 50:  # è‡³å°‘ä¿ç•™50ä¸ªæ ·æœ¬
                logger.warning(f"ç¨³å®šæ ·æœ¬å¤ªå°‘({stable_mask.sum()})ï¼Œä½¿ç”¨å…¨éƒ¨æ ·æœ¬")
                work_df = df.copy()
            else:
                work_df = df[stable_mask].copy()
                logger.info(f"ä½¿ç”¨{len(work_df)}ä¸ªç¨³å®šæ ·æœ¬è¿›è¡Œæ ¡å‡†")
        else:
            work_df = df.copy()
        
        # ç‰¹å¾åˆ—
        feature_columns = [
            "logic_rigor",
            "question_quality", 
            "reasoning_completeness",
            "natural_interaction",
            "rules_score",
            "step_count",
            "format_score"
        ]
        
        # æå–ç‰¹å¾çŸ©é˜µ
        X = work_df[feature_columns].values
        
        # æ ‡ç­¾ (ä»»åŠ¡æˆåŠŸ)
        y = work_df["task_success"].values
        
        # ç‰¹å¾å½’ä¸€åŒ–åˆ°[0,1]
        scaler = MinMaxScaler()
        X_scaled = scaler.fit_transform(X)
        
        # è¾“å‡ºç‰¹å¾ç»Ÿè®¡
        feature_stats = {}
        for i, col in enumerate(feature_columns):
            feature_stats[col] = {
                "mean": float(X_scaled[:, i].mean()),
                "std": float(X_scaled[:, i].std()),
                "min": float(X_scaled[:, i].min()),
                "max": float(X_scaled[:, i].max())
            }
        
        logger.info("ç‰¹å¾å½’ä¸€åŒ–å®Œæˆ")
        for col, stats in feature_stats.items():
            if stats["std"] > 3:  # æ£€æŸ¥å¼‚å¸¸å€¼
                logger.warning(f"ç‰¹å¾{col}æ–¹å·®è¿‡å¤§: {stats}")
        
        return X_scaled, y, feature_columns, work_df
    
    def get_prior_weights(self, feature_columns: List[str]) -> np.ndarray:
        """è·å–å…ˆéªŒæƒé‡"""
        # å°è¯•ä»ç°æœ‰weights.jsonè¯»å–
        weights_file = Path("configs/weights.json")
        if weights_file.exists():
            try:
                from .weights_loader import load_weights
                prior_dict = load_weights(str(weights_file))
            except Exception:
                # Fallback to old format
                with open(weights_file, 'r', encoding='utf-8') as f:
                    weights_data = json.load(f)
                prior_dict = weights_data.get("weights", {}) if isinstance(weights_data, dict) else {}
            w_prior = []
            for col in feature_columns:
                # æ˜ å°„å­—æ®µå
                if col in prior_dict:
                    w_prior.append(prior_dict[col])
                elif col == "logic_rigor" and "logic_rigor" in prior_dict:
                    w_prior.append(prior_dict["logic_rigor"])
                elif col == "rules_score" and "rules" in prior_dict:
                    w_prior.append(prior_dict["rules"])
                else:
                    w_prior.append(1.0 / len(feature_columns))  # å‡åŒ€é»˜è®¤
            
            w_prior = np.array(w_prior)
            logger.info(f"ä»weights.jsonåŠ è½½å…ˆéªŒæƒé‡: {dict(zip(feature_columns, w_prior))}")
        else:
            # å‡åŒ€å…ˆéªŒ
            w_prior = np.ones(len(feature_columns)) / len(feature_columns)
            logger.info(f"ä½¿ç”¨å‡åŒ€å…ˆéªŒæƒé‡: {w_prior}")
        
        return w_prior
    
    def fit_nnls_with_prior(self, X: np.ndarray, y: np.ndarray, w_prior: np.ndarray, lambda_reg: float) -> np.ndarray:
        """éè´Ÿæœ€å°äºŒä¹˜ + L2å…ˆéªŒæ­£åˆ™åŒ–"""
        n_features = X.shape[1]
        
        # å¢å¹¿ç³»ç»Ÿ: [X; sqrt(Î»)*I] w = [y; sqrt(Î»)*w_prior]
        sqrt_lambda = np.sqrt(lambda_reg)
        X_aug = np.vstack([X, sqrt_lambda * np.eye(n_features)])
        y_aug = np.concatenate([y, sqrt_lambda * w_prior])
        
        # éè´Ÿæœ€å°äºŒä¹˜æ±‚è§£
        w_fit, residual = nnls(X_aug, y_aug)
        
        return w_fit
    
    def adaptive_regularization(self, X: np.ndarray, y: np.ndarray, w_prior: np.ndarray, 
                              max_weight_ratio: float = 0.5, max_iterations: int = 3) -> Tuple[np.ndarray, float]:
        """è‡ªé€‚åº”æ­£åˆ™åŒ–ï¼Œæ§åˆ¶å•ç»´æƒé‡ä¸Šé™"""
        lambda_reg = self.lambda_reg
        
        for iteration in range(max_iterations):
            w_fit = self.fit_nnls_with_prior(X, y, w_prior, lambda_reg)
            
            # å½’ä¸€åŒ–æƒé‡
            w_normalized = w_fit / w_fit.sum() if w_fit.sum() > 0 else w_fit
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æƒé‡è¶…è¿‡é˜ˆå€¼
            max_weight = w_normalized.max()
            if max_weight <= max_weight_ratio:
                logger.info(f"æƒé‡æ”¶æ•›äºÎ»={lambda_reg:.3f}, max_weight={max_weight:.3f}")
                return w_normalized, lambda_reg
            
            # å¢åŠ æ­£åˆ™åŒ–å¼ºåº¦
            lambda_reg *= 2
            logger.info(f"è¿­ä»£{iteration+1}: max_weight={max_weight:.3f} > {max_weight_ratio}, å¢åŠ Î»åˆ°{lambda_reg:.3f}")
        
        # æœ€åæ‰‹åŠ¨æŠ•å½±åˆ°çº¦æŸèŒƒå›´
        w_normalized = np.minimum(w_normalized, max_weight_ratio)
        w_normalized = w_normalized / w_normalized.sum()
        
        logger.warning(f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œä½¿ç”¨è½¯æŠ•å½±: max_weight={w_normalized.max():.3f}")
        return w_normalized, lambda_reg
    
    def cross_validation(self, X: np.ndarray, y: np.ndarray, task_types: np.ndarray, 
                        w_prior: np.ndarray, lambda_reg: float) -> Dict[str, Any]:
        """åˆ†å±‚äº¤å‰éªŒè¯"""
        cv_results = {
            "rank_corr": [],
            "mae": [],
            "auc": []
        }
        
        # åˆ†å±‚KæŠ˜
        skf = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_seed)
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, task_types)):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            # æ‹Ÿåˆæƒé‡
            w_fold = self.fit_nnls_with_prior(X_train, y_train, w_prior, lambda_reg)
            w_fold = w_fold / w_fold.sum() if w_fold.sum() > 0 else w_fold
            
            # éªŒè¯é›†é¢„æµ‹
            y_pred = X_val @ w_fold
            
            # è®¡ç®—æŒ‡æ ‡
            if len(np.unique(y_val)) > 1:  # æœ‰å˜åŒ–æ‰èƒ½è®¡ç®—ç›¸å…³æ€§
                rank_corr, _ = spearmanr(y_pred, y_val)
                cv_results["rank_corr"].append(rank_corr or 0.0)
            else:
                cv_results["rank_corr"].append(0.0)
            
            mae = mean_absolute_error(y_val, y_pred)
            cv_results["mae"].append(mae)
            
            # AUC (å¦‚æœæœ‰äºŒåˆ†ç±»å˜åŒ–)
            if len(np.unique(y_val)) == 2:
                from sklearn.metrics import roc_auc_score
                try:
                    auc = roc_auc_score(y_val, y_pred)
                    cv_results["auc"].append(auc)
                except:
                    cv_results["auc"].append(0.5)
            else:
                cv_results["auc"].append(0.5)
            
            logger.debug(f"Fold {fold+1}: rank_corr={cv_results['rank_corr'][-1]:.4f}, mae={cv_results['mae'][-1]:.4f}")
        
        # æ±‡æ€»ç»Ÿè®¡
        cv_summary = {}
        for metric, values in cv_results.items():
            cv_summary[f"{metric}_mean"] = np.mean(values)
            cv_summary[f"{metric}_std"] = np.std(values)
            cv_summary[f"{metric}_median"] = np.median(values)
            cv_summary[f"{metric}_ci95"] = [
                np.percentile(values, 2.5),
                np.percentile(values, 97.5)
            ]
        
        logger.info(f"CVç»“æœ: rank_corr={cv_summary['rank_corr_mean']:.4f}Â±{cv_summary['rank_corr_std']:.4f}")
        
        return cv_summary
    
    def bootstrap_evaluation(self, X: np.ndarray, y: np.ndarray, w_fitted: np.ndarray) -> Dict[str, Any]:
        """Bootstrapè¯„ä¼°"""
        n_samples = len(X)
        bootstrap_results = {
            "rank_corr": [],
            "mae": [],
        }
        
        for i in range(self.bootstraps):
            # Bootstrapé‡‡æ ·
            boot_idx = np.random.choice(n_samples, size=n_samples, replace=True)
            X_boot = X[boot_idx]
            y_boot = y[boot_idx]
            
            # é¢„æµ‹
            y_pred_boot = X_boot @ w_fitted
            
            # è®¡ç®—æŒ‡æ ‡
            if len(np.unique(y_boot)) > 1:
                rank_corr, _ = spearmanr(y_pred_boot, y_boot)
                bootstrap_results["rank_corr"].append(rank_corr or 0.0)
            else:
                bootstrap_results["rank_corr"].append(0.0)
            
            mae = mean_absolute_error(y_boot, y_pred_boot)
            bootstrap_results["mae"].append(mae)
        
        # æ±‡æ€»ç»Ÿè®¡
        bootstrap_summary = {}
        for metric, values in bootstrap_results.items():
            bootstrap_summary[f"{metric}_mean"] = np.mean(values)
            bootstrap_summary[f"{metric}_std"] = np.std(values)
            bootstrap_summary[f"{metric}_ci95"] = [
                np.percentile(values, 2.5),
                np.percentile(values, 97.5)
            ]
        
        logger.info(f"Bootstrapç»“æœ: rank_corr={bootstrap_summary['rank_corr_mean']:.4f}Â±{bootstrap_summary['rank_corr_std']:.4f}")
        
        return bootstrap_summary
    
    def compute_diagnostics(self, X: np.ndarray, y: np.ndarray, w_fitted: np.ndarray, 
                          feature_columns: List[str]) -> Dict[str, Any]:
        """è®¡ç®—è¯Šæ–­ä¿¡æ¯"""
        diagnostics = {}
        
        # 1. å¹¶åˆ—ç‡
        y_pred = X @ w_fitted
        from scipy.stats import rankdata
        ranks = rankdata(y_pred, method="average")
        ties_ratio = 1 - len(np.unique(ranks)) / len(ranks)
        diagnostics["ties_ratio"] = ties_ratio
        
        if ties_ratio > 0.2:
            logger.warning(f"é«˜å¹¶åˆ—æ¯”ä¾‹: {ties_ratio:.3f}")
        
        # 2. å…±çº¿æ€§è¯Šæ–­
        corr_matrix = np.corrcoef(X.T)
        max_pair_corr = 0.0
        for i in range(len(feature_columns)):
            for j in range(i+1, len(feature_columns)):
                corr_val = abs(corr_matrix[i, j])
                if corr_val > max_pair_corr:
                    max_pair_corr = corr_val
        
        diagnostics["max_pair_corr"] = max_pair_corr
        if max_pair_corr > 0.9:
            logger.warning(f"é«˜å…±çº¿æ€§: max_pair_corr={max_pair_corr:.3f}")
        
        # 3. ç‰¹å¾é‡è¦æ€§ (Drop-oneæ¶ˆè)
        baseline_rank_corr, _ = spearmanr(y_pred, y) if len(np.unique(y)) > 1 else (0.0, 1.0)
        
        delta_corr_by_feature = {}
        for i, feature in enumerate(feature_columns):
            # ç§»é™¤ç¬¬iä¸ªç‰¹å¾
            X_drop = np.delete(X, i, axis=1)
            w_drop = np.delete(w_fitted, i)
            w_drop = w_drop / w_drop.sum() if w_drop.sum() > 0 else w_drop
            
            y_pred_drop = X_drop @ w_drop
            drop_rank_corr, _ = spearmanr(y_pred_drop, y) if len(np.unique(y)) > 1 else (0.0, 1.0)
            
            delta_corr = baseline_rank_corr - (drop_rank_corr or 0.0)
            delta_corr_by_feature[feature] = delta_corr
        
        diagnostics["delta_corr_by_feature"] = delta_corr_by_feature
        
        # 4. å¯é æ€§æ›²çº¿ (ç­‰é¢‘åˆ†ç®±)
        n_bins = 10
        bin_edges = np.percentile(y_pred, np.linspace(0, 100, n_bins + 1))
        bin_centers = []
        bin_success_rates = []
        
        for i in range(n_bins):
            if i == n_bins - 1:
                mask = (y_pred >= bin_edges[i]) & (y_pred <= bin_edges[i+1])
            else:
                mask = (y_pred >= bin_edges[i]) & (y_pred < bin_edges[i+1])
            
            if mask.sum() > 0:
                bin_center = y_pred[mask].mean()
                bin_success_rate = y[mask].mean()
                bin_centers.append(bin_center)
                bin_success_rates.append(bin_success_rate)
        
        diagnostics["reliability_curve"] = {
            "bin_centers": bin_centers,
            "bin_success_rates": bin_success_rates
        }
        
        return diagnostics
    
    def calibrate_weights(self, cv: int = 5, boot: int = 200, l2: float = 0.1,
                         seed: int = 42, shadow_file: str = "latest") -> Dict[str, Any]:
        """ä¸»æ ¡å‡†æµç¨‹"""
        logger.info("å¼€å§‹æƒé‡æ ¡å‡†...")
        
        # æ›´æ–°å‚æ•°
        self.cv_folds = cv
        self.bootstraps = boot  
        self.lambda_reg = l2
        self.random_seed = seed
        np.random.seed(seed)
        
        # 1. åŠ è½½æ•°æ®
        df = self.load_shadow_run_data(shadow_file)
        
        # 2. å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
        X, y, feature_columns, work_df = self.prepare_features_and_labels(df)
        
        # 3. è·å–å…ˆéªŒæƒé‡
        w_prior = self.get_prior_weights(feature_columns)
        
        # 4. è‡ªé€‚åº”æ­£åˆ™åŒ–æ‹Ÿåˆ
        w_fitted, final_lambda = self.adaptive_regularization(X, y, w_prior)
        
        # 5. äº¤å‰éªŒè¯
        task_types = work_df["task_type"].values
        cv_results = self.cross_validation(X, y, task_types, w_prior, final_lambda)
        
        # 6. Bootstrapè¯„ä¼°
        bootstrap_results = self.bootstrap_evaluation(X, y, w_fitted)
        
        # 7. è¯Šæ–­ä¿¡æ¯
        diagnostics = self.compute_diagnostics(X, y, w_fitted, feature_columns)
        
        # 8. æ„å»ºå®Œæ•´ç»“æœ
        weights_dict = dict(zip(feature_columns, w_fitted))
        prior_dict = dict(zip(feature_columns, w_prior))
        
        result = {
            "metadata": {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "n_samples": len(work_df),
                "n_features": len(feature_columns),
                "lambda_final": final_lambda,
                "cv_folds": cv,
                "bootstraps": boot,
                "seed": seed
            },
            "weights_prior": prior_dict,
            "weights_fit": weights_dict,
            "cv": cv_results,
            "bootstrap": bootstrap_results,
            "diagnostics": diagnostics,
            "feature_columns": feature_columns
        }
        
        return result
    
    def check_thresholds(self, result: Dict[str, Any], baseline_result: Dict[str, Any] = None) -> Dict[str, bool]:
        """æ£€æŸ¥éªŒæ”¶é—¨æ§›"""
        # å½“å‰æ€§èƒ½
        current_rank_corr = result["cv"]["rank_corr_median"]
        current_mae = result["cv"]["mae_median"]
        
        checks = {}
        
        # æƒé‡éè´Ÿæ£€æŸ¥
        weights = np.array(list(result["weights_fit"].values()))
        checks["weights_nonnegative"] = bool(np.all(weights >= 0))
        checks["max_weight_constraint"] = bool(np.max(weights) <= 0.5)
        
        if baseline_result:
            # ä¸åŸºçº¿å¯¹æ¯”
            baseline_rank_corr = baseline_result.get("cv", {}).get("rank_corr_median", current_rank_corr)
            baseline_mae = baseline_result.get("cv", {}).get("mae_median", current_mae)
            
            # ç›¸å¯¹æ”¹è¿›è®¡ç®—
            if baseline_rank_corr > 0:
                rank_corr_improve = (current_rank_corr - baseline_rank_corr) / baseline_rank_corr * 100
            else:
                rank_corr_improve = 0.0
            
            if baseline_mae > 0:
                mae_improve = (baseline_mae - current_mae) / baseline_mae * 100  # MAEé™ä½æ˜¯å¥½çš„
            else:
                mae_improve = 0.0
            
            checks["rank_corr_improve_8pct"] = rank_corr_improve >= 8.0
            checks["mae_improve_5pct"] = mae_improve >= 5.0
            
            checks["improvement_metrics"] = {
                "rank_corr_improve_pct": rank_corr_improve,
                "mae_improve_pct": mae_improve
            }
        else:
            # æ— åŸºçº¿ï¼Œä½¿ç”¨ç»å¯¹é˜ˆå€¼
            checks["rank_corr_improve_8pct"] = current_rank_corr >= 0.6  # ç»å¯¹é˜ˆå€¼
            checks["mae_improve_5pct"] = current_mae <= 0.3  # ç»å¯¹é˜ˆå€¼
            
            checks["improvement_metrics"] = {
                "rank_corr_improve_pct": 0.0,
                "mae_improve_pct": 0.0
            }
        
        # æ€»ä½“é€šè¿‡
        required_checks = [
            checks["weights_nonnegative"],
            checks["max_weight_constraint"],
            checks["rank_corr_improve_8pct"],
            checks["mae_improve_5pct"]
        ]
        checks["overall_pass"] = all(required_checks)
        
        return checks
    
    def save_weights(self, result: Dict[str, Any]) -> str:
        """ä¿å­˜æƒé‡åˆ°é…ç½®æ–‡ä»¶"""
        weights_data = {
            "version": time.strftime("%Y-%m-%d"),
            "lambda": result["metadata"]["lambda_final"],
            "weights": result["weights_fit"],
            "source_commit": "phase_2_2_calibration",
            "notes": "Phase 2.2 calibration (stable set)",
            "performance": {
                "cv_rank_corr": result["cv"]["rank_corr_median"],
                "cv_mae": result["cv"]["mae_median"]
            }
        }
        
        weights_file = "configs/weights.json"
        with open(weights_file, 'w', encoding='utf-8') as f:
            json.dump(weights_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æƒé‡å·²ä¿å­˜åˆ°: {weights_file}")
        return weights_file

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Weight Calibration - æƒé‡æ ¡å‡†")
    parser.add_argument("--cv", type=int, default=5, help="äº¤å‰éªŒè¯æŠ˜æ•°")
    parser.add_argument("--boot", type=int, default=200, help="Bootstrapæ¬¡æ•°")
    parser.add_argument("--l2", type=float, default=0.1, help="L2æ­£åˆ™åŒ–å¼ºåº¦")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--config", default="configs/default_config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--shadow_file", default="latest", help="path to report or 'latest'")
    parser.add_argument("--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    try:
        # åˆ›å»ºæ ¡å‡†å™¨
        calibrator = WeightCalibrator(args.config)
        
        # æ‰§è¡Œæ ¡å‡†
        result = calibrator.calibrate_weights(args.cv, args.boot, args.l2, args.seed, args.shadow_file)
        
        # æ£€æŸ¥é—¨æ§›
        threshold_checks = calibrator.check_thresholds(result)
        result["threshold_checks"] = threshold_checks
        
        # ç¡®å®šè¾“å‡ºæ–‡ä»¶
        if not args.output:
            timestamp = time.strftime("%Y%m%d")
            args.output = f"reports/calibration_report_{timestamp}.json"
        
        # ä¿å­˜ç»“æœ
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # è‡ªå®šä¹‰JSONç¼–ç å™¨å¤„ç†numpyç±»å‹
        def json_serializer(obj):
            if hasattr(obj, 'item'):  # numpy scalar
                return obj.item()
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            if isinstance(obj, (np.bool_, bool)):
                return bool(obj)
            raise TypeError(f'Object of type {obj.__class__.__name__} is not JSON serializable')
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, default=json_serializer)
        
        # ä¿å­˜æƒé‡
        if threshold_checks["overall_pass"]:
            calibrator.save_weights(result)
        
        # æ‰“å°ç»“æœ
        print("ğŸ¯ æƒé‡æ ¡å‡†ç»“æœ")
        print("=" * 60)
        print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {result['metadata']['n_samples']}")
        print(f"ğŸ”§ æœ€ç»ˆÎ»: {result['metadata']['lambda_final']:.4f}")
        print(f"ğŸ“ˆ CV rank-corr: {result['cv']['rank_corr_median']:.4f} ({result['cv']['rank_corr_ci95'][0]:.4f}-{result['cv']['rank_corr_ci95'][1]:.4f})")
        print(f"ğŸ“‰ CV MAE: {result['cv']['mae_median']:.4f} ({result['cv']['mae_ci95'][0]:.4f}-{result['cv']['mae_ci95'][1]:.4f})")
        
        print(f"\nğŸ” è¯Šæ–­ä¿¡æ¯:")
        print(f"  å¹¶åˆ—æ¯”ä¾‹: {result['diagnostics']['ties_ratio']:.4f}")
        print(f"  æœ€å¤§ç‰¹å¾ç›¸å…³æ€§: {result['diagnostics']['max_pair_corr']:.4f}")
        
        print(f"\nâš–ï¸ å­¦ä¹ åˆ°çš„æƒé‡:")
        for feature, weight in result["weights_fit"].items():
            prior_weight = result["weights_prior"][feature]
            change = weight - prior_weight
            print(f"  {feature}: {weight:.4f} (å…ˆéªŒ: {prior_weight:.4f}, å˜åŒ–: {change:+.4f})")
        
        print(f"\nğŸš¦ é—¨æ§›æ£€æŸ¥:")
        for check_name, passed in threshold_checks.items():
            if check_name.endswith('_pass') or check_name in ['weights_nonnegative', 'max_weight_constraint']:
                status = "âœ… PASS" if passed else "âŒ FAIL"
                print(f"  {check_name}: {status}")
        
        if "improvement_metrics" in threshold_checks:
            metrics = threshold_checks["improvement_metrics"]
            print(f"  ç›¸å…³æ€§æ”¹è¿›: {metrics['rank_corr_improve_pct']:.2f}%")
            print(f"  MAEæ”¹è¿›: {metrics['mae_improve_pct']:.2f}%")
        
        overall_status = "âœ… å…¨éƒ¨é€šè¿‡" if threshold_checks["overall_pass"] else "âŒ å­˜åœ¨æœªé€šè¿‡é¡¹"
        print(f"\nğŸ† æ€»ä½“çŠ¶æ€: {overall_status}")
        
        print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜: {output_path}")
        
        if threshold_checks["overall_pass"]:
            print(f"ğŸ“„ æƒé‡å·²ä¿å­˜: configs/weights.json")
        
        # è¿”å›é€€å‡ºç 
        sys.exit(0 if threshold_checks["overall_pass"] else 1)
        
    except Exception as e:
        logger.error(f"æƒé‡æ ¡å‡†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
