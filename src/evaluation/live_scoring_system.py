#!/usr/bin/env python3
"""
Live Scoring System - çœŸå®è¯„åˆ†ç³»ç»Ÿ
åŸºäºGPT-5æŒ‡å¯¼å®ç°çš„live_modeçœŸå®Geminiè¯„åˆ†
"""

import os
import json
import time
import asyncio
import statistics
from typing import Dict, List, Any, Tuple, Optional
import logging
from dataclasses import dataclass

logger = logging.getLogger(__name__)

# è®¾ç½®ç¯å¢ƒå˜é‡å¼€å…³
REWARD_LIVE_MODE = os.getenv("REWARD_LIVE_MODE", "false").lower() == "true"

@dataclass
class ScoringConfig:
    """è¯„åˆ†é…ç½®"""
    model_name: str = "gemini-2.5-pro"
    temperature: float = 0.0
    top_p: float = 0.0
    k_evaluations: int = 3
    variance_threshold: float = 0.08
    max_retries: int = 3
    timeout_seconds: int = 30

class LiveGeminiEvaluator:
    """çœŸå®Geminiè¯„åˆ†å™¨
    
    ç‰¹æ€§:
    - æ”¯æŒK=3å¤šè¯„ä¼°æ±‚median
    - æ–¹å·®æ£€æµ‹å’Œç¨³å®šæ€§æ ‡è®°
    - æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶
    - å¹¶å‘é™åˆ¶å’Œrate limiting
    """
    
    def __init__(self, config: ScoringConfig = None):
        self.config = config or ScoringConfig()
        
        # APIé…ç½®æ£€æŸ¥
        self.api_key = os.getenv("GEMINI_API_KEY")
        if REWARD_LIVE_MODE and not self.api_key:
            logger.warning("REWARD_LIVE_MODE=trueä½†æœªè®¾ç½®GEMINI_API_KEYï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
        
        # å¹¶å‘æ§åˆ¶
        self.semaphore = asyncio.Semaphore(5)  # æœ€å¤š5ä¸ªå¹¶å‘è¯·æ±‚
        self.request_times = []  # ç”¨äºrate limiting
        
        # è¯„åˆ†æç¤ºæ¨¡æ¿
        self.scoring_prompt = """
è¯·å¯¹ä»¥ä¸‹å¯¹è¯è¿›è¡Œå¤šç»´åº¦è´¨é‡è¯„ä¼°ï¼Œè¿”å›JSONæ ¼å¼ç»“æœã€‚

å¯¹è¯å†…å®¹:
{dialogue_text}

è¯„ä¼°ç»´åº¦ï¼ˆ0-1åˆ†ï¼‰ï¼š
1. logic_rigor: é€»è¾‘ä¸¥è°¨æ€§ - æ¨ç†æ˜¯å¦è¿è´¯ã€æ— çŸ›ç›¾ã€é€»è¾‘é“¾å®Œæ•´
2. question_quality: æé—®è´¨é‡ - æ¾„æ¸…é—®é¢˜æ˜¯å¦ç²¾å‡†ã€å¿…è¦ã€æœ‰é’ˆå¯¹æ€§
3. reasoning_completeness: æ¨ç†å®Œæ•´æ€§ - æ­¥éª¤æ˜¯å¦å®Œæ•´ã€æ¸…æ™°ã€æ˜“æ‡‚
4. natural_interaction: äº¤äº’è‡ªç„¶åº¦ - å¯¹è¯æ˜¯å¦æµç•…ã€ç¤¼è²Œã€äººæ€§åŒ–

è¯„åˆ†æ ‡å‡†ï¼š
- 0.9-1.0: ä¼˜ç§€ï¼Œè¯¥ç»´åº¦è¡¨ç°æ°å‡º
- 0.7-0.8: è‰¯å¥½ï¼Œè¯¥ç»´åº¦è¡¨ç°ä¸é”™
- 0.5-0.6: ä¸€èˆ¬ï¼Œè¯¥ç»´åº¦æœ‰æ”¹è¿›ç©ºé—´  
- 0.3-0.4: è¾ƒå·®ï¼Œè¯¥ç»´åº¦å­˜åœ¨æ˜æ˜¾é—®é¢˜
- 0.0-0.2: æå·®ï¼Œè¯¥ç»´åº¦è¡¨ç°å¾ˆä¸ç†æƒ³

è¯·è¿”å›æ ¼å¼ï¼š
{{
    "logic_rigor": 0.85,
    "question_quality": 0.78,
    "reasoning_completeness": 0.82,
    "natural_interaction": 0.76,
    "explanation": "ç®€è¦è¯´æ˜å„ç»´åº¦è¯„åˆ†ç†ç”±ï¼ˆ50å­—ä»¥å†…ï¼‰"
}}

é‡è¦ï¼šåªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚
"""
    
    async def evaluate_dialogue_live(self, dialogue: Dict) -> Tuple[Dict[str, float], float, Dict[str, Any]]:
        """å®æ—¶è¯„ä¼°å¯¹è¯è´¨é‡"""
        if not REWARD_LIVE_MODE or not self.api_key:
            return await self._simulate_evaluation(dialogue)
        
        dialogue_text = self._extract_dialogue_text(dialogue)
        
        # æ‰§è¡ŒKæ¬¡è¯„ä¼°
        evaluations = []
        metadata = {
            "api_calls": 0,
            "total_latency_ms": 0,
            "retries": 0,
            "errors": []
        }
        
        for i in range(self.config.k_evaluations):
            try:
                async with self.semaphore:
                    scores, latency_ms, retry_count = await self._single_evaluation(dialogue_text)
                    evaluations.append(scores)
                    metadata["api_calls"] += 1
                    metadata["total_latency_ms"] += latency_ms
                    metadata["retries"] += retry_count
                    
            except Exception as e:
                logger.error(f"è¯„ä¼°å¤±è´¥ (è½®æ¬¡ {i+1}): {e}")
                metadata["errors"].append(str(e))
                # ä½¿ç”¨å¤‡ç”¨è¯„åˆ†
                fallback_scores = await self._get_fallback_scores(dialogue_text)
                evaluations.append(fallback_scores)
        
        if not evaluations:
            # æ‰€æœ‰è¯„ä¼°éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
            return self._get_default_scores(), 0.0, metadata
        
        # è®¡ç®—medianå’Œæ–¹å·®
        median_scores, variance = self._aggregate_evaluations(evaluations)
        
        metadata["variance"] = variance
        metadata["stability"] = "stable" if variance <= self.config.variance_threshold else "unstable"
        
        return median_scores, variance, metadata
    
    async def _single_evaluation(self, dialogue_text: str) -> Tuple[Dict[str, float], int, int]:
        """å•æ¬¡APIè¯„ä¼°è°ƒç”¨"""
        start_time = time.time()
        retry_count = 0
        
        for attempt in range(self.config.max_retries):
            try:
                # Rate limiting
                await self._rate_limit()
                
                # è°ƒç”¨Gemini API (è¿™é‡Œéœ€è¦å®é™…çš„APIå®ç°)
                scores = await self._call_gemini_api(dialogue_text)
                
                latency_ms = int((time.time() - start_time) * 1000)
                return scores, latency_ms, retry_count
                
            except Exception as e:
                retry_count += 1
                if attempt < self.config.max_retries - 1:
                    # æŒ‡æ•°é€€é¿
                    wait_time = (2 ** attempt) + (time.time() % 1) * 0.3
                    await asyncio.sleep(wait_time)
                    logger.warning(f"APIè°ƒç”¨å¤±è´¥ï¼Œé‡è¯• {attempt + 1}/{self.config.max_retries}: {e}")
                else:
                    raise
        
        # ä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œ
        raise Exception("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥")
    
    async def _call_gemini_api(self, dialogue_text: str) -> Dict[str, float]:
        """è°ƒç”¨Gemini APIï¼ˆéœ€è¦å®é™…å®ç°ï¼‰"""
        # è¿™é‡Œéœ€è¦å®é™…çš„Gemini APIè°ƒç”¨
        # ç”±äºæˆ‘ä»¬æ²¡æœ‰çœŸå®çš„API keyï¼Œæš‚æ—¶ä½¿ç”¨æ¨¡æ‹Ÿ
        logger.warning("ä½¿ç”¨æ¨¡æ‹ŸGemini APIè°ƒç”¨")
        return await self._simulate_single_evaluation(dialogue_text)
    
    async def _simulate_single_evaluation(self, dialogue_text: str) -> Dict[str, float]:
        """æ¨¡æ‹Ÿå•æ¬¡è¯„ä¼°"""
        import random
        
        # æ¨¡æ‹ŸAPIå»¶è¿Ÿ
        await asyncio.sleep(random.uniform(0.5, 2.0))
        
        # åŸºäºæ–‡æœ¬ç‰¹å¾ç”Ÿæˆåˆç†çš„è¯„åˆ†
        base_scores = self._analyze_text_features(dialogue_text)
        
        # æ·»åŠ å°å¹…éšæœºå˜åŒ–æ¨¡æ‹ŸAPIä¸ç¡®å®šæ€§
        noise_level = 0.05
        scores = {}
        for key, base_score in base_scores.items():
            if key != "explanation":
                noise = random.gauss(0, noise_level)
                scores[key] = max(0.0, min(1.0, base_score + noise))
        
        return scores
    
    def _analyze_text_features(self, text: str) -> Dict[str, float]:
        """åŸºäºæ–‡æœ¬ç‰¹å¾åˆ†æè¯„åˆ†"""
        scores = {
            "logic_rigor": 0.70,
            "question_quality": 0.65,
            "reasoning_completeness": 0.68,
            "natural_interaction": 0.72
        }
        
        # é€»è¾‘ä¸¥è°¨æ€§
        if any(word in text for word in ["å› ä¸º", "æ‰€ä»¥", "é¦–å…ˆ", "ç„¶å", "å› æ­¤"]):
            scores["logic_rigor"] += 0.15
        if "<think>" in text:
            scores["logic_rigor"] += 0.10
        
        # æé—®è´¨é‡
        question_count = text.count("?") + text.count("ï¼Ÿ")
        if question_count > 0:
            scores["question_quality"] += min(0.20, question_count * 0.05)
        if any(word in text for word in ["è¯·é—®", "èƒ½å¦", "æ˜¯å¦"]):
            scores["question_quality"] += 0.10
        
        # æ¨ç†å®Œæ•´æ€§
        step_indicators = ["æ­¥éª¤", "ç¬¬ä¸€", "ç¬¬äºŒ", "â†’", "è®¡ç®—"]
        step_count = sum(text.count(indicator) for indicator in step_indicators)
        scores["reasoning_completeness"] += min(0.25, step_count * 0.03)
        
        # äº¤äº’è‡ªç„¶åº¦
        if any(word in text for word in ["è¯·", "è°¢è°¢", "å¥½çš„", "æ‚¨"]):
            scores["natural_interaction"] += 0.15
        if any(emoji in text for emoji in ["ğŸ˜Š", "âœ¨", "ğŸ¯"]):
            scores["natural_interaction"] += 0.10
        
        # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´
        for key in scores:
            scores[key] = max(0.1, min(1.0, scores[key]))
        
        return scores
    
    async def _simulate_evaluation(self, dialogue: Dict) -> Tuple[Dict[str, float], float, Dict[str, Any]]:
        """æ¨¡æ‹Ÿå®Œæ•´è¯„ä¼°æµç¨‹"""
        dialogue_text = self._extract_dialogue_text(dialogue)
        
        # ç”ŸæˆKæ¬¡è¯„ä¼°
        evaluations = []
        total_latency = 0
        
        for i in range(self.config.k_evaluations):
            start_time = time.time()
            scores = await self._simulate_single_evaluation(dialogue_text)
            latency = int((time.time() - start_time) * 1000)
            total_latency += latency
            evaluations.append(scores)
        
        # èšåˆç»“æœ
        median_scores, variance = self._aggregate_evaluations(evaluations)
        
        metadata = {
            "api_calls": self.config.k_evaluations,
            "total_latency_ms": total_latency,
            "retries": 0,
            "errors": [],
            "variance": variance,
            "stability": "stable" if variance <= self.config.variance_threshold else "unstable",
            "mode": "simulation"
        }
        
        return median_scores, variance, metadata
    
    def _aggregate_evaluations(self, evaluations: List[Dict[str, float]]) -> Tuple[Dict[str, float], float]:
        """èšåˆå¤šæ¬¡è¯„ä¼°ç»“æœ"""
        if not evaluations:
            return self._get_default_scores(), 0.0
        
        # è®¡ç®—æ¯ä¸ªç»´åº¦çš„median
        dimensions = ["logic_rigor", "question_quality", "reasoning_completeness", "natural_interaction"]
        median_scores = {}
        variances = []
        
        for dim in dimensions:
            values = [eval_result.get(dim, 0.5) for eval_result in evaluations]
            median_scores[dim] = statistics.median(values)
            
            # è®¡ç®—æ–¹å·®
            if len(values) > 1:
                variance = statistics.variance(values)
                variances.append(variance)
        
        # æ€»ä½“æ–¹å·®
        overall_variance = statistics.mean(variances) if variances else 0.0
        
        return median_scores, overall_variance
    
    async def _rate_limit(self):
        """é€Ÿç‡é™åˆ¶"""
        now = time.time()
        
        # æ¸…ç†æ—§çš„è¯·æ±‚æ—¶é—´
        self.request_times = [t for t in self.request_times if now - t < 60]
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶ï¼ˆæ¯åˆ†é’Ÿ60ä¸ªè¯·æ±‚ï¼‰
        if len(self.request_times) >= 60:
            sleep_time = 60 - (now - self.request_times[0])
            if sleep_time > 0:
                await asyncio.sleep(sleep_time)
        
        self.request_times.append(now)
    
    async def _get_fallback_scores(self, dialogue_text: str) -> Dict[str, float]:
        """è·å–å¤‡ç”¨è¯„åˆ†"""
        return self._analyze_text_features(dialogue_text)
    
    def _get_default_scores(self) -> Dict[str, float]:
        """è·å–é»˜è®¤è¯„åˆ†"""
        return {
            "logic_rigor": 0.5,
            "question_quality": 0.5,
            "reasoning_completeness": 0.5,
            "natural_interaction": 0.5
        }
    
    def _extract_dialogue_text(self, dialogue: Dict) -> str:
        """æå–å¯¹è¯æ–‡æœ¬"""
        if "turns" in dialogue:
            parts = []
            for turn in dialogue["turns"]:
                if isinstance(turn, dict):
                    role = turn.get("role", "")
                    content = turn.get("content", "")
                    parts.append(f"{role}: {content}")
            return "\n".join(parts)
        elif "content" in dialogue:
            return dialogue["content"]
        else:
            return str(dialogue)

class UnstableSampleTracker:
    """ä¸ç¨³å®šæ ·æœ¬è·Ÿè¸ªå™¨"""
    
    def __init__(self, output_file: str = "logs/unstable_samples.jsonl"):
        self.output_file = output_file
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    def track_unstable_sample(self, dialogue: Dict, variance: float, 
                            scores: Dict[str, float], metadata: Dict[str, Any]):
        """è®°å½•ä¸ç¨³å®šæ ·æœ¬"""
        record = {
            "timestamp": time.time(),
            "dialogue_id": dialogue.get("id", "unknown"),
            "variance": variance,
            "scores": scores,
            "metadata": metadata,
            "reason": "high_variance" if variance > 0.08 else "other"
        }
        
        # è¿½åŠ åˆ°JSONLæ–‡ä»¶
        with open(self.output_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(record, ensure_ascii=False) + "\n")
        
        logger.warning(f"è®°å½•ä¸ç¨³å®šæ ·æœ¬: {dialogue.get('id')} (variance={variance:.4f})")
    
    def get_unstable_stats(self) -> Dict[str, Any]:
        """è·å–ä¸ç¨³å®šæ ·æœ¬ç»Ÿè®¡"""
        try:
            with open(self.output_file, "r", encoding="utf-8") as f:
                records = [json.loads(line) for line in f if line.strip()]
            
            if not records:
                return {"total": 0, "avg_variance": 0.0}
            
            return {
                "total": len(records),
                "avg_variance": sum(r["variance"] for r in records) / len(records),
                "latest_timestamp": max(r["timestamp"] for r in records),
                "reasons": [r["reason"] for r in records]
            }
        except FileNotFoundError:
            return {"total": 0, "avg_variance": 0.0}

async def test_live_scoring():
    """æµ‹è¯•live scoringç³»ç»Ÿ"""
    print(f"ğŸ§ª æµ‹è¯•Live Scoringç³»ç»Ÿ (LIVE_MODE={REWARD_LIVE_MODE})")
    print("=" * 60)
    
    # æµ‹è¯•å¯¹è¯
    test_dialogue = {
        "id": "live_test_001",
        "turns": [
            {"role": "user", "content": "ä¸€ä¸ªæ­£æ–¹å½¢çš„å‘¨é•¿æ˜¯20å˜ç±³ï¼Œé¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ"},
            {"role": "assistant", "content": "<think>æ­£æ–¹å½¢å‘¨é•¿=4Ã—è¾¹é•¿ï¼Œæ‰€ä»¥è¾¹é•¿=20Ã·4=5å˜ç±³ã€‚é¢ç§¯=è¾¹é•¿Â²=5Â²=25å¹³æ–¹å˜ç±³ã€‚</think>\n\nè§£é¢˜æ­¥éª¤ï¼š\n1. æ±‚è¾¹é•¿ï¼šå‘¨é•¿Ã·4 = 20Ã·4 = 5å˜ç±³\n2. æ±‚é¢ç§¯ï¼šè¾¹é•¿Â² = 5Â² = 25å¹³æ–¹å˜ç±³\n\nç­”æ¡ˆï¼š25å¹³æ–¹å˜ç±³"}
        ]
    }
    
    # åˆ›å»ºè¯„ä¼°å™¨
    config = ScoringConfig(k_evaluations=3, variance_threshold=0.08)
    evaluator = LiveGeminiEvaluator(config)
    tracker = UnstableSampleTracker()
    
    print("å¼€å§‹è¯„ä¼°...")
    start_time = time.time()
    
    # æ‰§è¡Œè¯„ä¼°
    scores, variance, metadata = await evaluator.evaluate_dialogue_live(test_dialogue)
    
    execution_time = time.time() - start_time
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
    print(f"æ–¹å·®: {variance:.4f}")
    print(f"ç¨³å®šæ€§: {'ç¨³å®š' if variance <= config.variance_threshold else 'ä¸ç¨³å®š'}")
    
    print(f"\nğŸ“ˆ å„ç»´åº¦è¯„åˆ†:")
    for dim, score in scores.items():
        print(f"  {dim}: {score:.3f}")
    
    print(f"\nğŸ”§ å…ƒæ•°æ®:")
    print(json.dumps(metadata, indent=2, ensure_ascii=False))
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ ‡è®°ä¸ºä¸ç¨³å®š
    if variance > config.variance_threshold:
        tracker.track_unstable_sample(test_dialogue, variance, scores, metadata)
        print(f"\nâš ï¸  æ ·æœ¬å·²æ ‡è®°ä¸ºä¸ç¨³å®šï¼")
    
    # ç»Ÿè®¡ä¿¡æ¯
    unstable_stats = tracker.get_unstable_stats()
    print(f"\nğŸ“‹ ä¸ç¨³å®šæ ·æœ¬ç»Ÿè®¡:")
    print(json.dumps(unstable_stats, indent=2, ensure_ascii=False))

def main():
    """ä¸»å‡½æ•°"""
    asyncio.run(test_live_scoring())

if __name__ == "__main__":
    main()
