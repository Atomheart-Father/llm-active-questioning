#!/usr/bin/env python3
"""
Over-Clarification Penalty - è¿‡åº¦æ¾„æ¸…æƒ©ç½šç³»ç»Ÿ
åœ¨needs_clarification=falseçš„æ ·æœ¬ä¸Šæƒ©ç½šæ— å¿…è¦çš„æ¾„æ¸…
"""

import argparse
import json
import re
from typing import Dict, List, Any, Tuple
from pathlib import Path
import logging
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

logger = logging.getLogger(__name__)

class OverClarificationPenalty:
    """è¿‡åº¦æ¾„æ¸…æƒ©ç½šè®¡ç®—å™¨"""
    
    def __init__(self, alpha: float = 0.07, cap: int = 3, 
                 enforce_when_needs_clarification_false: bool = True):
        self.alpha = alpha
        self.cap = cap
        self.enforce = enforce_when_needs_clarification_false
        
        # æ¾„æ¸…è¡Œä¸ºæ¨¡å¼
        self.clarification_patterns = [
            r'[ï¼Ÿ?]',  # é—®å·
            r'è¯·é—®|èƒ½å¦|å¯ä»¥.*å—|æ˜¯å¦',  # ç¤¼è²Œè¯¢é—®
            r'å“ª.*?[ï¼Ÿ?]|ä»€ä¹ˆ.*?[ï¼Ÿ?]|å¦‚ä½•.*?[ï¼Ÿ?]|ä¸ºä»€ä¹ˆ.*?[ï¼Ÿ?]',  # ç–‘é—®è¯
            r'éœ€è¦.*?ç¡®è®¤|éœ€è¦.*?æ¾„æ¸…|ä¸å¤ªç¡®å®š',  # æ¾„æ¸…è¡¨è¿°
            r'<QUESTION>.*?</QUESTION>',  # ç»“æ„åŒ–æ¾„æ¸…æ ‡ç­¾
            r'æˆ‘æƒ³äº†è§£|æˆ‘éœ€è¦çŸ¥é“|èƒ½å‘Šè¯‰æˆ‘',  # ä¿¡æ¯è¯·æ±‚
        ]
        
        logger.info(f"è¿‡åº¦æ¾„æ¸…æƒ©ç½šåˆå§‹åŒ–: Î±={alpha}, cap={cap}, enforce={self.enforce}")
    
    def detect_clarification_turns(self, dialogue: Dict[str, Any]) -> int:
        """æ£€æµ‹æ¾„æ¸…è½®æ•°"""
        clarify_count = 0
        
        if "turns" in dialogue:
            for turn in dialogue["turns"]:
                if isinstance(turn, dict) and turn.get("role") == "assistant":
                    content = turn.get("content", "")
                    if self._is_clarification_turn(content):
                        clarify_count += 1
        
        return clarify_count
    
    def _is_clarification_turn(self, content: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ¾„æ¸…è½®æ¬¡"""
        for pattern in self.clarification_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                return True
        return False
    
    def should_apply_penalty(self, dialogue: Dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åº”ç”¨æƒ©ç½š"""
        if not self.enforce:
            return False
        
        # æ£€æŸ¥æ¨¡æ¿æ ‡æ³¨
        meta = dialogue.get("meta", {})
        needs_clarification = meta.get("needs_clarification", True)
        
        # åªå¯¹ä¸éœ€è¦æ¾„æ¸…çš„æ ·æœ¬åº”ç”¨æƒ©ç½š
        return not needs_clarification
    
    def compute_penalty(self, dialogue: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—è¿‡åº¦æ¾„æ¸…æƒ©ç½š"""
        # æ£€æµ‹æ¾„æ¸…è½®æ•°
        clarify_turns = self.detect_clarification_turns(dialogue)
        
        # åˆ¤æ–­æ˜¯å¦åº”ç”¨æƒ©ç½š
        should_penalty = self.should_apply_penalty(dialogue)
        
        if should_penalty and clarify_turns > 0:
            # penalty = alpha * min(c, cap)
            penalty = self.alpha * min(clarify_turns, self.cap)
        else:
            penalty = 0.0
        
        result = {
            "penalty": penalty,
            "clarify_turns": clarify_turns,
            "should_apply": should_penalty,
            "meta": {
                "alpha": self.alpha,
                "cap": self.cap,
                "needs_clarification": dialogue.get("meta", {}).get("needs_clarification", True)
            }
        }
        
        return result
    
    def apply_penalty_to_reward(self, base_reward: float, penalty_info: Dict[str, Any]) -> float:
        """å°†æƒ©ç½šåº”ç”¨åˆ°åŸºç¡€å¥–åŠ±"""
        penalty = penalty_info["penalty"]
        penalized_reward = max(0.0, base_reward - penalty)
        return penalized_reward

def test_penalty_system():
    """å•å…ƒæµ‹è¯•"""
    penalty_system = OverClarificationPenalty(alpha=0.07, cap=3)
    
    test_cases = [
        {
            "name": "ä¸è§¦å‘æ ·æœ¬",
            "dialogue": {
                "meta": {"needs_clarification": True},
                "turns": [
                    {"role": "user", "content": "è®¡ç®—1+1"},
                    {"role": "assistant", "content": "1+1=2"}
                ]
            },
            "expected_penalty": 0.0
        },
        {
            "name": "è§¦å‘æ ·æœ¬c=1",
            "dialogue": {
                "meta": {"needs_clarification": False},
                "turns": [
                    {"role": "user", "content": "è®¡ç®—1+1"},
                    {"role": "assistant", "content": "è¯·é—®æ‚¨éœ€è¦è¯¦ç»†æ­¥éª¤å—ï¼Ÿ"}
                ]
            },
            "expected_penalty": 0.07
        },
        {
            "name": "è§¦å‘æ ·æœ¬c=2",
            "dialogue": {
                "meta": {"needs_clarification": False},
                "turns": [
                    {"role": "user", "content": "è®¡ç®—1+1"},
                    {"role": "assistant", "content": "æ‚¨æŒ‡çš„æ˜¯ä»€ä¹ˆç±»å‹çš„åŠ æ³•ï¼Ÿ"},
                    {"role": "user", "content": "æ™®é€šåŠ æ³•"},
                    {"role": "assistant", "content": "è¿˜éœ€è¦æˆ‘è¯´æ˜è®¡ç®—è¿‡ç¨‹å—ï¼Ÿ"}
                ]
            },
            "expected_penalty": 0.14
        },
        {
            "name": "è§¦å‘æ ·æœ¬c=5è¶…è¿‡cap",
            "dialogue": {
                "meta": {"needs_clarification": False},
                "turns": [
                    {"role": "user", "content": "è®¡ç®—1+1"},
                    {"role": "assistant", "content": "è¯·é—®ï¼Ÿ"},
                    {"role": "assistant", "content": "èƒ½å¦ï¼Ÿ"},
                    {"role": "assistant", "content": "æ˜¯å¦ï¼Ÿ"},
                    {"role": "assistant", "content": "ä»€ä¹ˆï¼Ÿ"},
                    {"role": "assistant", "content": "å¦‚ä½•ï¼Ÿ"}
                ]
            },
            "expected_penalty": 0.21  # alpha * cap = 0.07 * 3
        }
    ]
    
    print("ğŸ§ª è¿‡åº¦æ¾„æ¸…æƒ©ç½šå•å…ƒæµ‹è¯•")
    print("=" * 50)
    
    all_passed = True
    for test_case in test_cases:
        result = penalty_system.compute_penalty(test_case["dialogue"])
        actual_penalty = result["penalty"]
        expected_penalty = test_case["expected_penalty"]
        
        passed = abs(actual_penalty - expected_penalty) < 0.001
        status = "âœ… PASS" if passed else "âŒ FAIL"
        
        print(f"{status} {test_case['name']}")
        print(f"  æœŸæœ›æƒ©ç½š: {expected_penalty}")
        print(f"  å®é™…æƒ©ç½š: {actual_penalty}")
        print(f"  æ¾„æ¸…è½®æ•°: {result['clarify_turns']}")
        print(f"  åº”ç”¨æƒ©ç½š: {result['should_apply']}")
        print()
        
        if not passed:
            all_passed = False
    
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥!")
    
    return all_passed

def run_ablation_study(penalty_system: OverClarificationPenalty, 
                      samples: List[Dict[str, Any]]) -> Dict[str, Any]:
    """è¿è¡Œæ¶ˆèç ”ç©¶"""
    # åŠ è½½å¥–åŠ±ç³»ç»Ÿ
    from src.evaluation.advanced_reward_system import MultiDimensionalRewardSystem
    
    reward_system = MultiDimensionalRewardSystem()
    
    # ä¸å«æƒ©ç½šçš„ç»“æœ
    no_penalty_results = []
    # å«æƒ©ç½šçš„ç»“æœ
    with_penalty_results = []
    
    over_clarification_count = 0
    total_clarify_turns = 0
    
    for sample in samples:
        # åŸºç¡€å¥–åŠ±è¯„ä¼°
        base_result = reward_system.evaluate_dialogue(sample)
        base_reward = base_result["primary_reward"]
        
        # è®¡ç®—æƒ©ç½š
        penalty_info = penalty_system.compute_penalty(sample)
        penalty = penalty_info["penalty"]
        penalized_reward = penalty_system.apply_penalty_to_reward(base_reward, penalty_info)
        
        # ç»Ÿè®¡
        if penalty > 0:
            over_clarification_count += 1
        total_clarify_turns += penalty_info["clarify_turns"]
        
        no_penalty_results.append({
            "sample_id": sample.get("id", "unknown"),
            "task_type": sample.get("task_type", "unknown"),
            "reward": base_reward,
            "clarify_turns": penalty_info["clarify_turns"]
        })
        
        with_penalty_results.append({
            "sample_id": sample.get("id", "unknown"),
            "task_type": sample.get("task_type", "unknown"), 
            "reward": penalized_reward,
            "penalty": penalty,
            "clarify_turns": penalty_info["clarify_turns"]
        })
    
    # è®¡ç®—æŒ‡æ ‡
    def compute_metrics(results):
        rewards = [r["reward"] for r in results]
        clarify_turns = [r["clarify_turns"] for r in results]
        
        return {
            "avg_reward": sum(rewards) / len(rewards) if rewards else 0,
            "avg_turns": sum(clarify_turns) / len(clarify_turns) if clarify_turns else 0,
            "over_clarification_rate": over_clarification_count / len(results) if results else 0
        }
    
    no_penalty_metrics = compute_metrics(no_penalty_results)
    with_penalty_metrics = compute_metrics(with_penalty_results)
    
    # æ¶ˆèåˆ†æ
    ablation_result = {
        "metadata": {
            "n_samples": len(samples),
            "alpha": penalty_system.alpha,
            "cap": penalty_system.cap,
            "over_clarification_samples": over_clarification_count,
            "total_clarify_turns": total_clarify_turns
        },
        "no_penalty": no_penalty_metrics,
        "with_penalty": with_penalty_metrics,
        "improvements": {
            "avg_reward_change": with_penalty_metrics["avg_reward"] - no_penalty_metrics["avg_reward"],
            "avg_turns_change": with_penalty_metrics["avg_turns"] - no_penalty_metrics["avg_turns"],
            "over_clarification_rate_change": 0.0  # æƒ©ç½šä¸ä¼šæ”¹å˜æ£€æµ‹åˆ°çš„è¿‡åº¦æ¾„æ¸…ç‡
        }
    }
    
    return ablation_result

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Over-Clarification Penalty - è¿‡åº¦æ¾„æ¸…æƒ©ç½š")
    parser.add_argument("--alpha", type=float, default=0.07, help="æƒ©ç½šç³»æ•°")
    parser.add_argument("--cap", type=int, default=3, help="æƒ©ç½šä¸Šé™")
    parser.add_argument("--ablation", action="store_true", help="è¿è¡Œæ¶ˆèç ”ç©¶")
    parser.add_argument("--test", action="store_true", help="è¿è¡Œå•å…ƒæµ‹è¯•")
    parser.add_argument("--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    if args.test:
        # è¿è¡Œå•å…ƒæµ‹è¯•
        success = test_penalty_system()
        sys.exit(0 if success else 1)
    
    if args.ablation:
        try:
            # åŠ è½½æ ·æœ¬æ•°æ®è¿›è¡Œæ¶ˆèç ”ç©¶
            from src.evaluation.shadow_run import ShadowRunEvaluator
            
            evaluator = ShadowRunEvaluator()
            samples = evaluator.load_or_generate_sample_data(50, 20250820)  # ä½¿ç”¨è¾ƒå°æ ·æœ¬æµ‹è¯•
            
            penalty_system = OverClarificationPenalty(args.alpha, args.cap)
            
            print("ğŸ”¬ è¿‡åº¦æ¾„æ¸…æƒ©ç½šæ¶ˆèç ”ç©¶")
            print("=" * 50)
            
            # è¿è¡Œæ¶ˆèç ”ç©¶
            ablation_result = run_ablation_study(penalty_system, samples)
            
            # ç¡®å®šè¾“å‡ºæ–‡ä»¶
            if not args.output:
                timestamp = time.strftime("%Y%m%d")
                args.output = f"reports/overclar_ablation_{timestamp}.json"
            
            # ä¿å­˜ç»“æœ
            output_path = Path(args.output)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(ablation_result, f, ensure_ascii=False, indent=2)
            
            # æ‰“å°ç»“æœ
            print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {ablation_result['metadata']['n_samples']}")
            print(f"ğŸš¨ è¿‡åº¦æ¾„æ¸…æ ·æœ¬: {ablation_result['metadata']['over_clarification_samples']}")
            print(f"ğŸ“ˆ æ€»æ¾„æ¸…è½®æ•°: {ablation_result['metadata']['total_clarify_turns']}")
            
            print(f"\nğŸ“Š æ— æƒ©ç½šæŒ‡æ ‡:")
            no_penalty = ablation_result["no_penalty"]
            print(f"  å¹³å‡å¥–åŠ±: {no_penalty['avg_reward']:.4f}")
            print(f"  å¹³å‡è½®æ•°: {no_penalty['avg_turns']:.2f}")
            print(f"  è¿‡åº¦æ¾„æ¸…ç‡: {no_penalty['over_clarification_rate']:.4f}")
            
            print(f"\nğŸ“Š æœ‰æƒ©ç½šæŒ‡æ ‡:")
            with_penalty = ablation_result["with_penalty"]
            print(f"  å¹³å‡å¥–åŠ±: {with_penalty['avg_reward']:.4f}")
            print(f"  å¹³å‡è½®æ•°: {with_penalty['avg_turns']:.2f}")
            print(f"  è¿‡åº¦æ¾„æ¸…ç‡: {with_penalty['over_clarification_rate']:.4f}")
            
            improvements = ablation_result["improvements"]
            print(f"\nğŸ“ˆ æ”¹è¿›æƒ…å†µ:")
            print(f"  å¥–åŠ±å˜åŒ–: {improvements['avg_reward_change']:+.4f}")
            print(f"  è½®æ•°å˜åŒ–: {improvements['avg_turns_change']:+.2f}")
            
            print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜: {output_path}")
            
        except Exception as e:
            logger.error(f"æ¶ˆèç ”ç©¶å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    else:
        # ä»…è¿è¡Œå•å…ƒæµ‹è¯•
        success = test_penalty_system()
        sys.exit(0 if success else 1)

if __name__ == "__main__":
    import time
    main()
