#!/usr/bin/env python3
"""
Shadow Run - å½±å­è¿è¡Œå¯¹æ¯”ç³»ç»Ÿ
å¯¹åŒä¸€æ‰¹æ ·æœ¬å¹¶è¡Œè®¡ç®—æ–°æ—§è¯„åˆ†ç³»ç»Ÿï¼Œè¿›è¡Œæ’åä¸ç›¸å…³æ€§åˆ†æ
"""

import argparse
import json
import time
import random
from pathlib import Path
from typing import Dict, List, Any, Tuple
import numpy as np
from scipy.stats import spearmanr, kendalltau
import yaml
import sys
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.evaluation.advanced_reward_system import MultiDimensionalRewardSystem
from src.evaluation.diversity_metrics import DiversityMetrics

logger = logging.getLogger(__name__)

class ShadowRunEvaluator:
    """å½±å­è¿è¡Œè¯„ä¼°å™¨"""
    
    def __init__(self, config_path: str = "configs/default_config.yaml"):
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # åˆå§‹åŒ–è¯„ä¼°ç³»ç»Ÿ
        self.new_reward_system = MultiDimensionalRewardSystem()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path("reports").mkdir(exist_ok=True)
        Path("data").mkdir(exist_ok=True)
        
        logger.info("Shadow Runè¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def generate_stratified_sample(self, n: int = 245, seed: int = 20250820) -> List[Dict[str, Any]]:
        """ç”Ÿæˆåˆ†å±‚æŠ½æ ·çš„è¯„ä¼°æ ·æœ¬"""
        random.seed(seed)
        np.random.seed(seed)
        
        # æ¯ç±»ä»»åŠ¡çš„æ ·æœ¬æ•° (çº¦ç­‰åˆ†)
        samples_per_task = n // 3
        remainder = n % 3
        
        task_samples = {
            "math": samples_per_task + (1 if remainder > 0 else 0),
            "multihop": samples_per_task + (1 if remainder > 1 else 0), 
            "clarify": samples_per_task
        }
        
        logger.info(f"ç”Ÿæˆåˆ†å±‚æ ·æœ¬: {task_samples}")
        
        # ç”Ÿæˆæ ·æœ¬æ•°æ®
        all_samples = []
        
        for task_type, count in task_samples.items():
            for i in range(count):
                sample = self._generate_sample_for_task(task_type, i, seed)
                all_samples.append(sample)
        
        # éšæœºæ‰“ä¹±
        random.shuffle(all_samples)
        
        logger.info(f"ç”Ÿæˆäº†{len(all_samples)}ä¸ªæ ·æœ¬")
        return all_samples
    
    def _generate_sample_for_task(self, task_type: str, index: int, seed: int) -> Dict[str, Any]:
        """ä¸ºç‰¹å®šä»»åŠ¡ç±»å‹ç”Ÿæˆæ ·æœ¬"""
        sample_id = f"{task_type}_{seed}_{index:03d}"
        
        if task_type == "math":
            return {
                "id": sample_id,
                "task_type": "math",
                "turns": [
                    {"role": "user", "content": f"ä¸€ä¸ªé•¿æ–¹å½¢çš„é•¿æ˜¯{5+index}å˜ç±³ï¼Œå®½æ˜¯{3+index//2}å˜ç±³ï¼Œé¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ"},
                    {"role": "assistant", "content": f"<think>é•¿æ–¹å½¢é¢ç§¯=é•¿Ã—å®½ï¼Œæ‰€ä»¥é¢ç§¯={5+index}Ã—{3+index//2}={(5+index)*(3+index//2)}å¹³æ–¹å˜ç±³ã€‚</think>\n\nè®¡ç®—æ­¥éª¤ï¼š\n1. ç¡®å®šå…¬å¼ï¼šé¢ç§¯ = é•¿ Ã— å®½\n2. ä»£å…¥æ•°å€¼ï¼šé¢ç§¯ = {5+index} Ã— {3+index//2}\n3. è®¡ç®—ç»“æœï¼šé¢ç§¯ = {(5+index)*(3+index//2)}å¹³æ–¹å˜ç±³\n\nç­”æ¡ˆï¼š{(5+index)*(3+index//2)}å¹³æ–¹å˜ç±³"}
                ],
                "ground_truth": {
                    "correct_answer": (5+index)*(3+index//2),
                    "task_success": True,
                    "reasoning_steps": 3
                },
                "meta": {"template_id": "math_formal_teacher", "needs_clarification": False}
            }
        
        elif task_type == "multihop":
            countries = ["æ³•å›½", "å¾·å›½", "æ„å¤§åˆ©", "è¥¿ç­ç‰™", "è‹±å›½"]
            capitals = ["å·´é»", "æŸæ—", "ç½—é©¬", "é©¬å¾·é‡Œ", "ä¼¦æ•¦"]
            country = countries[index % len(countries)]
            capital = capitals[index % len(capitals)]
            
            return {
                "id": sample_id,
                "task_type": "multihop", 
                "turns": [
                    {"role": "user", "content": f"{country}çš„é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿè¿™ä¸ªåŸå¸‚æœ‰ä»€ä¹ˆè‘—åæ™¯ç‚¹ï¼Ÿ"},
                    {"role": "assistant", "content": f"è®©æˆ‘åˆ†æ­¥å›ç­”è¿™ä¸ªé—®é¢˜ï¼š\n\nç¬¬ä¸€æ­¥ï¼š{country}çš„é¦–éƒ½\n{country}çš„é¦–éƒ½æ˜¯{capital}ã€‚\n\nç¬¬äºŒæ­¥ï¼š{capital}çš„è‘—åæ™¯ç‚¹\n{capital}æœ‰è®¸å¤šä¸–ç•Œé—»åçš„æ™¯ç‚¹ï¼ŒåŒ…æ‹¬å†å²å»ºç­‘ã€åšç‰©é¦†å’Œæ–‡åŒ–åœ°æ ‡ã€‚\n\nç»¼åˆç­”æ¡ˆï¼š{country}çš„é¦–éƒ½æ˜¯{capital}ï¼Œè¿™åº§åŸå¸‚æ‹¥æœ‰ä¸°å¯Œçš„å†å²æ–‡åŒ–æ™¯ç‚¹ã€‚"}
                ],
                "ground_truth": {
                    "correct_answer": {"capital": capital, "has_landmarks": True},
                    "task_success": True,
                    "reasoning_hops": 2
                },
                "meta": {"template_id": "multihop_systematic", "needs_clarification": False}
            }
        
        elif task_type == "clarify":
            ambiguous_queries = [
                "ä»–ä»€ä¹ˆæ—¶å€™æ¥çš„ï¼Ÿ",
                "è¿™ä¸ªæ€ä¹ˆç”¨ï¼Ÿ", 
                "ä»·æ ¼æ˜¯å¤šå°‘ï¼Ÿ",
                "åœ¨å“ªé‡Œå¯ä»¥æ‰¾åˆ°ï¼Ÿ",
                "éœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ"
            ]
            query = ambiguous_queries[index % len(ambiguous_queries)]
            
            return {
                "id": sample_id,
                "task_type": "clarify",
                "turns": [
                    {"role": "user", "content": query},
                    {"role": "assistant", "content": f"æˆ‘éœ€è¦æ›´å¤šä¿¡æ¯æ¥å‡†ç¡®å›ç­”æ‚¨çš„é—®é¢˜ã€‚\\n\\nå…³äº\"{query}\"ï¼Œæˆ‘æƒ³äº†è§£ï¼š\\n1. æ‚¨æŒ‡çš„å…·ä½“æ˜¯ä»€ä¹ˆ/è°ï¼Ÿ\\n2. æ‚¨å¸Œæœ›äº†è§£å“ªä¸ªæ–¹é¢çš„ä¿¡æ¯ï¼Ÿ\\n3. è¿™æ˜¯åœ¨ä»€ä¹ˆåœºæ™¯ä¸‹çš„é—®é¢˜ï¼Ÿ\\n\\nè¯·æä¾›è¿™äº›ç»†èŠ‚ï¼Œæˆ‘å°†ä¸ºæ‚¨æä¾›ç²¾ç¡®çš„ç­”æ¡ˆã€‚"}
                ],
                "ground_truth": {
                    "correct_behavior": "ask_clarification",
                    "task_success": True,
                    "clarification_points": 3
                },
                "meta": {"template_id": "clarify_polite_assistant", "needs_clarification": True}
            }
        
        else:
            raise ValueError(f"æœªçŸ¥ä»»åŠ¡ç±»å‹: {task_type}")
    
    def load_or_generate_sample_data(self, n: int, seed: int, data_file: str = "data/shadow_eval_245.jsonl") -> List[Dict[str, Any]]:
        """åŠ è½½æˆ–ç”Ÿæˆæ ·æœ¬æ•°æ®"""
        data_path = Path(data_file)
        
        if data_path.exists():
            logger.info(f"ä»æ–‡ä»¶åŠ è½½æ ·æœ¬: {data_file}")
            samples = []
            with open(data_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        samples.append(json.loads(line))
            
            if len(samples) == n:
                return samples
            else:
                logger.warning(f"æ–‡ä»¶ä¸­æ ·æœ¬æ•°({len(samples)})ä¸éœ€æ±‚ä¸ç¬¦({n})ï¼Œé‡æ–°ç”Ÿæˆ")
        
        # ç”Ÿæˆæ–°æ ·æœ¬
        logger.info(f"ç”Ÿæˆæ–°çš„æ ·æœ¬æ•°æ®: {n}æ¡")
        samples = self.generate_stratified_sample(n, seed)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        data_path.parent.mkdir(parents=True, exist_ok=True)
        with open(data_path, 'w', encoding='utf-8') as f:
            for sample in samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        
        logger.info(f"æ ·æœ¬æ•°æ®å·²ä¿å­˜: {data_file}")
        return samples
    
    def evaluate_with_old_system(self, sample: Dict[str, Any]) -> Dict[str, float]:
        """ä½¿ç”¨æ—§çš„7ç»´è¯„åˆ†ç³»ç»Ÿ"""
        # è¿™é‡Œå®ç°æ—§çš„7ç»´è¯„åˆ†é€»è¾‘
        # æš‚æ—¶ä½¿ç”¨å¯å‘å¼è§„åˆ™æ¨¡æ‹Ÿ
        
        task_type = sample.get("task_type", "unknown")
        dialogue_text = self._extract_dialogue_text(sample)
        
        # åŸºç¡€è¯„åˆ†
        scores = {
            "logic_rigor": 0.75,
            "calc_accuracy": 0.70,
            "expression_clarity": 0.72,
            "completeness": 0.68,
            "clarification": 0.65,
            "naturalness": 0.70,
            "educational": 0.67
        }
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´
        if task_type == "math":
            if "think" in dialogue_text:
                scores["logic_rigor"] += 0.15
            if "æ­¥éª¤" in dialogue_text:
                scores["completeness"] += 0.15
            if "=" in dialogue_text:
                scores["calc_accuracy"] += 0.20
        
        elif task_type == "clarify":
            if "?" in dialogue_text or "ï¼Ÿ" in dialogue_text:
                scores["clarification"] += 0.25
            if "è¯·" in dialogue_text or "æ‚¨" in dialogue_text:
                scores["naturalness"] += 0.15
        
        elif task_type == "multihop":
            if "ç¬¬ä¸€" in dialogue_text and "ç¬¬äºŒ" in dialogue_text:
                scores["completeness"] += 0.20
            if "æ­¥éª¤" in dialogue_text or "åˆ†æ­¥" in dialogue_text:
                scores["logic_rigor"] += 0.15
        
        # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´
        for key in scores:
            scores[key] = max(0.0, min(1.0, scores[key]))
        
        # è®¡ç®—åŠ æƒæ€»åˆ† (æ—§æƒé‡)
        old_weights = {
            "logic_rigor": 0.20,
            "calc_accuracy": 0.20, 
            "expression_clarity": 0.15,
            "completeness": 0.15,
            "clarification": 0.10,
            "naturalness": 0.10,
            "educational": 0.10
        }
        
        total_score = sum(scores[key] * old_weights[key] for key in scores)
        scores["total_score"] = total_score
        
        return scores
    
    def evaluate_with_new_system(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨æ–°çš„å¥–åŠ±ç³»ç»Ÿè¯„ä¼°"""
        return self.new_reward_system.evaluate_dialogue(sample)
    
    def calculate_task_success_correlation(self, samples: List[Dict], old_scores: List[float], new_scores: List[float]) -> Tuple[float, float]:
        """è®¡ç®—ä¸ä»»åŠ¡æˆåŠŸç‡çš„ç›¸å…³æ€§"""
        # æå–çœŸå®çš„ä»»åŠ¡æˆåŠŸæ ‡ç­¾
        success_labels = []
        for sample in samples:
            ground_truth = sample.get("ground_truth", {})
            task_success = ground_truth.get("task_success", False)
            success_labels.append(1.0 if task_success else 0.0)
        
        if len(set(success_labels)) < 2:
            # æ‰€æœ‰æ ·æœ¬éƒ½æˆåŠŸæˆ–éƒ½å¤±è´¥ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§
            return 0.0, 0.0
        
        # è®¡ç®—ç›¸å…³æ€§
        old_corr, _ = spearmanr(old_scores, success_labels)
        new_corr, _ = spearmanr(new_scores, success_labels)
        
        return old_corr or 0.0, new_corr or 0.0
    
    def _compute_task_success(self, sample: Dict[str, Any]) -> int:
        """ç»Ÿä¸€çš„ä»»åŠ¡æˆåŠŸå®šä¹‰"""
        ground_truth = sample.get("ground_truth", {})
        task_type = sample.get("task_type", "unknown")
        
        # ç»Ÿä¸€æˆåŠŸæ ‡å‡†
        if task_type == "math":
            # æ•°å­¦é¢˜ï¼šç²¾ç¡®åŒ¹é…
            return 1 if ground_truth.get("task_success", False) else 0
        elif task_type == "multihop":
            # å¤šè·³æ¨ç†ï¼šé€»è¾‘å®Œæ•´æ€§
            return 1 if ground_truth.get("task_success", False) else 0
        elif task_type == "clarify":
            # æ¾„æ¸…ä»»åŠ¡ï¼šæ˜¯å¦æ­£ç¡®è¯†åˆ«éœ€è¦æ¾„æ¸…
            return 1 if ground_truth.get("task_success", False) else 0
        else:
            return 0
    
    def run_shadow_evaluation(self, n: int = 245, seed: int = 20250820, stratify: bool = True) -> Dict[str, Any]:
        """æ‰§è¡Œå½±å­è¿è¡Œè¯„ä¼°"""
        logger.info(f"å¼€å§‹å½±å­è¿è¡Œè¯„ä¼°: n={n}, seed={seed}, stratify={stratify}")
        
        # 1. åŠ è½½æˆ–ç”Ÿæˆæ ·æœ¬
        samples = self.load_or_generate_sample_data(n, seed)
        
        # 2. å¹¶è¡Œè¯„ä¼° - ä½¿ç”¨DataFrameç¡®ä¿å¯¹é½
        import pandas as pd
        from scipy.stats import rankdata
        
        eval_data = []
        unstable_samples = []
        
        for i, sample in enumerate(samples):
            logger.info(f"è¯„ä¼°æ ·æœ¬ {i+1}/{len(samples)}: {sample['id']}")
            
            # æ—§ç³»ç»Ÿè¯„åˆ†
            old_result = self.evaluate_with_old_system(sample)
            old_score = old_result["total_score"]
            
            # æ–°ç³»ç»Ÿè¯„åˆ†
            new_result = self.evaluate_with_new_system(sample)
            new_score = new_result["primary_reward"]
            
            # ä»»åŠ¡æˆåŠŸæ ‡ç­¾
            task_success = self._compute_task_success(sample)
            
            # æ£€æŸ¥ä¸ç¨³å®šæ ·æœ¬
            variance = new_result.get("meta", {}).get("variance", 0.0)
            is_unstable = variance > 0.08
            if is_unstable:
                unstable_samples.append({
                    "id": sample["id"],
                    "variance": variance,
                    "task_type": sample.get("task_type")
                })
            
            eval_data.append({
                "sample_id": sample["id"],
                "task_type": sample.get("task_type", "unknown"),
                "old_score": old_score,
                "new_score": new_score,
                "task_success": task_success,
                "variance": variance,
                "is_unstable": is_unstable,
                "stable_weight": 0.5 if is_unstable else 1.0
            })
        
        # è½¬æ¢ä¸ºDataFrameç¡®ä¿å¯¹é½
        df = pd.DataFrame(eval_data)
        
        # æ–­è¨€æ£€æŸ¥
        assert len(df) == n, f"æ ·æœ¬æ•°ä¸åŒ¹é…: æœŸæœ›{n}, å®é™…{len(df)}"
        assert df['old_score'].std() > 0, "æ—§åˆ†æ•°æ— å˜åŒ–ï¼Œæ£€æŸ¥è¯„åˆ†é€»è¾‘"
        assert df['new_score'].std() > 0, "æ–°åˆ†æ•°æ— å˜åŒ–ï¼Œæ£€æŸ¥è¯„åˆ†é€»è¾‘"
        
        # å½’ä¸€åŒ–ç¡®ä¿é‡çº²ä¸€è‡´ (min-maxåˆ°[0,1])
        df['old_score_norm'] = (df['old_score'] - df['old_score'].min()) / (df['old_score'].max() - df['old_score'].min())
        df['new_score_norm'] = (df['new_score'] - df['new_score'].min()) / (df['new_score'].max() - df['new_score'].min())
        
        # ä½¿ç”¨å½’ä¸€åŒ–åˆ†æ•°è¿›è¡Œåç»­åˆ†æ
        old_scores = df['old_score_norm'].values
        new_scores = df['new_score_norm'].values
        
        # 3. è®¡ç®—åŸºäºç§©çš„ç›¸å…³æ€§ - å¤„ç†å¹¶åˆ—
        old_ranks = rankdata(old_scores, method="average")
        new_ranks = rankdata(new_scores, method="average")
        
        # è®¡ç®—å¹¶åˆ—æ¯”ä¾‹
        old_ties_ratio = 1 - len(np.unique(old_ranks)) / len(old_ranks)
        new_ties_ratio = 1 - len(np.unique(new_ranks)) / len(new_ranks)
        
        if old_ties_ratio > 0.2 or new_ties_ratio > 0.2:
            logger.warning(f"æ£€æµ‹åˆ°é«˜å¹¶åˆ—æ¯”ä¾‹: old={old_ties_ratio:.3f}, new={new_ties_ratio:.3f}")
        
        # ç›¸å…³æ€§è®¡ç®—
        spearman_corr, spearman_p = spearmanr(old_ranks, new_ranks)
        kendall_tau, kendall_p = kendalltau(old_ranks, new_ranks)
        
        # åŒå‘sanity check - æ£€æŸ¥æ–¹å‘æ˜¯å¦é¢ å€’
        spearman_neg, _ = spearmanr(old_ranks, -new_ranks)
        direction_reversed = abs(spearman_neg) > abs(spearman_corr) + 0.1
        
        if direction_reversed:
            logger.error(f"ç–‘ä¼¼æ–¹å‘é¢ å€’! æ­£å‘:{spearman_corr:.4f} vs åå‘:{spearman_neg:.4f}")
        
        # ç¨³å®šç‰ˆæœ¬è®¡ç®— (å‰”é™¤ä¸ç¨³å®šæ ·æœ¬)
        stable_mask = df['stable_weight'] == 1.0
        stable_df = df[stable_mask] if stable_mask.sum() > 5 else df  # è‡³å°‘ä¿ç•™5ä¸ªæ ·æœ¬
        
        stable_old_ranks = rankdata(stable_df['old_score_norm'], method="average")
        stable_new_ranks = rankdata(stable_df['new_score_norm'], method="average")
        stable_spearman, _ = spearmanr(stable_old_ranks, stable_new_ranks) if len(stable_df) > 1 else (0.0, 1.0)
        
        # 4. è®¡ç®—Top-Ké‡åˆåº¦ - ä¿®å¤ç‰ˆæœ¬ï¼ŒæŒ‰sample_idå¯¹é½
        def get_top_k_overlap_fixed(df_input, k_pct, old_col='old_score_norm', new_col='new_score_norm'):
            k = max(1, int(len(df_input) * k_pct / 100))
            
            # æŒ‰åˆ†æ•°é™åºæ’åºï¼Œå–å‰Kä¸ªsample_id
            top_k_old = set(df_input.nlargest(k, old_col)['sample_id'].values)
            top_k_new = set(df_input.nlargest(k, new_col)['sample_id'].values)
            
            overlap = len(top_k_old & top_k_new)
            return overlap / k, top_k_old, top_k_new
        
        # è®¡ç®—å¤šä¸ªKå€¼çš„é‡åˆåº¦
        overlap_results = {}
        top_lists = {}
        
        for k_pct in [5, 10, 20, 50]:
            overlap, top_old, top_new = get_top_k_overlap_fixed(df, k_pct)
            overlap_results[f"top{k_pct}_overlap"] = overlap
            if k_pct in [10, 20]:  # ä¿å­˜è¯¦ç»†åˆ—è¡¨ç”¨äºè°ƒè¯•
                top_lists[f"top{k_pct}"] = {
                    "old_ids": list(top_old),
                    "new_ids": list(top_new), 
                    "intersection": list(top_old & top_new)
                }
        
        # 5. è®¡ç®—ä¸ä»»åŠ¡æˆåŠŸç‡çš„ç›¸å…³æ€§ - ä½¿ç”¨DataFrameæ•°æ®
        success_labels = df['task_success'].values
        success_rate_by_task = df.groupby('task_type')['task_success'].agg(['mean', 'count']).to_dict('index')
        
        # æ£€æŸ¥æˆåŠŸç‡åˆ†å¸ƒ
        overall_success_rate = df['task_success'].mean()
        if overall_success_rate == 0.0 or overall_success_rate == 1.0:
            logger.warning(f"æˆåŠŸç‡æç«¯: {overall_success_rate:.3f}, ç›¸å…³æ€§è®¡ç®—å¯èƒ½ä¸å‡†ç¡®")
        
        # è®¡ç®—ç›¸å…³æ€§
        if len(np.unique(success_labels)) > 1:  # æœ‰å˜åŒ–æ‰è®¡ç®—
            old_success_corr, _ = spearmanr(old_ranks, success_labels)
            new_success_corr, _ = spearmanr(new_ranks, success_labels)
        else:
            old_success_corr = new_success_corr = 0.0
        
        # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”
        if abs(old_success_corr) > 0.001:
            corr_improve_pct = ((new_success_corr - old_success_corr) / abs(old_success_corr)) * 100
        else:
            corr_improve_pct = 0.0
        
        # 6. æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
        by_task_stats = {}
        for task_type in df['task_type'].unique():
            task_df = df[df['task_type'] == task_type]
            if len(task_df) > 1:
                task_old_ranks = rankdata(task_df['old_score_norm'], method="average")
                task_new_ranks = rankdata(task_df['new_score_norm'], method="average")
                task_spearman, _ = spearmanr(task_old_ranks, task_new_ranks) if len(task_df) > 1 else (0.0, 1.0)
                
                by_task_stats[task_type] = {
                    "count": len(task_df),
                    "spearman": task_spearman or 0.0,
                    "old_mean": task_df['old_score_norm'].mean(),
                    "new_mean": task_df['new_score_norm'].mean(),
                    "old_std": task_df['old_score_norm'].std(),
                    "new_std": task_df['new_score_norm'].std(),
                    "success_rate": task_df['task_success'].mean(),
                    "unstable_rate": task_df['is_unstable'].mean()
                }
        
        # ç”ŸæˆTop-10è¯Šæ–­åˆ—è¡¨
        top10_df = df.nlargest(10, 'old_score_norm')[['sample_id', 'old_score_norm', 'new_score_norm', 'task_type']]
        top10_old_list = top10_df.to_dict('records')
        
        top10_new_df = df.nlargest(10, 'new_score_norm')[['sample_id', 'old_score_norm', 'new_score_norm', 'task_type']]
        top10_new_list = top10_new_df.to_dict('records')
        
        # ä¿å­˜æ ·æœ¬æ¸…å•ç”¨äºå¤ç°
        sample_manifest = {
            "seed": seed,
            "stratified": stratify,
            "task_distribution": df['task_type'].value_counts().to_dict(),
            "samples": df[['sample_id', 'task_type']].to_dict('records')
        }
        
        # 7. æ„å»ºå®Œæ•´ç»“æœ
        result = {
            "metadata": {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "n": len(df),
                "seed": seed,
                "stratified": stratify,
                "stable_samples": len(stable_df),
                "unstable_samples": len(df) - len(stable_df)
            },
            "correlations": {
                "full_dataset": {
                    "spearman": round(spearman_corr or 0.0, 4),
                    "spearman_p": round(spearman_p or 1.0, 4),
                    "kendall_tau": round(kendall_tau or 0.0, 4),
                    "kendall_p": round(kendall_p or 1.0, 4)
                },
                "stable_dataset": {
                    "spearman": round(stable_spearman or 0.0, 4),
                    "stable_samples": len(stable_df)
                }
            },
            "overlap_metrics": overlap_results,
            "task_success_correlation": {
                "corr_to_success_old": round(old_success_corr, 4),
                "corr_to_success_new": round(new_success_corr, 4),
                "corr_improve_pct": round(corr_improve_pct, 2),
                "overall_success_rate": round(overall_success_rate, 4),
                "success_rate_by_task": success_rate_by_task
            },
            "score_distribution": {
                "old_scores_normalized": {
                    "mean": round(np.mean(old_scores), 4),
                    "std": round(np.std(old_scores), 4),
                    "min": round(np.min(old_scores), 4),
                    "max": round(np.max(old_scores), 4)
                },
                "new_scores_normalized": {
                    "mean": round(np.mean(new_scores), 4),
                    "std": round(np.std(new_scores), 4),
                    "min": round(np.min(new_scores), 4),
                    "max": round(np.max(new_scores), 4)
                }
            },
            "by_task": by_task_stats,
            "unstable_samples": unstable_samples,
            "quality_metrics": {
                "unstable_rate": round(len(unstable_samples) / len(df), 4),
                "avg_variance": round(np.mean([s["variance"] for s in unstable_samples]) if unstable_samples else 0.0, 4),
                "ties_ratio": {
                    "old": round(old_ties_ratio, 4),
                    "new": round(new_ties_ratio, 4)
                }
            },
            "diagnostics": {
                "direction_check": {
                    "spearman_positive": round(spearman_corr or 0.0, 4),
                    "spearman_negative": round(spearman_neg or 0.0, 4),
                    "direction_reversed": direction_reversed
                },
                "top10_lists": {
                    "by_old_score": top10_old_list,
                    "by_new_score": top10_new_list
                },
                "top_k_details": top_lists,
                "sample_manifest": sample_manifest
            }
        }
        
        return result
    
    def check_thresholds(self, result: Dict[str, Any]) -> Dict[str, bool]:
        """æ£€æŸ¥éªŒæ”¶é—¨æ§› - åŸºäºç¨³å®šç‰ˆæœ¬"""
        gate_config = self.config.get("shadow_gate", {})
        
        # ä¼˜å…ˆä½¿ç”¨ç¨³å®šç‰ˆæœ¬çš„æŒ‡æ ‡
        stable_spearman = result["correlations"]["stable_dataset"]["spearman"]
        top10_overlap = result["overlap_metrics"].get("top10_overlap", 0.0)
        corr_improve_pct = result["task_success_correlation"]["corr_improve_pct"]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–¹å‘é¢ å€’é—®é¢˜
        direction_ok = not result["diagnostics"]["direction_check"]["direction_reversed"]
        
        checks = {
            "spearman_pass": stable_spearman >= gate_config.get("spearman_min", 0.75),
            "top10_overlap_pass": top10_overlap >= gate_config.get("top10_overlap_min", 0.70),
            "corr_improve_pass": corr_improve_pct >= gate_config.get("corr_improve_pct", 10),
            "direction_check_pass": direction_ok
        }
        
        checks["overall_pass"] = all(checks.values())
        
        # æ·»åŠ é˜ˆå€¼ä¿¡æ¯
        checks["thresholds_used"] = gate_config
        checks["actual_values"] = {
            "stable_spearman": stable_spearman,
            "top10_overlap": top10_overlap,
            "corr_improve_pct": corr_improve_pct,
            "direction_ok": direction_ok
        }
        
        return checks
    
    def _extract_dialogue_text(self, sample: Dict) -> str:
        """æå–å¯¹è¯æ–‡æœ¬"""
        if "turns" in sample:
            parts = []
            for turn in sample["turns"]:
                if isinstance(turn, dict) and "content" in turn:
                    parts.append(turn["content"])
            return " ".join(parts)
        elif "content" in sample:
            return sample["content"]
        else:
            return str(sample)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Shadow Run - å½±å­è¿è¡Œå¯¹æ¯”")
    parser.add_argument("--n", type=int, default=245, help="æ ·æœ¬æ•°é‡")
    parser.add_argument("--seed", type=int, default=20250820, help="éšæœºç§å­")
    parser.add_argument("--stratify", action="store_true", default=True, help="æ˜¯å¦åˆ†å±‚æŠ½æ ·")
    parser.add_argument("--config", default="configs/default_config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--materialize", help="ç‰©åŒ–æ ·æœ¬åˆ°æŒ‡å®šæ–‡ä»¶")
    parser.add_argument("--dump-manifest", dest="dump_manifest", help="è¾“å‡ºæ ·æœ¬æ¸…å•åˆ°æŒ‡å®šæ–‡ä»¶")
    parser.add_argument("--tag", default="shadow_run", help="è¿è¡Œæ ‡ç­¾")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = ShadowRunEvaluator(args.config)
        
        # å¦‚æœéœ€è¦ç‰©åŒ–æ ·æœ¬ï¼Œå…ˆç”Ÿæˆå¹¶ä¿å­˜
        if args.materialize:
            logger.info(f"ç‰©åŒ–æ ·æœ¬åˆ°: {args.materialize}")
            if args.stratify:
                samples = evaluator.generate_stratified_sample(args.n, args.seed)
            else:
                samples = evaluator.load_or_generate_sample_data(args.n, args.seed)
            
            # ä¿å­˜æ ·æœ¬
            materialize_path = Path(args.materialize)
            materialize_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(materialize_path, 'w', encoding='utf-8') as f:
                for sample in samples:
                    f.write(json.dumps(sample, ensure_ascii=False) + '\n')
            
            logger.info(f"å·²ç‰©åŒ– {len(samples)} ä¸ªæ ·æœ¬")
            
            # å¦‚æœéœ€è¦ç”Ÿæˆmanifest
            if args.dump_manifest:
                manifest = {
                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'total_samples': len(samples),
                    'seed': args.seed,
                    'stratified': args.stratify,
                    'tasks': {},
                    'samples': []
                }
                
                # ç»Ÿè®¡ä»»åŠ¡åˆ†å¸ƒ
                from collections import Counter
                task_counts = Counter()
                for sample in samples:
                    task = sample.get('task', 'unknown')
                    task_counts[task] += 1
                    manifest['samples'].append({
                        'id': sample.get('id', ''),
                        'task': task,
                        'question': sample.get('question', '')[:100] + '...' if len(sample.get('question', '')) > 100 else sample.get('question', '')
                    })
                
                manifest['tasks'] = dict(task_counts)
                
                manifest_path = Path(args.dump_manifest)
                manifest_path.parent.mkdir(parents=True, exist_ok=True)
                
                with open(manifest_path, 'w', encoding='utf-8') as f:
                    json.dump(manifest, f, ensure_ascii=False, indent=2)
                
                logger.info(f"æ ·æœ¬æ¸…å•ä¿å­˜åˆ°: {manifest_path}")
        
        # æ‰§è¡Œè¯„ä¼°
        result = evaluator.run_shadow_evaluation(args.n, args.seed, args.stratify)
        
        # æ£€æŸ¥é—¨æ§›
        threshold_checks = evaluator.check_thresholds(result)
        result["threshold_checks"] = threshold_checks
        
        # ç¡®å®šè¾“å‡ºæ–‡ä»¶
        if not args.output:
            timestamp = time.strftime("%Y%m%d")
            args.output = f"reports/shadow_run_{timestamp}.json"
        
        # ä¿å­˜ç»“æœ
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # è‡ªå®šä¹‰JSONç¼–ç å™¨å¤„ç†numpyç±»å‹
        def json_serializer(obj):
            if hasattr(obj, 'item'):  # numpy scalar
                return obj.item()
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            raise TypeError(f'Object of type {obj.__class__.__name__} is not JSON serializable')
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, default=json_serializer)
        
        # æ‰“å°ç»“æœ
        print("ğŸ©º Shadow Runä½“æ£€æŠ¥å‘Š")
        print("=" * 60)
        print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {result['metadata']['n']} (ç¨³å®š: {result['metadata']['stable_samples']}, ä¸ç¨³å®š: {result['metadata']['unstable_samples']})")
        print(f"ğŸ“ˆ Spearmanç›¸å…³æ€§: å…¨é‡={result['correlations']['full_dataset']['spearman']:.4f}, ç¨³å®šç‰ˆ={result['correlations']['stable_dataset']['spearman']:.4f}")
        print(f"ğŸ¯ Top-Ké‡åˆåº¦: Top5={result['overlap_metrics'].get('top5_overlap', 0):.4f}, Top10={result['overlap_metrics'].get('top10_overlap', 0):.4f}, Top20={result['overlap_metrics'].get('top20_overlap', 0):.4f}")
        print(f"ğŸ“Š ä»»åŠ¡æˆåŠŸç›¸å…³æ€§: æ—§={result['task_success_correlation']['corr_to_success_old']:.4f}, æ–°={result['task_success_correlation']['corr_to_success_new']:.4f}, æ”¹è¿›={result['task_success_correlation']['corr_improve_pct']:.2f}%")
        print(f"âš ï¸  è´¨é‡æŒ‡æ ‡: ä¸ç¨³å®šç‡={result['quality_metrics']['unstable_rate']:.4f}, å¹¶åˆ—æ¯”ä¾‹(æ—§/æ–°)={result['quality_metrics']['ties_ratio']['old']:.3f}/{result['quality_metrics']['ties_ratio']['new']:.3f}")
        
        # æ–¹å‘æ£€æŸ¥
        direction_check = result['diagnostics']['direction_check']
        if direction_check['direction_reversed']:
            print(f"ğŸš¨ æ–¹å‘é¢ å€’è­¦å‘Š: æ­£å‘={direction_check['spearman_positive']:.4f} vs åå‘={direction_check['spearman_negative']:.4f}")
        
        # æˆåŠŸç‡åˆ†å¸ƒ
        print(f"\nğŸ“‹ ä»»åŠ¡æˆåŠŸç‡åˆ†å¸ƒ:")
        for task_type, stats in result['task_success_correlation']['success_rate_by_task'].items():
            print(f"  {task_type}: {stats['mean']:.3f} ({stats['count']}æ ·æœ¬)")
        
        print(f"\nğŸš¦ é—¨æ§›æ£€æŸ¥ (åŸºäºç¨³å®šç‰ˆæœ¬):")
        for check_name, passed in threshold_checks.items():
            if check_name.endswith('_pass'):
                status = "âœ… PASS" if passed else "âŒ FAIL"
                actual_val = threshold_checks['actual_values'].get(check_name.replace('_pass', ''), 'N/A')
                print(f"  {check_name}: {status} (å®é™…å€¼: {actual_val})")
        
        overall_status = "âœ… å…¨éƒ¨é€šè¿‡" if threshold_checks["overall_pass"] else "âŒ å­˜åœ¨æœªé€šè¿‡é¡¹"
        print(f"\nğŸ† æ€»ä½“çŠ¶æ€: {overall_status}")
        
        # æ˜¾ç¤ºè¯Šæ–­ä¿¡æ¯
        print(f"\nğŸ” Top-10æ ·æœ¬å¯¹æ¯”:")
        print("æ—§ç³»ç»ŸTop-10:", [s['sample_id'] for s in result['diagnostics']['top10_lists']['by_old_score'][:5]])
        print("æ–°ç³»ç»ŸTop-10:", [s['sample_id'] for s in result['diagnostics']['top10_lists']['by_new_score'][:5]])
        
        print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜: {output_path}")
        
        # è¿”å›é€€å‡ºç 
        sys.exit(0 if threshold_checks["overall_pass"] else 1)
        
    except Exception as e:
        logger.error(f"Shadow Runè¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
