#!/usr/bin/env python3
"""
å¤šæ ·æ€§æŒ‡æ ‡ç³»ç»Ÿ
åŸºäºGPT-5æŒ‡å¯¼å®ç°çš„æ–‡æœ¬å¤šæ ·æ€§åº¦é‡å·¥å…·
"""

import json
import math
import re
from collections import Counter, defaultdict
from typing import Dict, List, Any, Tuple
import numpy as np
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class DiversityMetrics:
    """å¤šæ ·æ€§æŒ‡æ ‡è®¡ç®—å™¨
    
    å®ç°GPT5è¦æ±‚çš„æ ¸å¿ƒæŒ‡æ ‡ï¼š
    - TTR (Type-Token Ratio)
    - Distinct-1/2 (è¯æ±‡å¤šæ ·æ€§)
    - n-gram KLæ•£åº¦
    - Zipfæ–œç‡
    - è§’è‰²/è¯­ä½“è¦†ç›–åº¦
    """
    
    def __init__(self):
        # ä¸­æ–‡åˆ†è¯ç®€åŒ–å¤„ç†
        self.word_pattern = re.compile(r'[\u4e00-\u9fff]+|[a-zA-Z]+|\d+')
        
    def tokenize_text(self, text: str) -> List[str]:
        """ç®€åŒ–çš„ä¸­æ–‡åˆ†è¯"""
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'<[^>]+>', '', text)  # ç§»é™¤æ ‡ç­¾
        text = re.sub(r'[^\u4e00-\u9fff\w\s]', ' ', text)  # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
        
        # æå–è¯æ±‡
        tokens = self.word_pattern.findall(text.lower())
        return [token for token in tokens if len(token) > 1]  # è¿‡æ»¤å•å­—ç¬¦
    
    def calculate_ttr(self, texts: List[str]) -> float:
        """è®¡ç®—Type-Token Ratio (è¯æ±‡ä¸°å¯Œåº¦)"""
        all_tokens = []
        for text in texts:
            all_tokens.extend(self.tokenize_text(text))
        
        if not all_tokens:
            return 0.0
        
        types = len(set(all_tokens))
        tokens = len(all_tokens)
        
        return types / tokens if tokens > 0 else 0.0
    
    def calculate_distinct_n(self, texts: List[str], n: int = 1) -> float:
        """è®¡ç®—Distinct-næŒ‡æ ‡"""
        all_ngrams = []
        
        for text in texts:
            tokens = self.tokenize_text(text)
            if len(tokens) >= n:
                ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]
                all_ngrams.extend(ngrams)
        
        if not all_ngrams:
            return 0.0
        
        unique_ngrams = len(set(all_ngrams))
        total_ngrams = len(all_ngrams)
        
        return unique_ngrams / total_ngrams if total_ngrams > 0 else 0.0
    
    def get_ngram_distribution(self, texts: List[str], n: int = 3) -> Dict[tuple, float]:
        """è·å–n-gramåˆ†å¸ƒ"""
        ngram_counts = Counter()
        
        for text in texts:
            tokens = self.tokenize_text(text)
            if len(tokens) >= n:
                ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]
                ngram_counts.update(ngrams)
        
        # è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        total = sum(ngram_counts.values())
        if total == 0:
            return {}
        
        return {ngram: count/total for ngram, count in ngram_counts.items()}
    
    def calculate_kl_divergence(self, dist1: Dict[tuple, float], dist2: Dict[tuple, float]) -> float:
        """è®¡ç®—KLæ•£åº¦"""
        if not dist1 or not dist2:
            return 0.0
        
        # è·å–æ‰€æœ‰n-gramçš„å¹¶é›†
        all_ngrams = set(dist1.keys()) | set(dist2.keys())
        
        kl_div = 0.0
        for ngram in all_ngrams:
            p = dist1.get(ngram, 1e-10)  # å¹³æ»‘å¤„ç†
            q = dist2.get(ngram, 1e-10)
            
            if p > 0:
                kl_div += p * math.log(p / q)
        
        return kl_div
    
    def calculate_zipf_slope(self, texts: List[str]) -> float:
        """è®¡ç®—Zipfåˆ†å¸ƒæ–œç‡"""
        # ç»Ÿè®¡è¯é¢‘
        word_counts = Counter()
        for text in texts:
            tokens = self.tokenize_text(text)
            word_counts.update(tokens)
        
        if len(word_counts) < 10:  # éœ€è¦è¶³å¤Ÿçš„è¯æ±‡
            return 0.0
        
        # æŒ‰é¢‘ç‡æ’åº
        frequencies = sorted(word_counts.values(), reverse=True)
        
        # è®¡ç®—Zipfæ–œç‡ (log-logçº¿æ€§æ‹Ÿåˆ)
        ranks = list(range(1, len(frequencies) + 1))
        
        # ä½¿ç”¨å¯¹æ•°å˜æ¢
        log_ranks = [math.log(r) for r in ranks]
        log_freqs = [math.log(f) for f in frequencies if f > 0]
        
        if len(log_ranks) != len(log_freqs) or len(log_ranks) < 2:
            return 0.0
        
        # è®¡ç®—çº¿æ€§å›å½’æ–œç‡
        n = len(log_ranks)
        sum_x = sum(log_ranks)
        sum_y = sum(log_freqs)
        sum_xy = sum(x * y for x, y in zip(log_ranks, log_freqs))
        sum_x2 = sum(x * x for x in log_ranks)
        
        if n * sum_x2 - sum_x * sum_x == 0:
            return 0.0
        
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)
        return abs(slope)  # è¿”å›ç»å¯¹å€¼
    
    def extract_style_attributes(self, dialogue_data: List[Dict]) -> Dict[str, int]:
        """æå–è¯­ä½“å’Œè§’è‰²å±æ€§ç»Ÿè®¡"""
        roles = set()
        styles = set()
        
        for dialogue in dialogue_data:
            # ä»å¯¹è¯å…ƒæ•°æ®ä¸­æå–
            meta = dialogue.get('meta', {})
            template_info = dialogue.get('template_info', {})
            
            if 'role' in meta:
                roles.add(meta['role'])
            if 'role' in template_info:
                roles.add(template_info['role'])
                
            if 'style' in meta:
                styles.add(meta['style'])
            if 'style_tag' in template_info:
                styles.add(template_info['style_tag'])
                
            # ä»å¯¹è¯å†…å®¹ä¸­æ¨æ–­
            content = self._extract_dialogue_content(dialogue)
            inferred_attrs = self._infer_style_attributes(content)
            roles.update(inferred_attrs['roles'])
            styles.update(inferred_attrs['styles'])
        
        return {
            'unique_roles': len(roles),
            'unique_styles': len(styles),
            'role_list': list(roles),
            'style_list': list(styles)
        }
    
    def _extract_dialogue_content(self, dialogue: Dict) -> str:
        """æå–å¯¹è¯å†…å®¹"""
        if 'turns' in dialogue:
            contents = []
            for turn in dialogue['turns']:
                if isinstance(turn, dict) and 'content' in turn:
                    contents.append(turn['content'])
            return ' '.join(contents)
        elif 'content' in dialogue:
            return dialogue['content']
        else:
            return str(dialogue)
    
    def _infer_style_attributes(self, content: str) -> Dict[str, List[str]]:
        """ä»å†…å®¹æ¨æ–­è¯­ä½“å’Œè§’è‰²ç‰¹å¾"""
        roles = []
        styles = []
        
        # è§’è‰²ç‰¹å¾è¯†åˆ«
        if any(word in content for word in ['ä½œä¸ºæ•°å­¦æ•™å¸ˆ', 'è§£é¢˜æ­¥éª¤', 'éªŒè¯ç­”æ¡ˆ']):
            roles.append('teacher')
        if any(word in content for word in ['æˆ‘æ¥å¸®ä½ ', 'æˆ‘çš„æ€è·¯', 'ğŸ˜Š']):
            roles.append('student')
        if any(word in content for word in ['ç ”ç©¶é—®é¢˜', 'åˆ†ææ¡†æ¶', 'ç½®ä¿¡åº¦']):
            roles.append('researcher') 
        if any(word in content for word in ['è°ƒæŸ¥ä¸»é¢˜', 'çº¿ç´¢', 'å‘ç°']):
            roles.append('journalist')
        if any(word in content for word in ['éœ€æ±‚', 'è§£å†³æ–¹æ¡ˆ', 'ä¸šåŠ¡åœºæ™¯']):
            roles.append('product_manager')
        
        # è¯­ä½“ç‰¹å¾è¯†åˆ«
        if any(word in content for word in ['ğŸ“', '#æ•°å­¦', 'âœ¨']):
            styles.append('social_media')
        if '```' in content or 'Query' in content:
            styles.append('technical')
        if any(word in content for word in ['äº²çˆ±çš„', 'æ„Ÿè°¢', 'ç¥å¥½']):
            styles.append('formal_polite')
        if any(word in content for word in ['å—¨', 'å“‡', 'è¶…æœ‰è¶£']):
            styles.append('casual')
        
        return {'roles': roles, 'styles': styles}
    
    def generate_diversity_report(self, current_data: List[Dict], 
                                baseline_data: List[Dict] = None,
                                output_file: str = None) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œæ•´çš„å¤šæ ·æ€§æŠ¥å‘Š"""
        logger.info(f"ç”Ÿæˆå¤šæ ·æ€§æŠ¥å‘Šï¼Œæ•°æ®é‡: {len(current_data)}")
        
        # æå–æ–‡æœ¬å†…å®¹
        current_texts = [self._extract_dialogue_content(d) for d in current_data]
        
        # è®¡ç®—åŸºç¡€æŒ‡æ ‡
        ttr = self.calculate_ttr(current_texts)
        distinct_1 = self.calculate_distinct_n(current_texts, 1)
        distinct_2 = self.calculate_distinct_n(current_texts, 2)
        zipf_slope = self.calculate_zipf_slope(current_texts)
        
        # è®¡ç®—è¯­ä½“å¤šæ ·æ€§
        style_stats = self.extract_style_attributes(current_data)
        
        # æ„å»ºæŠ¥å‘Š
        report = {
            "dataset_info": {
                "total_samples": len(current_data),
                "total_texts": len(current_texts),
                "avg_text_length": np.mean([len(self.tokenize_text(t)) for t in current_texts])
            },
            "lexical_diversity": {
                "ttr": round(ttr, 4),
                "distinct_1": round(distinct_1, 4),
                "distinct_2": round(distinct_2, 4),
                "zipf_slope": round(zipf_slope, 4)
            },
            "style_diversity": {
                "unique_roles": style_stats['unique_roles'],
                "unique_styles": style_stats['unique_styles'],
                "role_coverage": style_stats['role_list'],
                "style_coverage": style_stats['style_list']
            }
        }
        
        # ä¸åŸºçº¿å¯¹æ¯”ï¼ˆå¦‚æœæä¾›ï¼‰
        if baseline_data:
            baseline_texts = [self._extract_dialogue_content(d) for d in baseline_data]
            
            # è®¡ç®—KLæ•£åº¦
            current_dist = self.get_ngram_distribution(current_texts, 3)
            baseline_dist = self.get_ngram_distribution(baseline_texts, 3)
            kl_divergence = self.calculate_kl_divergence(current_dist, baseline_dist)
            
            baseline_style_stats = self.extract_style_attributes(baseline_data)
            
            report["baseline_comparison"] = {
                "kl_divergence_3gram": round(kl_divergence, 4),
                "ttr_change": round(ttr - self.calculate_ttr(baseline_texts), 4),
                "distinct_2_change": round(distinct_2 - self.calculate_distinct_n(baseline_texts, 2), 4),
                "role_coverage_increase": style_stats['unique_roles'] - baseline_style_stats['unique_roles'],
                "style_coverage_increase": style_stats['unique_styles'] - baseline_style_stats['unique_styles']
            }
        
        # éªŒæ”¶é—¨æ§›æ£€æŸ¥
        report["threshold_check"] = self._check_diversity_thresholds(report)
        
        # ä¿å­˜æŠ¥å‘Š
        if output_file:
            Path(output_file).parent.mkdir(parents=True, exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            logger.info(f"å¤šæ ·æ€§æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        
        return report
    
    def _check_diversity_thresholds(self, report: Dict) -> Dict[str, Any]:
        """æ£€æŸ¥GPT5è®¾å®šçš„å¤šæ ·æ€§é—¨æ§›"""
        thresholds = {
            "distinct_2_threshold": 0.60,
            "kl_divergence_threshold": 0.15,
            "min_role_coverage": 4,
            "min_style_coverage": 3
        }
        
        results = {}
        metrics = report["lexical_diversity"]
        style_metrics = report["style_diversity"]
        
        # æ£€æŸ¥distinct-2
        results["distinct_2_pass"] = metrics["distinct_2"] >= thresholds["distinct_2_threshold"]
        
        # æ£€æŸ¥KLæ•£åº¦ï¼ˆå¦‚æœæœ‰åŸºçº¿å¯¹æ¯”ï¼‰
        if "baseline_comparison" in report:
            kl_div = report["baseline_comparison"]["kl_divergence_3gram"]
            results["kl_divergence_pass"] = kl_div >= thresholds["kl_divergence_threshold"]
        else:
            results["kl_divergence_pass"] = None  # æ— åŸºçº¿æ•°æ®
        
        # æ£€æŸ¥è¦†ç›–åº¦
        results["role_coverage_pass"] = style_metrics["unique_roles"] >= thresholds["min_role_coverage"]
        results["style_coverage_pass"] = style_metrics["unique_styles"] >= thresholds["min_style_coverage"]
        
        # æ€»ä½“é€šè¿‡çŠ¶æ€
        required_checks = [
            results["distinct_2_pass"],
            results["role_coverage_pass"], 
            results["style_coverage_pass"]
        ]
        
        if results["kl_divergence_pass"] is not None:
            required_checks.append(results["kl_divergence_pass"])
        
        results["overall_pass"] = all(required_checks)
        results["thresholds_used"] = thresholds
        
        return results

def analyze_template_diversity(template_dir: str) -> Dict[str, Any]:
    """åˆ†ææ¨¡æ¿åŒ…çš„å¤šæ ·æ€§"""
    template_files = list(Path(template_dir).glob("*.json"))
    
    all_templates = []
    for file_path in template_files:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if 'templates' in data:
                all_templates.extend(data['templates'])
    
    # ç»Ÿè®¡æ¨¡æ¿å±æ€§
    roles = set()
    styles = set()
    task_types = set()
    
    for template in all_templates:
        roles.add(template.get('role', 'unknown'))
        styles.add(template.get('style_tag', 'unknown'))
        # ä»æ–‡ä»¶åæ¨æ–­ä»»åŠ¡ç±»å‹
        if 'math' in str(template.get('id', '')):
            task_types.add('math')
        elif 'multihop' in str(template.get('id', '')):
            task_types.add('multihop')
        elif 'clarify' in str(template.get('id', '')):
            task_types.add('clarification')
    
    return {
        "total_templates": len(all_templates),
        "unique_roles": len(roles),
        "unique_styles": len(styles),
        "unique_task_types": len(task_types),
        "role_list": list(roles),
        "style_list": list(styles),
        "task_type_list": list(task_types),
        "templates_per_task": len(all_templates) // max(1, len(task_types))
    }

def main():
    """æµ‹è¯•å‡½æ•°"""
    # æµ‹è¯•æ•°æ®
    test_dialogues = [
        {
            "id": "test_1",
            "meta": {"role": "teacher", "style": "formal"},
            "turns": [
                {"role": "user", "content": "ä¸€ä¸ªæ­£æ–¹å½¢çš„å‘¨é•¿æ˜¯20å˜ç±³ï¼Œé¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ"},
                {"role": "assistant", "content": "ä½œä¸ºæ•°å­¦æ•™å¸ˆï¼Œæˆ‘å°†ä¸ºæ‚¨æä¾›ä¸¥è°¨çš„è§£ç­”ã€‚è§£é¢˜æ­¥éª¤ï¼š1. æ±‚è¾¹é•¿ï¼š20Ã·4=5å˜ç±³ 2. æ±‚é¢ç§¯ï¼š5Â²=25å¹³æ–¹å˜ç±³"}
            ]
        },
        {
            "id": "test_2", 
            "meta": {"role": "student", "style": "casual"},
            "turns": [
                {"role": "user", "content": "å¸®æˆ‘ç®—ä¸ªé¢˜"},
                {"role": "assistant", "content": "å¥½çš„ï¼Œæˆ‘æ¥å¸®ä½ ç®—è¿™é“é¢˜ï¼ğŸ˜Š æˆ‘çš„æ€è·¯æ˜¯è¿™æ ·ï¼šé¦–å…ˆçœ‹çœ‹æ¡ä»¶ï¼Œç„¶åç”¨å…¬å¼è®¡ç®—ã€‚"}
            ]
        }
    ]
    
    # åˆ›å»ºå¤šæ ·æ€§åˆ†æå™¨
    diversity = DiversityMetrics()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = diversity.generate_diversity_report(
        test_dialogues, 
        output_file="logs/diversity_test_report.json"
    )
    
    print("ğŸ¯ å¤šæ ·æ€§åˆ†æç»“æœ:")
    print(json.dumps(report, indent=2, ensure_ascii=False))
    
    # åˆ†ææ¨¡æ¿å¤šæ ·æ€§
    template_analysis = analyze_template_diversity("templates/pack_v2/")
    print(f"\nğŸ“‹ æ¨¡æ¿å¤šæ ·æ€§åˆ†æ:")
    print(json.dumps(template_analysis, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    main()
