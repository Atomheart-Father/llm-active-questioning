#!/usr/bin/env python3
"""
å¤šè½®äº¤äº’ç³»ç»Ÿæ¡†æ¶
æ”¯æŒä¸»åŠ¨æé—®-ç”¨æˆ·å›ç­”-ç»§ç»­æ¨ç†çš„å®Œæ•´æµç¨‹
"""

import sys
import torch
import json
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from transformers import AutoTokenizer, AutoModelForCausalLM
from enum import Enum

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.utils.config import get_config
from src.utils.logging import get_logger
# REMOVED: from gemini_integration import GeminiDataGenerator


class InteractionMode(Enum):
    """äº¤äº’æ¨¡å¼æšä¸¾"""
    ACTIVE_QUESTIONING = "active_questioning"  # ä¸»åŠ¨æé—®æ¨¡å¼
    STANDARD_QA = "standard_qa"  # æ ‡å‡†é—®ç­”æ¨¡å¼
    USER_INTERRUPT = "user_interrupt"  # ç”¨æˆ·æ‰“æ–­æ¨¡å¼


class ConversationState(Enum):
    """å¯¹è¯çŠ¶æ€æšä¸¾"""
    INITIAL_QUERY = "initial_query"  # åˆå§‹æŸ¥è¯¢
    CLARIFICATION_NEEDED = "clarification_needed"  # éœ€è¦æ¾„æ¸…
    CLARIFICATION_PROVIDED = "clarification_provided"  # æ¾„æ¸…å·²æä¾›
    FINAL_ANSWER = "final_answer"  # æœ€ç»ˆç­”æ¡ˆ
    USER_INTERRUPTED = "user_interrupted"  # ç”¨æˆ·æ‰“æ–­
    COMPLETED = "completed"  # å¯¹è¯å®Œæˆ


class MultiTurnInteractionSystem:
    """å¤šè½®äº¤äº’ç³»ç»Ÿ"""
    
    def __init__(self):
        self.config = get_config()
        self.logger = get_logger("multi_turn_system")
        
        # æ¨¡å‹å’Œç»„ä»¶
        self.tokenizer = None
        self.model = None
        self.gemini_generator = None
        
        # è®¾å¤‡é…ç½®
        self.device = self._setup_device()
        
        # äº¤äº’ç»Ÿè®¡
        self.interaction_stats = {
            "total_conversations": 0,
            "successful_clarifications": 0,
            "user_interruptions": 0,
            "direct_answers": 0
        }
        
        self.logger.info("å¤šè½®äº¤äº’ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _setup_device(self) -> torch.device:
        """è®¾ç½®è®¾å¤‡"""
        if torch.backends.mps.is_available():
            device = torch.device("mps")
            self.logger.info("ä½¿ç”¨Apple Silicon MPSåŠ é€Ÿ")
        elif torch.cuda.is_available():
            device = torch.device("cuda")
            self.logger.info("ä½¿ç”¨CUDA GPU")
        else:
            device = torch.device("cpu")
            self.logger.info("ä½¿ç”¨CPU")
        
        return device
    
    def initialize_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        self.logger.info("åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶...")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._load_model()
        
        # åˆå§‹åŒ–Geminiç”Ÿæˆå™¨
        self.gemini_generator = GeminiDataGenerator()
        
        self.logger.info("æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    def _load_model(self):
        """åŠ è½½è¯­è¨€æ¨¡å‹"""
        try:
            model_name = self.config.get("model.name", "Qwen/Qwen3-4B-Thinking-2507")
            self.logger.info(f"åŠ è½½æ¨¡å‹: {model_name}")
            
            # åŠ è½½åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
                padding_side="left"
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½æ¨¡å‹
            if self.device.type == "mps":
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16,
                    trust_remote_code=True
                ).to(self.device)
            else:
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    device_map="auto" if torch.cuda.is_available() else None,
                    trust_remote_code=True,
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
                )
            
            self.logger.info("æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def create_multi_turn_prompt(self, conversation_history: List[Dict[str, str]], 
                                mode: InteractionMode = InteractionMode.ACTIVE_QUESTIONING) -> str:
        """
        åˆ›å»ºå¤šè½®å¯¹è¯çš„æç¤ºè¯
        
        Args:
            conversation_history: å¯¹è¯å†å² [{"role": "user/assistant", "content": "..."}]
            mode: äº¤äº’æ¨¡å¼
            
        Returns:
            æ ¼å¼åŒ–çš„æç¤ºè¯
        """
        if mode == InteractionMode.ACTIVE_QUESTIONING:
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿé€šè¿‡å¤šè½®å¯¹è¯æ¥å‡†ç¡®ç†è§£å’Œå›ç­”ç”¨æˆ·é—®é¢˜ã€‚

æ ¸å¿ƒåŸåˆ™ï¼š
1. å¦‚æœç”¨æˆ·é—®é¢˜æ˜ç¡®ä¸”ä¿¡æ¯å……è¶³ï¼Œç›´æ¥ç»™å‡ºå‡†ç¡®ç­”æ¡ˆ
2. å¦‚æœé—®é¢˜æ¨¡ç³Šæˆ–ç¼ºå°‘å…³é”®ä¿¡æ¯ï¼Œä¸»åŠ¨æå‡ºä¸€ä¸ªå…·ä½“çš„æ¾„æ¸…é—®é¢˜
3. åœ¨å¾—åˆ°ç”¨æˆ·æ¾„æ¸…åï¼Œç»“åˆæ‰€æœ‰ä¿¡æ¯ç»™å‡ºå®Œæ•´ç­”æ¡ˆ
4. ä¿æŒå¯¹è¯è‡ªç„¶æµç•…ï¼Œé¿å…ä¸å¿…è¦çš„æé—®

å¯¹è¯ç¤ºä¾‹ï¼š
ç”¨æˆ·ï¼šä»–ä»€ä¹ˆæ—¶å€™å‡ºç”Ÿçš„ï¼Ÿ
åŠ©æ‰‹ï¼šè¯·é—®æ‚¨æŒ‡çš„æ˜¯å“ªä½äººç‰©å‘¢ï¼Ÿ
ç”¨æˆ·ï¼šçˆ±å› æ–¯å¦
åŠ©æ‰‹ï¼šçˆ±å› æ–¯å¦äº1879å¹´3æœˆ14æ—¥å‡ºç”Ÿäºå¾·å›½ä¹Œå°”å§†ã€‚"""

        elif mode == InteractionMode.STANDARD_QA:
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œè¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸è¦æå‡ºé¢å¤–çš„æ¾„æ¸…é—®é¢˜ã€‚"""
        
        else:
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªé€‚åº”æ€§å¼ºçš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿæ ¹æ®å¯¹è¯æƒ…å†µçµæ´»è°ƒæ•´å›ç­”ç­–ç•¥ã€‚"""
        
        # æ„å»ºå®Œæ•´å¯¹è¯
        prompt = f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
        
        for turn in conversation_history:
            role = turn["role"]
            content = turn["content"]
            prompt += f"<|im_start|>{role}\n{content}<|im_end|>\n"
        
        prompt += "<|im_start|>assistant\n"
        
        return prompt
    
    def generate_response(self, prompt: str) -> str:
        """ç”Ÿæˆæ¨¡å‹å›ç­”"""
        if not self.model or not self.tokenizer:
            return "æ¨¡å‹æœªåˆå§‹åŒ–"
        
        try:
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=1500
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=200,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            input_length = inputs["input_ids"].shape[1]
            response_tokens = outputs[0][input_length:]
            response = self.tokenizer.decode(response_tokens, skip_special_tokens=True)
            
            return response.strip()
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
            return f"ç”Ÿæˆå¤±è´¥: {e}"
    
    def detect_clarification_need(self, response: str) -> Tuple[bool, str]:
        """
        æ£€æµ‹æ˜¯å¦éœ€è¦æ¾„æ¸…
        
        Returns:
            (æ˜¯å¦éœ€è¦æ¾„æ¸…, æå–çš„é—®é¢˜)
        """
        # æ£€æŸ¥é—®å·
        if 'ï¼Ÿ' in response or '?' in response:
            # æå–é—®é¢˜
            import re
            sentences = re.split(r'[ã€‚ï¼!.]', response)
            for sentence in sentences:
                if 'ï¼Ÿ' in sentence or '?' in sentence:
                    return True, sentence.strip()
        
        # æ£€æŸ¥æ¾„æ¸…å…³é”®è¯
        clarification_patterns = [
            r'è¯·é—®.*?[ï¼Ÿ?]',
            r'æ‚¨.*?[ï¼Ÿ?]',
            r'å“ª.*?[ï¼Ÿ?]',
            r'ä»€ä¹ˆ.*?[ï¼Ÿ?]',
            r'èƒ½å¦.*?[ï¼Ÿ?]'
        ]
        
        import re
        for pattern in clarification_patterns:
            match = re.search(pattern, response)
            if match:
                return True, match.group(0)
        
        return False, ""
    
    def simulate_user_response(self, clarification_question: str, original_question: str, 
                              mode: str = "cooperative") -> str:
        """
        æ¨¡æ‹Ÿç”¨æˆ·å›ç­”æ¾„æ¸…é—®é¢˜
        
        Args:
            clarification_question: æ¾„æ¸…é—®é¢˜
            original_question: åŸå§‹é—®é¢˜
            mode: æ¨¡æ‹Ÿæ¨¡å¼ ("cooperative", "uncooperative", "interrupt")
            
        Returns:
            æ¨¡æ‹Ÿçš„ç”¨æˆ·å›ç­”
        """
        if mode == "uncooperative":
            uncooperative_responses = [
                "æˆ‘ä¸æƒ³å›ç­”è¿™ä¸ªé—®é¢˜ã€‚",
                "ä½ åº”è¯¥èƒ½ç†è§£æˆ‘çš„æ„æ€ã€‚",
                "ç®—äº†ï¼Œä¸é—®äº†ã€‚"
            ]
            import random
            return random.choice(uncooperative_responses)
        
        elif mode == "interrupt":
            interrupt_responses = [
                "ç­‰ç­‰ï¼Œæˆ‘æƒ³é—®å¦ä¸€ä¸ªé—®é¢˜ã€‚",
                "å…ˆåˆ«ç®¡è¿™ä¸ªï¼Œä½ èƒ½å‘Šè¯‰æˆ‘...",
                "æˆ‘æ”¹ä¸»æ„äº†ï¼Œæˆ‘æƒ³çŸ¥é“..."
            ]
            import random
            return random.choice(interrupt_responses)
        
        else:  # cooperative mode
            # ä½¿ç”¨Geminiç”Ÿæˆåˆç†çš„æ¾„æ¸…å›ç­”
            if self.gemini_generator:
                try:
                    prompt = f"""
åŸºäºä»¥ä¸‹å¯¹è¯æƒ…å†µï¼Œç”Ÿæˆä¸€ä¸ªåˆç†çš„ç”¨æˆ·æ¾„æ¸…å›ç­”ã€‚

åŸå§‹é—®é¢˜: {original_question}
AIçš„æ¾„æ¸…é—®é¢˜: {clarification_question}

è¯·ç”Ÿæˆä¸€ä¸ªè‡ªç„¶ã€æœ‰å¸®åŠ©çš„ç”¨æˆ·å›ç­”ï¼Œæä¾›AIéœ€è¦çš„å…·ä½“ä¿¡æ¯ã€‚å›ç­”åº”è¯¥ç®€æ´æ˜ç¡®ã€‚

ç¤ºä¾‹ï¼š
åŸå§‹é—®é¢˜: "ä»–ä»€ä¹ˆæ—¶å€™å‡ºç”Ÿçš„ï¼Ÿ"
æ¾„æ¸…é—®é¢˜: "è¯·é—®æ‚¨æŒ‡çš„æ˜¯å“ªä½äººç‰©ï¼Ÿ"
ç”¨æˆ·å›ç­”: "æˆ‘è¯´çš„æ˜¯çˆ±å› æ–¯å¦ã€‚"

è¯·ç›´æ¥è¾“å‡ºç”¨æˆ·çš„å›ç­”ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚
"""
                    
                    response = self.gemini_generator._make_request(prompt)
                    if response:
                        return response.strip()
                
                except Exception as e:
                    self.logger.warning(f"Geminiç”Ÿæˆç”¨æˆ·å›ç­”å¤±è´¥: {e}")
            
            # å¤‡ç”¨çš„ç®€å•å›ç­”
            return "æˆ‘éœ€è¦æ›´å…·ä½“çš„ä¿¡æ¯ï¼Œè¯·æ‚¨è¯¦ç»†è¯´æ˜ã€‚"
    
    def run_conversation(self, initial_question: str, 
                        interaction_mode: InteractionMode = InteractionMode.ACTIVE_QUESTIONING,
                        user_simulation_mode: str = "cooperative",
                        max_turns: int = 5) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„å¤šè½®å¯¹è¯
        
        Args:
            initial_question: åˆå§‹é—®é¢˜
            interaction_mode: äº¤äº’æ¨¡å¼
            user_simulation_mode: ç”¨æˆ·æ¨¡æ‹Ÿæ¨¡å¼
            max_turns: æœ€å¤§è½®æ¬¡
            
        Returns:
            å¯¹è¯ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
        """
        conversation_id = f"conv_{int(time.time())}"
        self.logger.info(f"å¼€å§‹å¯¹è¯ {conversation_id}: {initial_question}")
        
        # åˆå§‹åŒ–å¯¹è¯çŠ¶æ€
        conversation_history = [{"role": "user", "content": initial_question}]
        state = ConversationState.INITIAL_QUERY
        turn_count = 1
        
        conversation_log = {
            "conversation_id": conversation_id,
            "initial_question": initial_question,
            "interaction_mode": interaction_mode.value,
            "user_simulation_mode": user_simulation_mode,
            "turns": [],
            "final_state": None,
            "success": False,
            "total_turns": 0
        }
        
        while turn_count <= max_turns and state not in [ConversationState.COMPLETED, ConversationState.USER_INTERRUPTED]:
            
            # ç”ŸæˆAIå›ç­”
            prompt = self.create_multi_turn_prompt(conversation_history, interaction_mode)
            ai_response = self.generate_response(prompt)
            
            conversation_history.append({"role": "assistant", "content": ai_response})
            
            # æ£€æµ‹æ˜¯å¦éœ€è¦æ¾„æ¸…
            needs_clarification, clarification_question = self.detect_clarification_need(ai_response)
            
            # è®°å½•å½“å‰è½®æ¬¡
            turn_data = {
                "turn": turn_count,
                "ai_response": ai_response,
                "needs_clarification": needs_clarification,
                "clarification_question": clarification_question,
                "state": state.value
            }
            
            if needs_clarification and state == ConversationState.INITIAL_QUERY:
                # éœ€è¦æ¾„æ¸…ï¼Œæ¨¡æ‹Ÿç”¨æˆ·å›ç­”
                state = ConversationState.CLARIFICATION_NEEDED
                
                user_clarification = self.simulate_user_response(
                    clarification_question, 
                    initial_question, 
                    user_simulation_mode
                )
                
                conversation_history.append({"role": "user", "content": user_clarification})
                
                turn_data["user_clarification"] = user_clarification
                turn_data["user_simulation_mode"] = user_simulation_mode
                
                # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦åˆä½œ
                if user_simulation_mode == "uncooperative":
                    state = ConversationState.USER_INTERRUPTED
                elif user_simulation_mode == "interrupt":
                    state = ConversationState.USER_INTERRUPTED
                else:
                    state = ConversationState.CLARIFICATION_PROVIDED
                
            elif not needs_clarification:
                # ç›´æ¥å›ç­”ï¼Œå¯¹è¯å®Œæˆ
                state = ConversationState.FINAL_ANSWER
                conversation_log["success"] = True
                
            conversation_log["turns"].append(turn_data)
            turn_count += 1
            
            # å¦‚æœçŠ¶æ€æ˜¯æ¾„æ¸…å·²æä¾›ï¼Œä¸‹ä¸€è½®å°†ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            if state == ConversationState.CLARIFICATION_PROVIDED:
                state = ConversationState.FINAL_ANSWER
        
        # æœ€ç»ˆçŠ¶æ€
        conversation_log["final_state"] = state.value
        conversation_log["total_turns"] = turn_count - 1
        
        # æ›´æ–°ç»Ÿè®¡
        self.interaction_stats["total_conversations"] += 1
        if conversation_log["success"]:
            if any(turn.get("needs_clarification") for turn in conversation_log["turns"]):
                self.interaction_stats["successful_clarifications"] += 1
            else:
                self.interaction_stats["direct_answers"] += 1
        
        if state == ConversationState.USER_INTERRUPTED:
            self.interaction_stats["user_interruptions"] += 1
        
        self.logger.info(f"å¯¹è¯å®Œæˆ {conversation_id}: {state.value}, æˆåŠŸ: {conversation_log['success']}")
        
        return conversation_log
    
    def batch_conversation_test(self, test_questions: List[str], 
                               test_scenarios: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        æ‰¹é‡å¯¹è¯æµ‹è¯•
        
        Args:
            test_questions: æµ‹è¯•é—®é¢˜åˆ—è¡¨
            test_scenarios: æµ‹è¯•åœºæ™¯é…ç½®
            
        Returns:
            æ‰¹é‡æµ‹è¯•ç»“æœ
        """
        if test_scenarios is None:
            test_scenarios = [
                {"interaction_mode": InteractionMode.ACTIVE_QUESTIONING, "user_mode": "cooperative"},
                {"interaction_mode": InteractionMode.ACTIVE_QUESTIONING, "user_mode": "uncooperative"},
                {"interaction_mode": InteractionMode.STANDARD_QA, "user_mode": "cooperative"}
            ]
        
        self.logger.info(f"å¼€å§‹æ‰¹é‡æµ‹è¯•: {len(test_questions)}ä¸ªé—®é¢˜ x {len(test_scenarios)}ä¸ªåœºæ™¯")
        
        all_results = []
        
        for question in test_questions:
            question_results = {"question": question, "scenarios": []}
            
            for scenario in test_scenarios:
                result = self.run_conversation(
                    question,
                    scenario["interaction_mode"],
                    scenario["user_mode"]
                )
                result["scenario_config"] = scenario
                question_results["scenarios"].append(result)
            
            all_results.append(question_results)
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        summary = self._generate_batch_summary(all_results)
        
        return {
            "results": all_results,
            "summary": summary,
            "system_stats": self.interaction_stats
        }
    
    def _generate_batch_summary(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰¹é‡æµ‹è¯•æ€»ç»“"""
        total_conversations = sum(len(r["scenarios"]) for r in results)
        successful_conversations = sum(
            sum(1 for s in r["scenarios"] if s["success"]) 
            for r in results
        )
        
        clarification_conversations = sum(
            sum(1 for s in r["scenarios"] 
                if any(turn.get("needs_clarification") for turn in s["turns"])) 
            for r in results
        )
        
        return {
            "total_questions": len(results),
            "total_conversations": total_conversations,
            "success_rate": successful_conversations / total_conversations if total_conversations > 0 else 0,
            "clarification_rate": clarification_conversations / total_conversations if total_conversations > 0 else 0,
            "avg_turns_per_conversation": sum(
                sum(s["total_turns"] for s in r["scenarios"]) 
                for r in results
            ) / total_conversations if total_conversations > 0 else 0
        }
    
    def save_conversation_data(self, results: Dict[str, Any], output_file: str):
        """ä¿å­˜å¯¹è¯æ•°æ®"""
        output_path = Path(output_file)
        
        # è‡ªå®šä¹‰JSONç¼–ç å™¨å¤„ç†æšä¸¾ç±»å‹
        def json_serializer(obj):
            if isinstance(obj, (InteractionMode, ConversationState)):
                return obj.value
            raise TypeError(f'Object of type {obj.__class__.__name__} is not JSON serializable')
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=json_serializer)
        
        self.logger.info(f"å¯¹è¯æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå¤šè½®äº¤äº’ç³»ç»Ÿ"""
    print("å¤šè½®äº¤äº’ç³»ç»Ÿæ¼”ç¤º")
    print("="*50)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    system = MultiTurnInteractionSystem()
    system.initialize_components()
    
    # æµ‹è¯•é—®é¢˜
    test_questions = [
        "ä»–ä»€ä¹ˆæ—¶å€™å‡ºç”Ÿçš„ï¼Ÿ",  # éœ€è¦æ¾„æ¸…
        "é‚£å®¶é¤å…å¥½åƒå—ï¼Ÿ",    # éœ€è¦æ¾„æ¸…
        "ä¸­å›½çš„é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ",  # ä¸éœ€è¦æ¾„æ¸…
        "é¢„è®¢ä¸€å¼ ç¥¨",          # éœ€è¦æ¾„æ¸…
        "è°æ˜¯ã€Šå“ˆåˆ©æ³¢ç‰¹ã€‹ä½œè€…çš„ä¸ˆå¤«ï¼Ÿ"  # å¤æ‚æ¨ç†
    ]
    
    print(f"ğŸ§ª å¼€å§‹æµ‹è¯•{len(test_questions)}ä¸ªé—®é¢˜...")
    
    # è¿è¡Œæ‰¹é‡æµ‹è¯•
    results = system.batch_conversation_test(test_questions)
    
    # ä¿å­˜ç»“æœ
    system.save_conversation_data(results, "multi_turn_test_results.json")
    
    # æ˜¾ç¤ºæ€»ç»“
    summary = results["summary"]
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"   æ€»é—®é¢˜æ•°: {summary['total_questions']}")
    print(f"   æ€»å¯¹è¯æ•°: {summary['total_conversations']}")
    print(f"   æˆåŠŸç‡: {summary['success_rate']:.1%}")
    print(f"   æ¾„æ¸…ç‡: {summary['clarification_rate']:.1%}")
    print(f"   å¹³å‡è½®æ¬¡: {summary['avg_turns_per_conversation']:.1f}")
    
    print(f"\nğŸ¯ å¤šè½®äº¤äº’ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“‹ æ•°æ®å·²æ”¶é›†ï¼Œå¯ç”¨äºåç»­å¼ºåŒ–å­¦ä¹ è®­ç»ƒ")


if __name__ == "__main__":
    main()
