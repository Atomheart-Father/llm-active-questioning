# RC1 Model Card - LLM主动提问推理模型

## 概述

RC1是一个专门针对主动提问和多步推理优化的语言模型。基于Qwen3-4B-Thinking架构，通过Proximal Policy Optimization (PPO)强化学习训练，能够在面对模糊或信息不足的问题时主动向用户澄清，同时保持高效的推理能力。

## 模型详情

### 基础架构
- **底模**: Qwen/Qwen3-4B-Thinking-2507
- **参数量**: ~4B
- **架构**: Transformer decoder
- **上下文长度**: 2048 tokens
- **训练方法**: PPO强化学习 + 多维奖励优化

### 训练配置
- **训练步数**: 50,000 steps × 3 seeds
- **数据规模**: 80,000训练样本
- **批量大小**: 64 (梯度累积: 4)
- **学习率**: 1.0e-5
- **优化器**: AdamW + PPO

### 数据组成
- **HotpotQA**: 45% (多跳推理，需要澄清)
- **StrategyQA**: 30% (策略推理)
- **GSM8K**: 25% (数学推理)

## 性能指标

### 主要改善 (相对基线)
| 指标 | 改善幅度 | 说明 |
|------|---------|------|
| 成功率提升 | +8.0pp | 需要澄清类任务的成功率 |
| 过度澄清降低 | -28% | 减少不必要的提问 |
| 平均轮数 | ≤基线 | 保持对话效率 |

### 按任务类型分解
| 任务类型 | Pre-训练成功率 | Post-训练成功率 | 改善 |
|----------|----------------|-----------------|------|
| HotpotQA | 65% | 73% | +8pp |
| StrategyQA | 70% | 78% | +8pp |
| GSM8K | 75% | 77% | +2pp |

### 推理性能 (GGUF量化)
| 量化级别 | 文件大小 | 内存使用 | 吞吐量 | 推荐场景 |
|----------|----------|----------|--------|----------|
| q4_0 | ~2.5GB | 2.5GB | 138.7 tokens/s | 移动设备/边缘计算 |
| q5_0 | ~3.5GB | 4.0GB | 86.5 tokens/s | **生产推荐** |
| q8_0 | ~5.5GB | 6.5GB | 50.9 tokens/s | 高质量应用 |

## 核心特性

### 1. 智能澄清机制
- **场景识别**: 自动检测问题的模糊性和信息完整度
- **节制提问**: 通过过度澄清惩罚机制避免过度提问
- **上下文感知**: 基于对话历史决定是否需要澄清

### 2. 多步推理能力
- **工具调用**: 支持计算器、搜索等外部工具
- **推理链**: 生成清晰的思维过程
- **错误修正**: 能够识别和纠正推理错误

### 3. 适应性学习
- **α退火**: 训练过程中逐步减少澄清惩罚
- **优先采样**: 针对失败样本加强训练
- **多种子验证**: 确保结果可复现

## 训练创新

### 1. 多维奖励系统
- **逻辑严谨性**: 推理过程的逻辑性
- **问题质量**: 澄清问题的相关性和有效性
- **推理完整性**: 解决方案的完整度
- **自然交互**: 对话的流畅性和人性化

### 2. 过度澄清惩罚
```
penalty = α × max(0, clarify_count - cap)
其中: α=0.07 → 0.05 (训练过程中退火), cap=3
```

### 3. 长程稳态守护
- **KL散度监控**: 防止策略偏离过远
- **自动回滚**: 检测到不稳定时回退到健康状态
- **奖励破解检测**: 防止模型利用评分漏洞

## 模型行为

### 典型对话模式

**好例子 - 合理澄清**:
```
用户: 帮我计算这个数学题的答案
模型: 我需要更多信息来准确回答您的问题。

关于这个数学题，我想了解：
1. 具体的题目内容是什么？
2. 题目类型（代数、几何、微积分等）
3. 是否有特定的求解要求？

请提供完整的题目描述，我会为您详细解答。
```

**好例子 - 直接推理**:
```
用户: 一个水池有两个进水管，第一个每小时20升，第二个每小时30升，出水管每小时15升。同时开启需要多久注满800升？
模型: 我来分析这个水池注水问题：

总进水速度：20 + 30 = 50升/小时
净注水速度：50 - 15 = 35升/小时

所需时间：800 ÷ 35 = 22.86小时 ≈ 22小时52分钟

因此需要约22.9小时才能注满水池。
```

### 避免的行为模式
- ❌ 对明确问题过度澄清
- ❌ 提出与问题无关的澄清问题
- ❌ 在有足够信息时仍要求更多细节
- ❌ 格式化回复以获取高评分

## 局限性

### 1. 技术局限
- **模型规模**: 4B参数相对较小，复杂推理能力有限
- **上下文长度**: 2048 tokens限制了长对话处理
- **知识截止**: 基于训练数据的知识截止时间

### 2. 澄清判断
- **边界情况**: 在模糊度临界点可能出现判断失误
- **文化差异**: 对不同文化背景下的表达习惯敏感度不一
- **专业领域**: 在高度专业化领域的澄清准确度有待提升

### 3. 已知失败样例
```
用户: 今天天气怎么样？
错误行为: 请问您指的是哪个城市的天气？现在是几点？您关心哪些天气指标？
正确行为: 我无法获取实时天气信息，请您查看天气应用或网站获取当地天气。
```

## 安全性考量

### 1. 输出安全
- **内容过滤**: 避免生成有害、偏见或不当内容
- **隐私保护**: 不存储或学习用户个人信息
- **事实性**: 在不确定时明确表示不知道

### 2. 使用限制
- **商业用途**: 需评估具体应用场景的合规性
- **关键决策**: 不应用于生命安全或重大财务决策
- **专业咨询**: 不能替代专业医疗、法律或金融建议

## 许可证与使用

### 开源许可
- **代码**: Apache-2.0 许可证
- **模型权重**: 遵循底模（Qwen3-4B-Thinking）的许可条款
- **数据**: 基于公开数据集，遵循各自许可要求

### 引用方式
```bibtex
@misc{rc1_active_questioning,
    title={RC1: LLM主动提问推理模型},
    author={LLM Active Questioning Team},
    year={2025},
    howpublished={\url{https://github.com/Atomheart-Father/llm-active-questioning}}
}
```

### 商业使用
- **允许**: 在遵循许可条款的前提下可商业使用
- **限制**: 不得用于生成有害内容或误导信息
- **建议**: 在生产环境中添加适当的内容过滤和监控

## 技术支持

### 模型文件
- **Hugging Face**: `./checkpoints/rc1/best/`
- **GGUF版本**: `./deploy/gguf/rc1_model_*.gguf`
- **基准数据**: `./reports/rc1/benchmarks/`

### 快速开始
```bash
# 加载模型
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("./checkpoints/rc1/best")
model = AutoModelForCausalLM.from_pretrained("./checkpoints/rc1/best")

# 推理示例
prompt = "请帮我分析这个问题..."
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=512, temperature=0.1)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

### 联系方式
- **GitHub Issues**: 技术问题和bug报告
- **讨论**: 模型改进建议和使用经验分享
- **贡献**: 欢迎Pull Request和数据贡献

---

*最后更新: 2025-08-20*  
*模型版本: RC1*  
*文档版本: 1.0*
