#!/usr/bin/env python3
"""
Score Cache CLI - ç¼“å­˜ç®¡ç†å‘½ä»¤è¡Œå·¥å…·
åŸºäºGPT-5æŒ‡å¯¼å®ç°çš„ç¼“å­˜ç®¡ç†å·¥å…·
"""

import argparse
import json
import sqlite3
import time
import sys
import os
from pathlib import Path
from typing import Dict, List, Any
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

logger = logging.getLogger(__name__)

class ScoreCacheCLI:
    """è¯„åˆ†ç¼“å­˜CLIç®¡ç†å™¨"""
    
    def __init__(self, db_path: str = "gemini_cache.sqlite"):
        self.db_path = db_path
        if not os.path.exists(db_path):
            print(f"âš ï¸  ç¼“å­˜æ•°æ®åº“ä¸å­˜åœ¨: {db_path}")
            print("è¯·å…ˆè¿è¡Œè¯„åˆ†ç³»ç»Ÿåˆ›å»ºç¼“å­˜æ•°æ®åº“")
            sys.exit(1)
    
    def get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        return sqlite3.connect(self.db_path, check_same_thread=False)
    
    def stat(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self.get_connection() as conn:
            cur = conn.cursor()
            
            # åŸºç¡€ç»Ÿè®¡
            cur.execute("SELECT COUNT(*) FROM gemini_score_cache")
            total_entries = cur.fetchone()[0]
            
            # æœ‰æ•ˆç¼“å­˜ç»Ÿè®¡
            now = int(time.time() * 1000)
            cur.execute("SELECT COUNT(*) FROM gemini_score_cache WHERE expiry_at > ?", (now,))
            valid_entries = cur.fetchone()[0]
            
            # çŠ¶æ€ç»Ÿè®¡
            cur.execute("""
                SELECT status, COUNT(*) 
                FROM gemini_score_cache 
                GROUP BY status
            """)
            status_counts = dict(cur.fetchall())
            
            # å»¶è¿Ÿç»Ÿè®¡
            cur.execute("""
                SELECT AVG(api_latency_ms), MAX(api_latency_ms), MIN(api_latency_ms)
                FROM gemini_score_cache 
                WHERE api_latency_ms > 0
            """)
            latency_stats = cur.fetchone()
            
            # æ–¹å·®ç»Ÿè®¡
            cur.execute("""
                SELECT AVG(variance), MAX(variance), 
                       COUNT(CASE WHEN variance > 0.08 THEN 1 END)
                FROM gemini_score_cache 
                WHERE variance IS NOT NULL
            """)
            variance_stats = cur.fetchone()
            
            # TTLå³å°†è¿‡æœŸç»Ÿè®¡
            hour_from_now = now + (60 * 60 * 1000)
            cur.execute("""
                SELECT COUNT(*) FROM gemini_score_cache 
                WHERE expiry_at > ? AND expiry_at < ?
            """, (now, hour_from_now))
            expiring_soon = cur.fetchone()[0]
            
            # æ¨¡å‹/ç‰ˆæœ¬åˆ†å¸ƒ
            cur.execute("""
                SELECT scoring_spec, COUNT(*) 
                FROM gemini_score_cache 
                GROUP BY scoring_spec 
                ORDER BY COUNT(*) DESC
                LIMIT 10
            """)
            spec_distribution = cur.fetchall()
            
            return {
                "database_info": {
                    "db_path": self.db_path,
                    "db_size_mb": round(os.path.getsize(self.db_path) / (1024*1024), 2),
                    "last_updated": time.strftime("%Y-%m-%d %H:%M:%S")
                },
                "cache_stats": {
                    "total_entries": total_entries,
                    "valid_entries": valid_entries,
                    "expired_entries": total_entries - valid_entries,
                    "hit_rate": round(valid_entries / max(1, total_entries), 3),
                    "expiring_in_1h": expiring_soon
                },
                "status_distribution": status_counts,
                "performance_stats": {
                    "avg_latency_ms": round(latency_stats[0] or 0, 1),
                    "max_latency_ms": latency_stats[1] or 0,
                    "min_latency_ms": latency_stats[2] or 0
                },
                "variance_stats": {
                    "avg_variance": round(variance_stats[0] or 0, 4),
                    "max_variance": round(variance_stats[1] or 0, 4),
                    "unstable_count": variance_stats[2] or 0,
                    "unstable_rate": round((variance_stats[2] or 0) / max(1, total_entries), 3)
                },
                "spec_distribution": [
                    {"spec": spec, "count": count} 
                    for spec, count in spec_distribution
                ]
            }
    
    def invalidate(self, spec_pattern: str = None, model: str = None, 
                  version: str = None, dims: str = None) -> int:
        """å¤±æ•ˆç¼“å­˜é¡¹"""
        with self.get_connection() as conn:
            cur = conn.cursor()
            
            if spec_pattern:
                # ç›´æ¥æŒ‰æ¨¡å¼å¤±æ•ˆ
                cur.execute("""
                    UPDATE gemini_score_cache 
                    SET expiry_at = 0 
                    WHERE scoring_spec LIKE ?
                """, (f"%{spec_pattern}%",))
                
            else:
                # æ„å»ºå…·ä½“çš„æ¨¡å¼
                patterns = []
                if model:
                    patterns.append(f"{model}|%")
                if version:
                    patterns.append(f"%|v={version}|%")
                if dims:
                    patterns.append(f"%|dims={dims}")
                
                if not patterns:
                    print("âŒ è¯·æä¾›è‡³å°‘ä¸€ä¸ªå¤±æ•ˆæ¡ä»¶")
                    return 0
                
                # æ‰§è¡Œå¤±æ•ˆ
                total_invalidated = 0
                for pattern in patterns:
                    cur.execute("""
                        UPDATE gemini_score_cache 
                        SET expiry_at = 0 
                        WHERE scoring_spec LIKE ?
                    """, (pattern,))
                    total_invalidated += cur.rowcount
            
            conn.commit()
            invalidated_count = cur.rowcount if spec_pattern else total_invalidated
            
            print(f"âœ… å·²å¤±æ•ˆ {invalidated_count} ä¸ªç¼“å­˜é¡¹")
            return invalidated_count
    
    def replay_bad(self, reason: str = "unstable", limit: int = 100) -> List[Dict[str, Any]]:
        """é‡æ”¾é—®é¢˜æ ·æœ¬"""
        with self.get_connection() as conn:
            cur = conn.cursor()
            
            if reason == "unstable":
                # æŸ¥æ‰¾é«˜æ–¹å·®æ ·æœ¬
                cur.execute("""
                    SELECT key, payload_norm, scoring_spec, variance, api_latency_ms
                    FROM gemini_score_cache
                    WHERE variance > 0.08
                    ORDER BY variance DESC
                    LIMIT ?
                """, (limit,))
                
            elif reason == "high_latency":
                # æŸ¥æ‰¾é«˜å»¶è¿Ÿæ ·æœ¬
                cur.execute("""
                    SELECT key, payload_norm, scoring_spec, variance, api_latency_ms
                    FROM gemini_score_cache
                    WHERE api_latency_ms > 5000
                    ORDER BY api_latency_ms DESC
                    LIMIT ?
                """, (limit,))
                
            elif reason == "error":
                # æŸ¥æ‰¾é”™è¯¯æ ·æœ¬
                cur.execute("""
                    SELECT key, payload_norm, scoring_spec, variance, api_latency_ms
                    FROM gemini_score_cache
                    WHERE status = 'error'
                    ORDER BY updated_at DESC
                    LIMIT ?
                """, (limit,))
                
            else:
                print(f"âŒ ä¸æ”¯æŒçš„åŸå› : {reason}")
                print("æ”¯æŒçš„åŸå› : unstable, high_latency, error")
                return []
            
            results = cur.fetchall()
            
            bad_samples = []
            for row in results:
                key, payload_norm, scoring_spec, variance, latency = row
                
                try:
                    payload = json.loads(payload_norm)
                    bad_samples.append({
                        "cache_key": key,
                        "dialogue": payload,
                        "scoring_spec": scoring_spec,
                        "variance": variance,
                        "latency_ms": latency,
                        "reason": reason
                    })
                except json.JSONDecodeError:
                    continue
            
            print(f"ğŸ“‹ æ‰¾åˆ° {len(bad_samples)} ä¸ªé—®é¢˜æ ·æœ¬ (åŸå› : {reason})")
            return bad_samples
    
    def cleanup_expired(self) -> int:
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        with self.get_connection() as conn:
            cur = conn.cursor()
            
            now = int(time.time() * 1000)
            cur.execute("""
                DELETE FROM gemini_score_cache 
                WHERE expiry_at > 0 AND expiry_at < ?
            """, (now,))
            
            deleted_count = cur.rowcount
            conn.commit()
            
            print(f"ğŸ—‘ï¸  æ¸…ç†äº† {deleted_count} ä¸ªè¿‡æœŸç¼“å­˜é¡¹")
            return deleted_count
    
    def export_cache(self, output_file: str, spec_filter: str = None) -> int:
        """å¯¼å‡ºç¼“å­˜æ•°æ®"""
        with self.get_connection() as conn:
            cur = conn.cursor()
            
            if spec_filter:
                cur.execute("""
                    SELECT * FROM gemini_score_cache 
                    WHERE scoring_spec LIKE ?
                    ORDER BY created_at
                """, (f"%{spec_filter}%",))
            else:
                cur.execute("""
                    SELECT * FROM gemini_score_cache 
                    ORDER BY created_at
                """)
            
            rows = cur.fetchall()
            columns = [desc[0] for desc in cur.description]
            
            # è½¬æ¢ä¸ºJSONæ ¼å¼
            export_data = []
            for row in rows:
                record = dict(zip(columns, row))
                
                # è§£æJSONå­—æ®µ
                try:
                    record["payload_norm"] = json.loads(record["payload_norm"])
                    record["score_json"] = json.loads(record["score_json"])
                except json.JSONDecodeError:
                    pass
                
                export_data.append(record)
            
            # ä¿å­˜æ–‡ä»¶
            Path(output_file).parent.mkdir(parents=True, exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“¤ å¯¼å‡ºäº† {len(export_data)} æ¡ç¼“å­˜è®°å½•åˆ°: {output_file}")
            return len(export_data)
    
    def backup_database(self, backup_path: str = None) -> str:
        """å¤‡ä»½æ•°æ®åº“"""
        if not backup_path:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            backup_path = f"backup/gemini_cache_backup_{timestamp}.sqlite"
        
        Path(backup_path).parent.mkdir(parents=True, exist_ok=True)
        
        # ä½¿ç”¨SQLiteçš„å¤‡ä»½API
        with self.get_connection() as source:
            with sqlite3.connect(backup_path) as backup:
                source.backup(backup)
        
        backup_size = os.path.getsize(backup_path)
        print(f"ğŸ’¾ æ•°æ®åº“å·²å¤‡ä»½åˆ°: {backup_path} ({backup_size} bytes)")
        return backup_path

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Score Cache CLI - è¯„åˆ†ç¼“å­˜ç®¡ç†å·¥å…·")
    parser.add_argument("--db", default="gemini_cache.sqlite", help="ç¼“å­˜æ•°æ®åº“è·¯å¾„")
    
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")
    
    # statå‘½ä»¤
    stat_parser = subparsers.add_parser("stat", help="æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡ä¿¡æ¯")
    stat_parser.add_argument("--json", action="store_true", help="ä»¥JSONæ ¼å¼è¾“å‡º")
    
    # invalidateå‘½ä»¤
    invalidate_parser = subparsers.add_parser("invalidate", help="å¤±æ•ˆç¼“å­˜é¡¹")
    invalidate_parser.add_argument("--spec", help="æŒ‰è§„èŒƒæ¨¡å¼å¤±æ•ˆ")
    invalidate_parser.add_argument("--model", help="æŒ‰æ¨¡å‹åå¤±æ•ˆ")
    invalidate_parser.add_argument("--version", help="æŒ‰ç‰ˆæœ¬å¤±æ•ˆ")
    invalidate_parser.add_argument("--dims", help="æŒ‰ç»´åº¦å¤±æ•ˆ")
    
    # replay-badå‘½ä»¤
    replay_parser = subparsers.add_parser("replay-bad", help="é‡æ”¾é—®é¢˜æ ·æœ¬")
    replay_parser.add_argument("--reason", choices=["unstable", "high_latency", "error"], 
                              default="unstable", help="é—®é¢˜åŸå› ")
    replay_parser.add_argument("--limit", type=int, default=100, help="æœ€å¤§æ ·æœ¬æ•°")
    replay_parser.add_argument("--output", help="ä¿å­˜åˆ°æ–‡ä»¶")
    
    # cleanupå‘½ä»¤
    cleanup_parser = subparsers.add_parser("cleanup", help="æ¸…ç†è¿‡æœŸç¼“å­˜")
    
    # exportå‘½ä»¤
    export_parser = subparsers.add_parser("export", help="å¯¼å‡ºç¼“å­˜æ•°æ®")
    export_parser.add_argument("output_file", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    export_parser.add_argument("--filter", help="è§„èŒƒè¿‡æ»¤æ¡ä»¶")
    
    # backupå‘½ä»¤
    backup_parser = subparsers.add_parser("backup", help="å¤‡ä»½æ•°æ®åº“")
    backup_parser.add_argument("--output", help="å¤‡ä»½æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # åˆ›å»ºCLIå®ä¾‹
    cli = ScoreCacheCLI(args.db)
    
    try:
        if args.command == "stat":
            stats = cli.stat()
            if args.json:
                print(json.dumps(stats, indent=2, ensure_ascii=False))
            else:
                print("ğŸ“Š ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯")
                print("=" * 50)
                print(f"ğŸ“ æ•°æ®åº“: {stats['database_info']['db_path']} ({stats['database_info']['db_size_mb']}MB)")
                print(f"ğŸ“ˆ æ€»æ¡ç›®: {stats['cache_stats']['total_entries']}")
                print(f"âœ… æœ‰æ•ˆæ¡ç›®: {stats['cache_stats']['valid_entries']}")
                print(f"â° å³å°†è¿‡æœŸ: {stats['cache_stats']['expiring_in_1h']}")
                print(f"ğŸ¯ å‘½ä¸­ç‡: {stats['cache_stats']['hit_rate']}")
                print(f"âš¡ å¹³å‡å»¶è¿Ÿ: {stats['performance_stats']['avg_latency_ms']}ms")
                print(f"ğŸ“Š ä¸ç¨³å®šæ ·æœ¬: {stats['variance_stats']['unstable_count']} ({stats['variance_stats']['unstable_rate']})")
        
        elif args.command == "invalidate":
            cli.invalidate(args.spec, args.model, args.version, args.dims)
        
        elif args.command == "replay-bad":
            bad_samples = cli.replay_bad(args.reason, args.limit)
            if args.output:
                with open(args.output, 'w', encoding='utf-8') as f:
                    json.dump(bad_samples, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ é—®é¢˜æ ·æœ¬å·²ä¿å­˜åˆ°: {args.output}")
            else:
                for i, sample in enumerate(bad_samples[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"\næ ·æœ¬ {i+1}:")
                    print(f"  åŸå› : {sample['reason']}")
                    print(f"  æ–¹å·®: {sample['variance']}")
                    print(f"  å»¶è¿Ÿ: {sample['latency_ms']}ms")
        
        elif args.command == "cleanup":
            cli.cleanup_expired()
        
        elif args.command == "export":
            cli.export_cache(args.output_file, args.filter)
        
        elif args.command == "backup":
            cli.backup_database(args.output)
    
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
