#!/usr/bin/env python3
"""
Score Cache CLI - ÁºìÂ≠òÁÆ°ÁêÜÂëΩ‰ª§Ë°åÂ∑•ÂÖ∑
Âü∫‰∫éGPT-5ÊåáÂØºÂÆûÁé∞ÁöÑÁºìÂ≠òÁÆ°ÁêÜÂ∑•ÂÖ∑
"""

import argparse
import json
import sqlite3
import time
import sys
import os
from pathlib import Path
from typing import Dict, List, Any
import logging

# Ê∑ªÂä†È°πÁõÆË∑ØÂæÑ
sys.path.insert(0, str(Path(__file__).parent.parent))

logger = logging.getLogger(__name__)

class ScoreCacheCLI:
    """ËØÑÂàÜÁºìÂ≠òCLIÁÆ°ÁêÜÂô®"""
    
    def __init__(self, db_path: str = "gemini_cache.sqlite"):
        self.db_path = db_path
        if not os.path.exists(db_path):
            print(f"‚ö†Ô∏è  ÁºìÂ≠òÊï∞ÊçÆÂ∫ì‰∏çÂ≠òÂú®: {db_path}")
            print("ËØ∑ÂÖàËøêË°åËØÑÂàÜÁ≥ªÁªüÂàõÂª∫ÁºìÂ≠òÊï∞ÊçÆÂ∫ì")
            sys.exit(1)
    
    def get_connection(self):
        """Ëé∑ÂèñÊï∞ÊçÆÂ∫ìËøûÊé•"""
        return sqlite3.connect(self.db_path, check_same_thread=False)
    
    def stat(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÁºìÂ≠òÁªüËÆ°‰ø°ÊÅØ"""
        with self.get_connection() as conn:
            cur = conn.cursor()
            
            # Âü∫Á°ÄÁªüËÆ°
            cur.execute("SELECT COUNT(*) FROM gemini_score_cache")
            total_entries = cur.fetchone()[0]
            
            # ÊúâÊïàÁºìÂ≠òÁªüËÆ°
            now = int(time.time() * 1000)
            cur.execute("SELECT COUNT(*) FROM gemini_score_cache WHERE expiry_at > ?", (now,))
            valid_entries = cur.fetchone()[0]
            
            # Áä∂ÊÄÅÁªüËÆ°
            cur.execute("""
                SELECT status, COUNT(*) 
                FROM gemini_score_cache 
                GROUP BY status
            """)
            status_counts = dict(cur.fetchall())
            
            # Âª∂ËøüÁªüËÆ°
            cur.execute("""
                SELECT AVG(api_latency_ms), MAX(api_latency_ms), MIN(api_latency_ms)
                FROM gemini_score_cache 
                WHERE api_latency_ms > 0
            """)
            latency_stats = cur.fetchone()
            
            # ÊñπÂ∑ÆÁªüËÆ°
            cur.execute("""
                SELECT AVG(variance), MAX(variance), 
                       COUNT(CASE WHEN variance > 0.08 THEN 1 END)
                FROM gemini_score_cache 
                WHERE variance IS NOT NULL
            """)
            variance_stats = cur.fetchone()
            
            # TTLÂç≥Â∞ÜËøáÊúüÁªüËÆ°
            hour_from_now = now + (60 * 60 * 1000)
            cur.execute("""
                SELECT COUNT(*) FROM gemini_score_cache 
                WHERE expiry_at > ? AND expiry_at < ?
            """, (now, hour_from_now))
            expiring_soon = cur.fetchone()[0]
            
            # Ê®°Âûã/ÁâàÊú¨ÂàÜÂ∏É
            cur.execute("""
                SELECT scoring_spec, COUNT(*) 
                FROM gemini_score_cache 
                GROUP BY scoring_spec 
                ORDER BY COUNT(*) DESC
                LIMIT 10
            """)
            spec_distribution = cur.fetchall()
            
            return {
                "database_info": {
                    "db_path": self.db_path,
                    "db_size_mb": round(os.path.getsize(self.db_path) / (1024*1024), 2),
                    "last_updated": time.strftime("%Y-%m-%d %H:%M:%S")
                },
                "cache_stats": {
                    "total_entries": total_entries,
                    "valid_entries": valid_entries,
                    "expired_entries": total_entries - valid_entries,
                    "hit_rate": round(valid_entries / max(1, total_entries), 3),
                    "expiring_in_1h": expiring_soon
                },
                "status_distribution": status_counts,
                "performance_stats": {
                    "avg_latency_ms": round(latency_stats[0] or 0, 1),
                    "max_latency_ms": latency_stats[1] or 0,
                    "min_latency_ms": latency_stats[2] or 0
                },
                "variance_stats": {
                    "avg_variance": round(variance_stats[0] or 0, 4),
                    "max_variance": round(variance_stats[1] or 0, 4),
                    "unstable_count": variance_stats[2] or 0,
                    "unstable_rate": round((variance_stats[2] or 0) / max(1, total_entries), 3)
                },
                "spec_distribution": [
                    {"spec": spec, "count": count} 
                    for spec, count in spec_distribution
                ]
            }
    
    def invalidate(self, spec_pattern: str = None, model: str = None, 
                  version: str = None, dims: str = None) -> int:
        """Â§±ÊïàÁºìÂ≠òÈ°π"""
        with self.get_connection() as conn:
            cur = conn.cursor()
            
            if spec_pattern:
                # Áõ¥Êé•ÊåâÊ®°ÂºèÂ§±Êïà
                cur.execute("""
                    UPDATE gemini_score_cache 
                    SET expiry_at = 0 
                    WHERE scoring_spec LIKE ?
                """, (f"%{spec_pattern}%",))
                
            else:
                # ÊûÑÂª∫ÂÖ∑‰ΩìÁöÑÊ®°Âºè
                patterns = []
                if model:
                    patterns.append(f"{model}|%")
                if version:
                    patterns.append(f"%|v={version}|%")
                if dims:
                    patterns.append(f"%|dims={dims}")
                
                if not patterns:
                    print("‚ùå ËØ∑Êèê‰æõËá≥Â∞ë‰∏Ä‰∏™Â§±ÊïàÊù°‰ª∂")
                    return 0
                
                # ÊâßË°åÂ§±Êïà
                total_invalidated = 0
                for pattern in patterns:
                    cur.execute("""
                        UPDATE gemini_score_cache 
                        SET expiry_at = 0 
                        WHERE scoring_spec LIKE ?
                    """, (pattern,))
                    total_invalidated += cur.rowcount
            
            conn.commit()
            invalidated_count = cur.rowcount if spec_pattern else total_invalidated
            
            print(f"‚úÖ Â∑≤Â§±Êïà {invalidated_count} ‰∏™ÁºìÂ≠òÈ°π")
            return invalidated_count
    
    def replay_bad(self, reason: str = "unstable", limit: int = 100) -> List[Dict[str, Any]]:
        """ÈáçÊîæÈóÆÈ¢òÊ†∑Êú¨"""
        with self.get_connection() as conn:
            cur = conn.cursor()
            
            if reason == "unstable":
                # Êü•ÊâæÈ´òÊñπÂ∑ÆÊ†∑Êú¨
                cur.execute("""
                    SELECT key, payload_norm, scoring_spec, variance, api_latency_ms
                    FROM gemini_score_cache
                    WHERE variance > 0.08
                    ORDER BY variance DESC
                    LIMIT ?
                """, (limit,))
                
            elif reason == "high_latency":
                # Êü•ÊâæÈ´òÂª∂ËøüÊ†∑Êú¨
                cur.execute("""
                    SELECT key, payload_norm, scoring_spec, variance, api_latency_ms
                    FROM gemini_score_cache
                    WHERE api_latency_ms > 5000
                    ORDER BY api_latency_ms DESC
                    LIMIT ?
                """, (limit,))
                
            elif reason == "error":
                # Êü•ÊâæÈîôËØØÊ†∑Êú¨
                cur.execute("""
                    SELECT key, payload_norm, scoring_spec, variance, api_latency_ms
                    FROM gemini_score_cache
                    WHERE status = 'error'
                    ORDER BY updated_at DESC
                    LIMIT ?
                """, (limit,))
                
            else:
                print(f"‚ùå ‰∏çÊîØÊåÅÁöÑÂéüÂõ†: {reason}")
                print("ÊîØÊåÅÁöÑÂéüÂõ†: unstable, high_latency, error")
                return []
            
            results = cur.fetchall()
            
            bad_samples = []
            for row in results:
                key, payload_norm, scoring_spec, variance, latency = row
                
                try:
                    payload = json.loads(payload_norm)
                    bad_samples.append({
                        "cache_key": key,
                        "dialogue": payload,
                        "scoring_spec": scoring_spec,
                        "variance": variance,
                        "latency_ms": latency,
                        "reason": reason
                    })
                except json.JSONDecodeError:
                    continue
            
            print(f"üìã ÊâæÂà∞ {len(bad_samples)} ‰∏™ÈóÆÈ¢òÊ†∑Êú¨ (ÂéüÂõ†: {reason})")
            return bad_samples
    
    def cleanup_expired(self) -> int:
        """Ê∏ÖÁêÜËøáÊúüÁºìÂ≠ò"""
        with self.get_connection() as conn:
            cur = conn.cursor()
            
            now = int(time.time() * 1000)
            cur.execute("""
                DELETE FROM gemini_score_cache 
                WHERE expiry_at > 0 AND expiry_at < ?
            """, (now,))
            
            deleted_count = cur.rowcount
            conn.commit()
            
            print(f"üóëÔ∏è  Ê∏ÖÁêÜ‰∫Ü {deleted_count} ‰∏™ËøáÊúüÁºìÂ≠òÈ°π")
            return deleted_count
    
    def export_cache(self, output_file: str, spec_filter: str = None) -> int:
        """ÂØºÂá∫ÁºìÂ≠òÊï∞ÊçÆ"""
        with self.get_connection() as conn:
            cur = conn.cursor()
            
            if spec_filter:
                cur.execute("""
                    SELECT * FROM gemini_score_cache 
                    WHERE scoring_spec LIKE ?
                    ORDER BY created_at
                """, (f"%{spec_filter}%",))
            else:
                cur.execute("""
                    SELECT * FROM gemini_score_cache 
                    ORDER BY created_at
                """)
            
            rows = cur.fetchall()
            columns = [desc[0] for desc in cur.description]
            
            # ËΩ¨Êç¢‰∏∫JSONÊ†ºÂºè
            export_data = []
            for row in rows:
                record = dict(zip(columns, row))
                
                # Ëß£ÊûêJSONÂ≠óÊÆµ
                try:
                    record["payload_norm"] = json.loads(record["payload_norm"])
                    record["score_json"] = json.loads(record["score_json"])
                except json.JSONDecodeError:
                    pass
                
                export_data.append(record)
            
            # ‰øùÂ≠òÊñá‰ª∂
            Path(output_file).parent.mkdir(parents=True, exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            print(f"üì§ ÂØºÂá∫‰∫Ü {len(export_data)} Êù°ÁºìÂ≠òËÆ∞ÂΩïÂà∞: {output_file}")
            return len(export_data)
    
    def backup_database(self, backup_path: str = None) -> str:
        """Â§á‰ªΩÊï∞ÊçÆÂ∫ì"""
        if not backup_path:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            backup_path = f"backup/gemini_cache_backup_{timestamp}.sqlite"
        
        Path(backup_path).parent.mkdir(parents=True, exist_ok=True)
        
        # ‰ΩøÁî®SQLiteÁöÑÂ§á‰ªΩAPI
        with self.get_connection() as source:
            with sqlite3.connect(backup_path) as backup:
                source.backup(backup)
        
        backup_size = os.path.getsize(backup_path)
        print(f"üíæ Êï∞ÊçÆÂ∫ìÂ∑≤Â§á‰ªΩÂà∞: {backup_path} ({backup_size} bytes)")
        return backup_path

def main():
    """‰∏ªÂáΩÊï∞"""
    parser = argparse.ArgumentParser(description="Score Cache CLI - ËØÑÂàÜÁºìÂ≠òÁÆ°ÁêÜÂ∑•ÂÖ∑")
    parser.add_argument("--db", default="gemini_cache.sqlite", help="ÁºìÂ≠òÊï∞ÊçÆÂ∫ìË∑ØÂæÑ")
    
    subparsers = parser.add_subparsers(dest="command", help="ÂèØÁî®ÂëΩ‰ª§")
    
    # statÂëΩ‰ª§
    stat_parser = subparsers.add_parser("stat", help="ÊòæÁ§∫ÁºìÂ≠òÁªüËÆ°‰ø°ÊÅØ")
    stat_parser.add_argument("--json", action="store_true", help="‰ª•JSONÊ†ºÂºèËæìÂá∫")
    
    # invalidateÂëΩ‰ª§
    invalidate_parser = subparsers.add_parser("invalidate", help="Â§±ÊïàÁºìÂ≠òÈ°π")
    invalidate_parser.add_argument("--spec", help="ÊåâËßÑËåÉÊ®°ÂºèÂ§±Êïà")
    invalidate_parser.add_argument("--model", help="ÊåâÊ®°ÂûãÂêçÂ§±Êïà")
    invalidate_parser.add_argument("--version", help="ÊåâÁâàÊú¨Â§±Êïà")
    invalidate_parser.add_argument("--dims", help="ÊåâÁª¥Â∫¶Â§±Êïà")
    
    # replay-badÂëΩ‰ª§
    replay_parser = subparsers.add_parser("replay-bad", help="ÈáçÊîæÈóÆÈ¢òÊ†∑Êú¨")
    replay_parser.add_argument("--reason", choices=["unstable", "high_latency", "error"], 
                              default="unstable", help="ÈóÆÈ¢òÂéüÂõ†")
    replay_parser.add_argument("--limit", type=int, default=100, help="ÊúÄÂ§ßÊ†∑Êú¨Êï∞")
    replay_parser.add_argument("--output", help="‰øùÂ≠òÂà∞Êñá‰ª∂")
    
    # cleanupÂëΩ‰ª§
    cleanup_parser = subparsers.add_parser("cleanup", help="Ê∏ÖÁêÜËøáÊúüÁºìÂ≠ò")
    
    # exportÂëΩ‰ª§
    export_parser = subparsers.add_parser("export", help="ÂØºÂá∫ÁºìÂ≠òÊï∞ÊçÆ")
    export_parser.add_argument("output_file", help="ËæìÂá∫Êñá‰ª∂Ë∑ØÂæÑ")
    export_parser.add_argument("--filter", help="ËßÑËåÉËøáÊª§Êù°‰ª∂")
    
    # backupÂëΩ‰ª§
    backup_parser = subparsers.add_parser("backup", help="Â§á‰ªΩÊï∞ÊçÆÂ∫ì")
    backup_parser.add_argument("--output", help="Â§á‰ªΩÊñá‰ª∂Ë∑ØÂæÑ")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # ÂàõÂª∫CLIÂÆû‰æã
    cli = ScoreCacheCLI(args.db)
    
    try:
        if args.command == "stat":
            stats = cli.stat()
            if args.json:
                print(json.dumps(stats, indent=2, ensure_ascii=False))
            else:
                print("üìä ÁºìÂ≠òÁªüËÆ°‰ø°ÊÅØ")
                print("=" * 50)
                print(f"üìÅ Êï∞ÊçÆÂ∫ì: {stats['database_info']['db_path']} ({stats['database_info']['db_size_mb']}MB)")
                print(f"üìà ÊÄªÊù°ÁõÆ: {stats['cache_stats']['total_entries']}")
                print(f"‚úÖ ÊúâÊïàÊù°ÁõÆ: {stats['cache_stats']['valid_entries']}")
                print(f"‚è∞ Âç≥Â∞ÜËøáÊúü: {stats['cache_stats']['expiring_in_1h']}")
                print(f"üéØ ÂëΩ‰∏≠Áéá: {stats['cache_stats']['hit_rate']}")
                print(f"‚ö° Âπ≥ÂùáÂª∂Ëøü: {stats['performance_stats']['avg_latency_ms']}ms")
                print(f"üìä ‰∏çÁ®≥ÂÆöÊ†∑Êú¨: {stats['variance_stats']['unstable_count']} ({stats['variance_stats']['unstable_rate']})")
        
        elif args.command == "invalidate":
            cli.invalidate(args.spec, args.model, args.version, args.dims)
        
        elif args.command == "replay-bad":
            bad_samples = cli.replay_bad(args.reason, args.limit)
            if args.output:
                with open(args.output, 'w', encoding='utf-8') as f:
                    json.dump(bad_samples, f, ensure_ascii=False, indent=2)
                print(f"üíæ ÈóÆÈ¢òÊ†∑Êú¨Â∑≤‰øùÂ≠òÂà∞: {args.output}")
            else:
                for i, sample in enumerate(bad_samples[:5]):  # Âè™ÊòæÁ§∫Ââç5‰∏™
                    print(f"\nÊ†∑Êú¨ {i+1}:")
                    print(f"  ÂéüÂõ†: {sample['reason']}")
                    print(f"  ÊñπÂ∑Æ: {sample['variance']}")
                    print(f"  Âª∂Ëøü: {sample['latency_ms']}ms")
        
        elif args.command == "cleanup":
            cli.cleanup_expired()
        
        elif args.command == "export":
            cli.export_cache(args.output_file, args.filter)
        
        elif args.command == "backup":
            cli.backup_database(args.output)
    
    except Exception as e:
        print(f"‚ùå ÊâßË°åÂ§±Ë¥•: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
