#!/usr/bin/env python3
"""
å½±å­æ•°æ®å®¡è®¡è„šæœ¬ï¼šçœŸä¼ª & å¤šæ ·æ€§ & å»æ¨¡æ¿åŒ–æ£€æµ‹
"""

import argparse
import json
import re
import sys
import hashlib
import random
from pathlib import Path
from collections import Counter

def load_samples(jsonl_file):
    """åŠ è½½JSONLæ ·æœ¬"""
    samples = []
    with open(jsonl_file, "r", encoding="utf-8") as f:
        for line in f:
            samples.append(json.loads(line.strip()))
    return samples

def audit_authenticity(samples):
    """çœŸä¼ªæº¯æºå®¡è®¡"""
    print("ğŸ” çœŸä¼ªæº¯æºå®¡è®¡...")
    
    allowed_datasets = {"hotpot_qa", "tasksource/bigbench", "gsm8k"}
    by_task = {}
    fingerprints_by_task = {}
    
    for sample in samples:
        # æ£€æŸ¥source
        if sample.get("source") != "hf":
            return False, f"æ ·æœ¬sourceä¸ä¸ºhf: {sample.get('id')}"
        
        # æ£€æŸ¥hf_dataset
        hf_dataset = sample.get("hf_dataset")
        if hf_dataset not in allowed_datasets:
            return False, f"éå…è®¸æ•°æ®é›†: {hf_dataset}"
        
        # æ£€æŸ¥questionå’Œtask
        if not sample.get("question") or sample.get("task") == "unknown":
            return False, f"questionä¸ºç©ºæˆ–taskä¸ºunknown: {sample.get('id')}"
        
        # ç»Ÿè®¡ä»»åŠ¡åˆ†å¸ƒ
        task = sample.get("task")
        by_task[task] = by_task.get(task, 0) + 1
        
        # æ”¶é›†æŒ‡çº¹
        fingerprint = sample.get("hf_fingerprint")
        if fingerprint:
            if task not in fingerprints_by_task:
                fingerprints_by_task[task] = set()
            fingerprints_by_task[task].add(fingerprint)
    
    # æ£€æŸ¥ä»»åŠ¡åˆ†å¸ƒï¼šæ¯ä¸ªä»»åŠ¡åº”åœ¨[70, 90]èŒƒå›´å†…
    total = len(samples)
    for task, count in by_task.items():
        if not (70 <= count <= 90):
            return False, f"ä»»åŠ¡{task}åˆ†å¸ƒå¼‚å¸¸: {count}/{total} (åº”åœ¨[70,90])"
    
    # æ£€æŸ¥æŒ‡çº¹æ•°é‡ï¼šæ¯ä¸ªä»»åŠ¡â‰¤3ä¸ªæŒ‡çº¹
    for task, fingerprints in fingerprints_by_task.items():
        if len(fingerprints) > 3:
            return False, f"ä»»åŠ¡{task}æŒ‡çº¹è¿‡å¤š: {len(fingerprints)} > 3"
    
    print(f"  âœ… ä»»åŠ¡åˆ†å¸ƒ: {by_task}")
    print(f"  âœ… æŒ‡çº¹ç»Ÿè®¡: {[(k, len(v)) for k, v in fingerprints_by_task.items()]}")
    
    return True, by_task

def mask_question(question):
    """ç”Ÿæˆæ©ç é—®é¢˜ï¼šå°å†™+æ•°å­—æ›¿æ¢ä¸º#"""
    masked = question.lower()
    masked = re.sub(r'\d+', '#', masked)
    return masked

def audit_detemplatization(samples):
    """å»æ¨¡æ¿åŒ–/åæ”¹æ•°å­—å‡‘æ•°æ£€æµ‹"""
    print("ğŸ” å»æ¨¡æ¿åŒ–å®¡è®¡...")
    
    questions = [sample.get("question", "") for sample in samples]
    
    # 1. æ©ç å”¯ä¸€ç‡
    masks = [mask_question(q) for q in questions]
    unique_masks = set(masks)
    mask_uniqueness = len(unique_masks) / len(masks)
    
    # æœ€é¢‘ç¹æ©ç å æ¯”
    mask_counts = Counter(masks)
    most_common_mask_ratio = mask_counts.most_common(1)[0][1] / len(masks) if mask_counts else 0
    
    print(f"  ğŸ“Š æ©ç å”¯ä¸€ç‡: {mask_uniqueness:.3f} (éœ€â‰¥0.60)")
    print(f"  ğŸ“Š æœ€é¢‘ç¹æ©ç å æ¯”: {most_common_mask_ratio:.3f} (éœ€â‰¤0.10)")
    
    if mask_uniqueness < 0.60:
        return False, f"æ©ç å”¯ä¸€ç‡è¿‡ä½: {mask_uniqueness:.3f} < 0.60"
    
    if most_common_mask_ratio > 0.10:
        return False, f"æœ€é¢‘ç¹æ©ç å æ¯”è¿‡é«˜: {most_common_mask_ratio:.3f} > 0.10"
    
    # 2. ç›¸ä¼¼åº¦æŠ½æ£€ï¼š5-gram Jaccard
    def get_5grams(text):
        text = text.lower()
        return set(text[i:i+5] for i in range(len(text)-4))
    
    def jaccard_similarity(set1, set2):
        if not set1 and not set2:
            return 1.0
        if not set1 or not set2:
            return 0.0
        return len(set1 & set2) / len(set1 | set2)
    
    # éšæœºæŠ½2000å¯¹
    n_pairs = min(2000, len(questions) * (len(questions) - 1) // 2)
    high_sim_pairs = 0
    
    random.seed(42)
    pairs = [(i, j) for i in range(len(questions)) for j in range(i+1, len(questions))]
    sampled_pairs = random.sample(pairs, n_pairs)
    
    for i, j in sampled_pairs:
        grams1 = get_5grams(questions[i])
        grams2 = get_5grams(questions[j])
        sim = jaccard_similarity(grams1, grams2)
        if sim >= 0.9:
            high_sim_pairs += 1
    
    high_sim_ratio = high_sim_pairs / n_pairs
    print(f"  ğŸ“Š é«˜ç›¸ä¼¼åº¦å¯¹æ¯”ä¾‹: {high_sim_ratio:.3f} (éœ€â‰¤0.01)")
    
    if high_sim_ratio > 0.01:
        return False, f"é«˜ç›¸ä¼¼åº¦å¯¹è¿‡å¤š: {high_sim_ratio:.3f} > 0.01"
    
    # 3. é•¿åº¦åˆ†å¸ƒ
    lengths = [len(q) for q in questions]
    mean_length = sum(lengths) / len(lengths)
    std_length = (sum((l - mean_length) ** 2 for l in lengths) / len(lengths)) ** 0.5
    
    print(f"  ğŸ“Š é¢˜å¹²é•¿åº¦: å‡å€¼={mean_length:.1f}, æ ‡å‡†å·®={std_length:.1f}")
    
    if not (30 <= mean_length <= 300):
        return False, f"é¢˜å¹²é•¿åº¦å‡å€¼å¼‚å¸¸: {mean_length:.1f} ä¸åœ¨[30,300]"
    
    if std_length < 15:
        return False, f"é¢˜å¹²é•¿åº¦æ ‡å‡†å·®è¿‡å°: {std_length:.1f} < 15"
    
    # 4. é‡å¤å‰ç¼€æ£€æŸ¥
    prefixes = [q[:12] for q in questions if len(q) >= 12]
    if prefixes:
        prefix_counts = Counter(prefixes)
        most_common_prefix_ratio = prefix_counts.most_common(1)[0][1] / len(prefixes)
        print(f"  ğŸ“Š æœ€å¸¸è§å‰ç¼€å æ¯”: {most_common_prefix_ratio:.3f} (éœ€â‰¤0.20)")
        
        if most_common_prefix_ratio > 0.20:
            return False, f"é‡å¤å‰ç¼€è¿‡å¤š: {most_common_prefix_ratio:.3f} > 0.20"
    
    return True, {
        "mask_uniqueness": mask_uniqueness,
        "most_common_mask_ratio": most_common_mask_ratio,
        "high_sim_ratio": high_sim_ratio,
        "mean_length": mean_length,
        "std_length": std_length,
        "most_common_prefix_ratio": most_common_prefix_ratio if prefixes else 0
    }

def audit_duplicates(samples):
    """é‡å¤/ç©ºæ ·æœ¬æ£€æŸ¥"""
    print("ğŸ” é‡å¤æ ·æœ¬å®¡è®¡...")
    
    # è§„èŒƒåŒ–æ–‡æœ¬
    def normalize_text(text):
        if not text:
            return ""
        # å»ç©ºæ ¼ï¼Œå…¨è§’åŠè§’ç»Ÿä¸€
        text = re.sub(r'\s+', ' ', text.strip())
        text = text.replace('ã€€', ' ')  # å…¨è§’ç©ºæ ¼
        # å…¨è§’è½¬åŠè§’æ•°å­—å’Œå­—æ¯
        full_to_half = str.maketrans(
            'ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½š',
            '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'
        )
        return text.translate(full_to_half)
    
    normalized_questions = []
    empty_answers = 0
    
    for sample in samples:
        question = normalize_text(sample.get("question", ""))
        answer = sample.get("answer", "")
        
        normalized_questions.append(question)
        
        # æ£€æŸ¥ç©ºç­”æ¡ˆï¼ˆStrategyQAç‰¹æ®Šå¤„ç†ï¼‰
        if not answer:
            empty_answers += 1
        elif sample.get("task") == "strategyqa" and answer not in ["yes", "no"]:
            empty_answers += 1
    
    # è®¡ç®—é‡å¤ç‡
    question_counts = Counter(normalized_questions)
    duplicates = sum(count - 1 for count in question_counts.values() if count > 1)
    duplicate_ratio = duplicates / len(samples)
    
    print(f"  ğŸ“Š å®Œå…¨é‡å¤ç‡: {duplicate_ratio:.3f} (éœ€â‰¤0.01)")
    print(f"  ğŸ“Š ç©ºç­”æ¡ˆæ•°: {empty_answers} (éœ€=0)")
    
    if duplicate_ratio > 0.01:
        return False, f"é‡å¤ç‡è¿‡é«˜: {duplicate_ratio:.3f} > 0.01"
    
    if empty_answers > 0:
        return False, f"å­˜åœ¨ç©ºç­”æ¡ˆ: {empty_answers}ä¸ª"
    
    return True, {
        "duplicate_ratio": duplicate_ratio,
        "empty_answers": empty_answers
    }

def main():
    parser = argparse.ArgumentParser(description="å½±å­æ•°æ®å®¡è®¡")
    parser.add_argument("input_file", help="è¾“å…¥JSONLæ–‡ä»¶")
    parser.add_argument("--report", required=True, help="å®¡è®¡æŠ¥å‘ŠJSON")
    
    args = parser.parse_args()
    
    print(f"ğŸ©º å½±å­æ•°æ®å®¡è®¡: {args.input_file}")
    
    # åŠ è½½æ ·æœ¬
    try:
        samples = load_samples(args.input_file)
        print(f"ğŸ“Š åŠ è½½æ ·æœ¬: {len(samples)}æ¡")
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return 1
    
    audit_results = {
        "total_samples": len(samples),
        "timestamp": Path(args.input_file).stat().st_mtime,
        "passed": True,
        "failures": []
    }
    
    # çœŸä¼ªæº¯æºå®¡è®¡
    passed, result = audit_authenticity(samples)
    if not passed:
        audit_results["passed"] = False
        audit_results["failures"].append(f"çœŸä¼ªæº¯æº: {result}")
        print(f"âŒ {result}")
    else:
        audit_results["by_task"] = result
    
    # å»æ¨¡æ¿åŒ–å®¡è®¡
    passed, result = audit_detemplatization(samples)
    if not passed:
        audit_results["passed"] = False
        audit_results["failures"].append(f"å»æ¨¡æ¿åŒ–: {result}")
        print(f"âŒ {result}")
    else:
        audit_results["detemplatization"] = result
    
    # é‡å¤æ ·æœ¬å®¡è®¡
    passed, result = audit_duplicates(samples)
    if not passed:
        audit_results["passed"] = False
        audit_results["failures"].append(f"é‡å¤æ£€æµ‹: {result}")
        print(f"âŒ {result}")
    else:
        audit_results["duplicates"] = result
    
    # ä¿å­˜æŠ¥å‘Š
    Path(args.report).parent.mkdir(parents=True, exist_ok=True)
    with open(args.report, "w", encoding="utf-8") as f:
        json.dump(audit_results, f, ensure_ascii=False, indent=2)
    
    if audit_results["passed"]:
        print("âœ… æ‰€æœ‰å®¡è®¡é€šè¿‡")
        return 0
    else:
        print(f"âŒ å®¡è®¡å¤±è´¥: {len(audit_results['failures'])}é¡¹")
        for failure in audit_results["failures"]:
            print(f"  - {failure}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
