#!/usr/bin/env python3
"""
é˜²ä¼ªé—¸é—¨æ£€æŸ¥ - ç¡®ä¿RC1ä¸ºçœŸå®è®­ç»ƒï¼Œæ‹’ç»ä»»ä½•dry-runäº§ç‰©
"""

import os
import json
import hashlib
import sqlite3
from pathlib import Path
import sys
import time

def check_scorer_connectivity():
    """æ£€æŸ¥æ‰“åˆ†å™¨çœŸå®è¿é€šæ€§"""
    print("ğŸ” æ£€æŸ¥æ‰“åˆ†å™¨è¿é€šæ€§...")
    
    # æ£€æŸ¥API Keyé…ç½®
    scorer_provider = os.getenv("SCORER_PROVIDER")
    api_key = os.getenv("SCORER_API_KEY")
    
    assert scorer_provider in {"deepseek_r1", "gemini", "gpt35"}, f"âŒ æ‰“åˆ†å™¨æœªé…ç½®: {scorer_provider}"
    assert api_key, "âŒ SCORER_API_KEYæœªè®¾ç½®ï¼šæ‹’ç»dry-run"
    
    # æ£€æŸ¥ç¼“å­˜æ•°æ®åº“çš„çœŸå®ä½¿ç”¨æƒ…å†µ
    cache_db = Path("gemini_cache.sqlite")
    if cache_db.exists():
        conn = sqlite3.connect(cache_db)
        cursor = conn.cursor()
        
        # æ£€æŸ¥æœ€è¿‘çš„çœŸå®APIè°ƒç”¨
        cursor.execute("""
            SELECT COUNT(*) as total, 
                   SUM(CASE WHEN tries > 0 THEN 1 ELSE 0 END) as real_calls,
                   AVG(latency_ms) as avg_latency
            FROM cache 
            WHERE timestamp > datetime('now', '-1 hour')
        """)
        result = cursor.fetchone()
        conn.close()
        
        total, real_calls, avg_latency = result
        if total > 0:
            hit_rate = (total - real_calls) / total
            print(f"  ğŸ“Š æœ€è¿‘1å°æ—¶: {total}æ¬¡è¯„åˆ†, {real_calls}æ¬¡çœŸå®APIè°ƒç”¨")
            print(f"  ğŸ“Š ç¼“å­˜å‘½ä¸­ç‡: {hit_rate:.1%}, å¹³å‡å»¶è¿Ÿ: {avg_latency:.1f}ms")
            
            # é˜²ä¼ªæ£€æŸ¥ï¼šç¼“å­˜å‘½ä¸­ç‡ä¸èƒ½è¿‡é«˜ï¼ˆé¦–è½®ä¸¥æ ¼ï¼‰
            assert hit_rate < 0.90, f"âŒ ç¼“å­˜å‘½ä¸­ç‡è¿‡é«˜({hit_rate:.1%})ï¼Œç–‘ä¼¼dry-run"
            assert real_calls >= 1, "âŒ æ— çœŸå®APIè°ƒç”¨ï¼Œç–‘ä¼¼dry-run"
        
    print("  âœ… æ‰“åˆ†å™¨è¿é€šæ€§æ£€æŸ¥é€šè¿‡")

def check_training_data():
    """æ£€æŸ¥è®­ç»ƒæ•°æ®çœŸå®æ€§"""
    print("ğŸ” æ£€æŸ¥è®­ç»ƒæ•°æ®...")
    
    # æ£€æŸ¥shadowè¯„ä¼°æ•°æ®
    shadow_file = Path("data/shadow_eval_245.jsonl")
    assert shadow_file.exists(), "âŒ shadowè¯„ä¼°æ•°æ®ç¼ºå¤±"
    
    with open(shadow_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    assert len(lines) >= 245, f"âŒ è¯„ä¼°æ ·æœ¬ä¸è¶³: {len(lines)} < 245"
    
    # è®¡ç®—æ•°æ®å“ˆå¸Œå¹¶è®°å½•
    data_content = ''.join(lines)
    data_hash = hashlib.sha256(data_content.encode()).hexdigest()
    
    # æ£€æŸ¥æˆåŠŸç‡åˆ†å¸ƒï¼ˆé˜²æ­¢å…¨0/å…¨1ï¼‰
    success_counts = {"math": 0, "multihop": 0, "clarify": 0}
    total_counts = {"math": 0, "multihop": 0, "clarify": 0}
    
    for line in lines:
        try:
            sample = json.loads(line.strip())
            task_type = sample.get("task_type", "unknown")
            if task_type in success_counts:
                total_counts[task_type] += 1
                # æ¨¡æ‹ŸæˆåŠŸåˆ¤æ–­ï¼ˆå®é™…åº”æœ‰å…·ä½“é€»è¾‘ï¼‰
                if sample.get("success", False):
                    success_counts[task_type] += 1
        except:
            continue
    
    # é˜²ä¼ªæ£€æŸ¥ï¼šæˆåŠŸç‡ä¸èƒ½æç«¯
    for task_type in success_counts:
        if total_counts[task_type] > 0:
            success_rate = success_counts[task_type] / total_counts[task_type]
            assert 0.1 <= success_rate <= 0.9, f"âŒ {task_type}æˆåŠŸç‡æç«¯: {success_rate:.1%}"
    
    # ä¿å­˜æ•°æ®æ¸…å•
    manifest = {
        "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
        "total_samples": len(lines),
        "data_hash": data_hash,
        "success_rates": {k: success_counts[k]/total_counts[k] if total_counts[k] > 0 else 0 
                         for k in success_counts}
    }
    
    with open("reports/rc1/data_hash.json", 'w', encoding='utf-8') as f:
        json.dump(manifest, f, indent=2)
    
    print(f"  âœ… æ•°æ®æ ·æœ¬: {len(lines)}æ¡")
    print(f"  âœ… æ•°æ®å“ˆå¸Œ: {data_hash[:16]}...")
    print(f"  âœ… æˆåŠŸç‡åˆ†å¸ƒ: {manifest['success_rates']}")

def check_model_checkpoints():
    """æ£€æŸ¥æ¨¡å‹æƒé‡çœŸå®æ€§"""
    print("ğŸ” æ£€æŸ¥æ¨¡å‹æƒé‡...")
    
    checkpoints_dir = Path("checkpoints/rc1")
    real_checkpoints = []
    
    # éå†æ‰€æœ‰checkpointç›®å½•
    for seed_dir in checkpoints_dir.iterdir():
        if seed_dir.is_dir() and seed_dir.name.isdigit():
            for step_dir in seed_dir.iterdir():
                if step_dir.is_dir() and step_dir.name.startswith("step_"):
                    # æ£€æŸ¥æƒé‡æ–‡ä»¶
                    has_real_weights = False
                    
                    # æ£€æŸ¥LoRAæƒé‡
                    adapter_model = step_dir / "adapter_model.safetensors"
                    if adapter_model.exists() and adapter_model.stat().st_size > 5 * 1024 * 1024:  # >5MB
                        has_real_weights = True
                        print(f"  âœ… LoRAæƒé‡: {step_dir} ({adapter_model.stat().st_size // 1024 // 1024}MB)")
                    
                    # æ£€æŸ¥å…¨å‚æƒé‡
                    for weight_file in step_dir.glob("pytorch_model*.bin"):
                        if weight_file.stat().st_size > 50 * 1024 * 1024:  # >50MB
                            has_real_weights = True
                            print(f"  âœ… å…¨å‚æƒé‡: {step_dir} ({weight_file.stat().st_size // 1024 // 1024}MB)")
                            break
                    
                    for weight_file in step_dir.glob("*.safetensors"):
                        if weight_file.name != "adapter_model.safetensors" and weight_file.stat().st_size > 50 * 1024 * 1024:
                            has_real_weights = True
                            print(f"  âœ… SafeTensorsæƒé‡: {step_dir} ({weight_file.stat().st_size // 1024 // 1024}MB)")
                            break
                    
                    if has_real_weights:
                        real_checkpoints.append(str(step_dir))
                    else:
                        # æ£€æŸ¥æ˜¯å¦ä¸ºå ä½ç¬¦
                        readme_file = step_dir / "README.md"
                        if readme_file.exists():
                            with open(readme_file, 'r', encoding='utf-8') as f:
                                content = f.read()
                            if "å ä½ç¬¦" in content or "placeholder" in content.lower():
                                print(f"  âŒ æ£€æµ‹åˆ°å ä½ç¬¦checkpoint: {step_dir}")
                                assert False, f"âŒ å‘ç°å ä½ç¬¦æƒé‡ï¼Œæ‹’ç»dry-run: {step_dir}"
    
    assert len(real_checkpoints) > 0, "âŒ æœªæ‰¾åˆ°ä»»ä½•çœŸå®æ¨¡å‹æƒé‡ï¼Œç–‘ä¼¼dry-run"
    print(f"  âœ… çœŸå®checkpoint: {len(real_checkpoints)}ä¸ª")

def check_training_curves():
    """æ£€æŸ¥è®­ç»ƒæ›²çº¿çœŸå®æ€§"""
    print("ğŸ” æ£€æŸ¥è®­ç»ƒæ›²çº¿...")
    
    # æ£€æŸ¥æœ€ç»ˆæŠ¥å‘Š
    report_file = Path("reports/rc1/rc1_final_report.json")
    if report_file.exists():
        with open(report_file, 'r', encoding='utf-8') as f:
            report = json.load(f)
        
        # æ£€æŸ¥æ¯ä¸ªç§å­çš„è®­ç»ƒæ›²çº¿
        for seed_result in report.get("seed_results", []):
            if "training" in seed_result and "training_curves" in seed_result["training"]:
                curves = seed_result["training"]["training_curves"]
                
                # æ£€æŸ¥å¥–åŠ±æ›²çº¿æ–¹å·®
                rewards = curves.get("rewards", [])
                kl_divs = curves.get("kl_divs", [])
                
                if len(rewards) > 1:
                    reward_std = __import__('statistics').stdev(rewards)
                    assert reward_std > 0, f"âŒ å¥–åŠ±æ›²çº¿æ— å˜åŒ–ï¼Œç–‘ä¼¼å¸¸æ•°/æ¨¡æ‹Ÿ"
                    print(f"  âœ… ç§å­{seed_result['seed']} å¥–åŠ±æ›²çº¿æ–¹å·®: {reward_std:.4f}")
                
                if len(kl_divs) > 1:
                    kl_std = __import__('statistics').stdev(kl_divs)
                    assert kl_std > 0, f"âŒ KLæ›²çº¿æ— å˜åŒ–ï¼Œç–‘ä¼¼å¸¸æ•°/æ¨¡æ‹Ÿ"
                    print(f"  âœ… ç§å­{seed_result['seed']} KLæ›²çº¿æ–¹å·®: {kl_std:.4f}")

def check_shadow_evaluation():
    """æ£€æŸ¥å½±å­è¯„ä¼°çœŸå®æ€§"""
    print("ğŸ” æ£€æŸ¥å½±å­è¯„ä¼°...")
    
    report_file = Path("reports/rc1/rc1_final_report.json")
    if report_file.exists():
        with open(report_file, 'r', encoding='utf-8') as f:
            report = json.load(f)
        
        # æ£€æŸ¥å½±å­æŒ‡æ ‡
        if "best_checkpoint" in report and "metrics" in report["best_checkpoint"]:
            shadow_metrics = report["best_checkpoint"]["metrics"].get("shadow_metrics", {})
            
            spearman = shadow_metrics.get("spearman")
            top10_overlap = shadow_metrics.get("top10_overlap")
            corr_improve = shadow_metrics.get("corr_improve_pct")
            
            # é˜²ä¼ªæ£€æŸ¥ï¼šä¸èƒ½ä¸ºNoneæˆ–æ˜æ˜¾çš„æ„é€ å€¼
            assert spearman is not None, "âŒ Spearmanç›¸å…³æ€§ä¸ºç©º"
            assert top10_overlap is not None, "âŒ Top10é‡åˆä¸ºç©º"
            assert corr_improve is not None, "âŒ ç›¸å…³æ€§æ”¹å–„ä¸ºç©º"
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ˜æ˜¾çš„æ¨¡æ‹Ÿå€¼
            assert not (spearman == 0.0 and top10_overlap == 0.0 and corr_improve == 0.0), \
                "âŒ å½±å­æŒ‡æ ‡å…¨ä¸º0ï¼Œç–‘ä¼¼æ„é€ "
            
            print(f"  âœ… Spearman: {spearman:.3f}")
            print(f"  âœ… Top10é‡åˆ: {top10_overlap:.3f}")
            print(f"  âœ… ç›¸å…³æ€§æ”¹å–„: {corr_improve:.1f}%")

def main():
    """ä¸»æ£€æŸ¥æµç¨‹"""
    print("ğŸš¨ RC1é˜²ä¼ªé—¸é—¨æ£€æŸ¥")
    print("=" * 50)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    run_mode = os.getenv("RUN_MODE")
    assert run_mode == "prod", f"âŒ RUN_MODE={run_mode}ï¼Œå¿…é¡»ä¸º'prod'"
    print(f"âœ… è¿è¡Œæ¨¡å¼: {run_mode}")
    
    try:
        check_scorer_connectivity()
        print()
        
        check_training_data()
        print()
        
        check_model_checkpoints()
        print()
        
        check_training_curves()
        print()
        
        check_shadow_evaluation()
        print()
        
        print("=" * 50)
        print("âœ… é˜²ä¼ªæ£€æŸ¥å…¨éƒ¨é€šè¿‡ - ç¡®è®¤ä¸ºçœŸå®è®­ç»ƒ")
        return 0
        
    except AssertionError as e:
        print(f"\nâŒ é˜²ä¼ªæ£€æŸ¥å¤±è´¥: {e}")
        print("âŒ æ‹’ç»è¿›å…¥RC1å‘å¸ƒæµç¨‹")
        return 1
    except Exception as e:
        print(f"\nâŒ æ£€æŸ¥è¿‡ç¨‹å‡ºé”™: {e}")
        return 1

if __name__ == "__main__":
    exit(main())
