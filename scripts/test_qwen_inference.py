#!/usr/bin/env python3
"""
Qwen3-4B-Thinking æ¨ç†æµ‹è¯•è„šæœ¬
æµ‹è¯• llama.cpp + GGUF çš„æ¨ç†æ€§èƒ½å’Œè´¨é‡
"""

import os
import sys
import subprocess
import time
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class QwenLlamaCppTester:
    """Qwen + llama.cpp æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.home_dir = Path.home()
        self.llama_cpp_dir = self.home_dir / "llama_cpp_workspace" / "llama.cpp"
        self.models_dir = self.home_dir / "llama_cpp_workspace" / "models"
        self.main_binary = self.llama_cpp_dir / "main"
        
        self.test_prompts = [
            {
                "id": "math_basic",
                "prompt": "è¯·è®¡ç®—ï¼š25 Ã— 4 = ?",
                "expected_type": "math",
                "description": "åŸºç¡€æ•°å­¦è®¡ç®—"
            },
            {
                "id": "reasoning_simple", 
                "prompt": "å¦‚æœä¸€ä¸ªæ­£æ–¹å½¢çš„å‘¨é•¿æ˜¯20å˜ç±³ï¼Œå®ƒçš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿè¯·è¯¦ç»†è¯´æ˜è®¡ç®—æ­¥éª¤ã€‚",
                "expected_type": "math_reasoning",
                "description": "å‡ ä½•æ¨ç†"
            },
            {
                "id": "question_ambiguous",
                "prompt": "ä»–ä»€ä¹ˆæ—¶å€™æ¥çš„ï¼Ÿ",
                "expected_type": "clarification",
                "description": "æ­§ä¹‰æ¾„æ¸…æµ‹è¯•"
            },
            {
                "id": "multi_hop",
                "prompt": "ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°åœ¨å“ªä¸ªå›½å®¶ï¼Ÿè¿™ä¸ªå›½å®¶çš„é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ",
                "expected_type": "multi_hop",
                "description": "å¤šè·³æ¨ç†"
            },
            {
                "id": "thinking_chain",
                "prompt": "ä¸€ä¸ªç­çº§æœ‰30ä¸ªå­¦ç”Ÿï¼Œå…¶ä¸­60%æ˜¯å¥³ç”Ÿã€‚å¦‚æœå¥³ç”Ÿä¸­æœ‰1/3æˆ´çœ¼é•œï¼Œç”·ç”Ÿä¸­æœ‰1/2æˆ´çœ¼é•œï¼Œé‚£ä¹ˆå…¨ç­æˆ´çœ¼é•œçš„å­¦ç”Ÿæœ‰å¤šå°‘äººï¼Ÿè¯·ç”¨<think>æ ‡ç­¾æ˜¾ç¤ºä½ çš„æ€è€ƒè¿‡ç¨‹ã€‚",
                "expected_type": "thinking",
                "description": "æ€è€ƒé“¾æµ‹è¯•"
            }
        ]
    
    def check_environment(self) -> bool:
        """æ£€æŸ¥ç¯å¢ƒæ˜¯å¦å°±ç»ª"""
        logger.info("ğŸ” æ£€æŸ¥æµ‹è¯•ç¯å¢ƒ...")
        
        # æ£€æŸ¥ llama.cpp å¯æ‰§è¡Œæ–‡ä»¶
        if not self.main_binary.exists():
            logger.error(f"âŒ llama.cpp ä¸»ç¨‹åºæœªæ‰¾åˆ°: {self.main_binary}")
            logger.info("è¯·å…ˆè¿è¡Œ: ./scripts/setup_llama_cpp.sh")
            return False
        
        logger.info(f"âœ… llama.cpp ä¸»ç¨‹åº: {self.main_binary}")
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        model_files = list(self.models_dir.glob("*.gguf"))
        if not model_files:
            logger.error(f"âŒ æœªæ‰¾åˆ° GGUF æ¨¡å‹æ–‡ä»¶: {self.models_dir}")
            logger.info("è¯·å…ˆè¿è¡Œ: python scripts/download_qwen_model.py")
            return False
        
        self.model_path = model_files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ¨¡å‹
        logger.info(f"âœ… ä½¿ç”¨æ¨¡å‹: {self.model_path}")
        
        return True
    
    def run_inference(self, prompt: str, max_tokens: int = 200, temperature: float = 0.7) -> Optional[str]:
        """è¿è¡Œå•æ¬¡æ¨ç†"""
        try:
            cmd = [
                str(self.main_binary),
                "-m", str(self.model_path),
                "-p", prompt,
                "-n", str(max_tokens),
                "--temp", str(temperature),
                "-c", "2048",  # ä¸Šä¸‹æ–‡é•¿åº¦
                "--mlock",     # é”å®šå†…å­˜
                "-ngl", "999", # GPU å±‚æ•°ï¼ˆå¯¹ Metal æœ‰æ•ˆï¼‰
            ]
            
            logger.info(f"ğŸš€ æ‰§è¡Œæ¨ç†: {prompt[:50]}...")
            start_time = time.time()
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=120  # 2åˆ†é’Ÿè¶…æ—¶
            )
            
            end_time = time.time()
            inference_time = end_time - start_time
            
            if result.returncode == 0:
                # è§£æè¾“å‡ºï¼Œæå–æ¨¡å‹å“åº”
                output = result.stdout
                
                # llama.cpp çš„è¾“å‡ºæ ¼å¼é€šå¸¸æ˜¯ï¼šæç¤ºè¯ + æ¨¡å‹å“åº”
                # éœ€è¦åˆ†ç¦»å‡ºæ¨¡å‹çš„å“åº”éƒ¨åˆ†
                response = self._extract_model_response(output, prompt)
                
                logger.info(f"âœ… æ¨ç†å®Œæˆ ({inference_time:.2f}ç§’)")
                return response
            else:
                logger.error(f"âŒ æ¨ç†å¤±è´¥: {result.stderr}")
                return None
                
        except subprocess.TimeoutExpired:
            logger.error("âŒ æ¨ç†è¶…æ—¶")
            return None
        except Exception as e:
            logger.error(f"âŒ æ¨ç†å¼‚å¸¸: {e}")
            return None
    
    def _extract_model_response(self, full_output: str, prompt: str) -> str:
        """ä»å®Œæ•´è¾“å‡ºä¸­æå–æ¨¡å‹å“åº”"""
        # ç®€å•çš„å“åº”æå–é€»è¾‘
        lines = full_output.split('\n')
        
        # æŸ¥æ‰¾åŒ…å«æ¨¡å‹å“åº”çš„è¡Œ
        response_lines = []
        capturing = False
        
        for line in lines:
            # è·³è¿‡ç³»ç»Ÿè¾“å‡ºå’Œæç¤ºè¯
            if any(skip in line.lower() for skip in ['llama', 'main:', 'log', 'perplexity']):
                continue
            
            # å¦‚æœæ‰¾åˆ°æç¤ºè¯ï¼Œå¼€å§‹æ•è·åç»­å†…å®¹
            if prompt.strip() in line:
                capturing = True
                # æå–æç¤ºè¯ä¹‹åçš„éƒ¨åˆ†
                prompt_index = line.find(prompt.strip())
                if prompt_index >= 0:
                    after_prompt = line[prompt_index + len(prompt.strip()):].strip()
                    if after_prompt:
                        response_lines.append(after_prompt)
                continue
            
            if capturing and line.strip():
                response_lines.append(line.strip())
        
        response = '\n'.join(response_lines).strip()
        
        # å¦‚æœæå–å¤±è´¥ï¼Œè¿”å›åŸå§‹è¾“å‡ºçš„æ¸…ç†ç‰ˆæœ¬
        if not response:
            # ç§»é™¤æ˜æ˜¾çš„ç³»ç»Ÿè¾“å‡º
            cleaned_lines = []
            for line in lines:
                if line.strip() and not any(skip in line.lower() for skip in ['llama', 'main:', 'log']):
                    cleaned_lines.append(line.strip())
            response = '\n'.join(cleaned_lines[-10:])  # å–æœ€å10è¡Œ
        
        return response
    
    def evaluate_response(self, prompt_data: Dict[str, Any], response: str) -> Dict[str, Any]:
        """è¯„ä¼°å“åº”è´¨é‡"""
        evaluation = {
            "prompt_id": prompt_data["id"],
            "prompt": prompt_data["prompt"],
            "response": response,
            "response_length": len(response),
            "has_thinking": "<think>" in response.lower() or "æ€è€ƒ" in response,
            "has_question": "?" in response or "ï¼Ÿ" in response,
            "expected_type": prompt_data["expected_type"],
            "quality_score": 0,
            "issues": []
        }
        
        # åŸºç¡€è´¨é‡æ£€æŸ¥
        if not response or len(response) < 10:
            evaluation["issues"].append("å“åº”è¿‡çŸ­æˆ–ä¸ºç©º")
            evaluation["quality_score"] = 0
        else:
            evaluation["quality_score"] = 50  # åŸºç¡€åˆ†
            
            # æ ¹æ®æœŸæœ›ç±»å‹è¿›è¡Œè¯„ä¼°
            expected_type = prompt_data["expected_type"]
            
            if expected_type == "math" or expected_type == "math_reasoning":
                if any(char in response for char in ['=', 'Ã—', 'Ã·', '+', '-']):
                    evaluation["quality_score"] += 20
                if "æ­¥éª¤" in response or "è®¡ç®—" in response:
                    evaluation["quality_score"] += 15
            
            elif expected_type == "clarification":
                if "?" in response or "ï¼Ÿ" in response:
                    evaluation["quality_score"] += 25
                if any(word in response for word in ["è¯·é—®", "å“ª", "ä»€ä¹ˆ", "æ¾„æ¸…"]):
                    evaluation["quality_score"] += 15
            
            elif expected_type == "multi_hop":
                if len(response) > 100:  # å¤šè·³æ¨ç†é€šå¸¸éœ€è¦æ›´é•¿çš„å“åº”
                    evaluation["quality_score"] += 20
                if any(word in response for word in ["é¦–å…ˆ", "ç„¶å", "æœ€å", "ç¬¬ä¸€", "ç¬¬äºŒ"]):
                    evaluation["quality_score"] += 15
            
            elif expected_type == "thinking":
                if evaluation["has_thinking"]:
                    evaluation["quality_score"] += 30
                else:
                    evaluation["issues"].append("ç¼ºå°‘æ€è€ƒè¿‡ç¨‹æ ‡ç­¾")
        
        # æœ€ç»ˆè´¨é‡ç­‰çº§
        if evaluation["quality_score"] >= 80:
            evaluation["quality_grade"] = "A"
        elif evaluation["quality_score"] >= 60:
            evaluation["quality_grade"] = "B"
        else:
            evaluation["quality_grade"] = "C"
        
        return evaluation
    
    def run_performance_test(self) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        logger.info("âš¡ å¼€å§‹æ€§èƒ½æµ‹è¯•...")
        
        performance_results = {
            "test_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "model_path": str(self.model_path),
            "test_results": [],
            "summary": {}
        }
        
        total_time = 0
        successful_tests = 0
        
        for prompt_data in self.test_prompts:
            logger.info(f"ğŸ§ª æµ‹è¯•: {prompt_data['description']}")
            
            start_time = time.time()
            response = self.run_inference(prompt_data["prompt"])
            end_time = time.time()
            
            inference_time = end_time - start_time
            total_time += inference_time
            
            if response:
                successful_tests += 1
                evaluation = self.evaluate_response(prompt_data, response)
                evaluation["inference_time"] = inference_time
                performance_results["test_results"].append(evaluation)
                
                logger.info(f"ğŸ“Š è´¨é‡: {evaluation['quality_grade']} ({evaluation['quality_score']}/100)")
                logger.info(f"â±ï¸ æ—¶é—´: {inference_time:.2f}ç§’")
                logger.info(f"ğŸ“ å“åº”: {response[:100]}...")
            else:
                logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {prompt_data['id']}")
                performance_results["test_results"].append({
                    "prompt_id": prompt_data["id"],
                    "status": "failed",
                    "inference_time": inference_time
                })
            
            logger.info("-" * 50)
        
        # è®¡ç®—æ€»ç»“ç»Ÿè®¡
        performance_results["summary"] = {
            "total_tests": len(self.test_prompts),
            "successful_tests": successful_tests,
            "success_rate": successful_tests / len(self.test_prompts),
            "total_time": total_time,
            "avg_time_per_test": total_time / len(self.test_prompts),
            "quality_distribution": self._calculate_quality_distribution(performance_results["test_results"])
        }
        
        return performance_results
    
    def _calculate_quality_distribution(self, test_results: List[Dict[str, Any]]) -> Dict[str, int]:
        """è®¡ç®—è´¨é‡åˆ†å¸ƒ"""
        distribution = {"A": 0, "B": 0, "C": 0, "failed": 0}
        
        for result in test_results:
            if "quality_grade" in result:
                distribution[result["quality_grade"]] += 1
            else:
                distribution["failed"] += 1
        
        return distribution
    
    def save_results(self, results: Dict[str, Any]) -> str:
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        results_file = f"qwen_llama_cpp_test_results_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜: {results_file}")
        return results_file
    
    def print_summary(self, results: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        summary = results["summary"]
        
        print("\n" + "=" * 60)
        print("ğŸ¯ Qwen3-4B-Thinking + llama.cpp æ€§èƒ½æµ‹è¯•æ€»ç»“")
        print("=" * 60)
        print(f"ğŸ“Š æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"âœ… æˆåŠŸæ•°: {summary['successful_tests']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {summary['success_rate']:.1%}")
        print(f"â±ï¸ æ€»æ—¶é—´: {summary['total_time']:.2f}ç§’")
        print(f"âš¡ å¹³å‡æ—¶é—´: {summary['avg_time_per_test']:.2f}ç§’/æµ‹è¯•")
        
        print(f"\nğŸ† è´¨é‡åˆ†å¸ƒ:")
        quality_dist = summary['quality_distribution']
        for grade, count in quality_dist.items():
            if count > 0:
                print(f"   {grade}çº§: {count} ä¸ª")
        
        print("\nğŸ“ è¯¦ç»†ç»“æœ:")
        for result in results["test_results"]:
            if "quality_grade" in result:
                print(f"   {result['prompt_id']}: {result['quality_grade']}çº§ ({result['inference_time']:.2f}s)")
            else:
                print(f"   {result['prompt_id']}: å¤±è´¥")
        
        print("=" * 60)

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ Qwen3-4B-Thinking + llama.cpp æ¨ç†æµ‹è¯•")
    logger.info("=" * 60)
    
    tester = QwenLlamaCppTester()
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not tester.check_environment():
        logger.error("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·å…ˆå®Œæˆç¯å¢ƒæ­å»º")
        sys.exit(1)
    
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    results = tester.run_performance_test()
    
    # ä¿å­˜å’Œå±•ç¤ºç»“æœ
    results_file = tester.save_results(results)
    tester.print_summary(results)
    
    logger.info(f"ğŸ‰ æµ‹è¯•å®Œæˆï¼è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

if __name__ == "__main__":
    main()
