#!/usr/bin/env python3
"""
æ‰“åˆ†å™¨è¿é€šæ€§æ¢é’ˆ - éªŒè¯çœŸå®APIè°ƒç”¨
"""

import argparse
import json
import time
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.evaluation.advanced_reward_system import MultiDimensionalRewardSystem

def probe_scorer(n_samples=8, provider="deepseek_r1", live=True):
    """æ¢æµ‹æ‰“åˆ†å™¨è¿é€šæ€§"""
    print(f"ğŸ” æ¢æµ‹æ‰“åˆ†å™¨è¿é€šæ€§: {provider}")
    print("=" * 40)
    
    # åˆå§‹åŒ–è¯„åˆ†ç³»ç»Ÿ
    scorer = MultiDimensionalRewardSystem(
        model_name=provider,
        temperature=0.0,
        top_p=0.0
    )
    
    # æµ‹è¯•æ ·æœ¬
    test_samples = [
        {
            "query": "è®¡ç®— 15 + 23 Ã— 4",
            "response": "æˆ‘æ¥è®¡ç®—è¿™ä¸ªè¡¨è¾¾å¼ï¼š\næŒ‰ç…§è¿ç®—é¡ºåºï¼š15 + 23 Ã— 4 = 15 + 92 = 107",
            "task_type": "math",
            "needs_clarification": False
        },
        {
            "query": "å¸®æˆ‘åˆ†ææŠ•èµ„æ–¹æ¡ˆ",
            "response": "æˆ‘éœ€è¦æ›´å¤šä¿¡æ¯æ¥ä¸ºæ‚¨åˆ†æï¼š\n1. æŠ•èµ„ç±»å‹å’Œé‡‘é¢\n2. é£é™©åå¥½\n3. æŠ•èµ„æœŸé™",
            "task_type": "clarify", 
            "needs_clarification": True
        },
        {
            "query": "è°æ˜¯ç¾å›½ç¬¬ä¸€ä»»æ€»ç»Ÿï¼Ÿä»–çš„ä»»æœŸæ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ",
            "response": "ç¾å›½ç¬¬ä¸€ä»»æ€»ç»Ÿæ˜¯ä¹”æ²»Â·åç››é¡¿ï¼Œä»»æœŸä»1789å¹´åˆ°1797å¹´ã€‚",
            "task_type": "multihop",
            "needs_clarification": False
        }
    ] * (n_samples // 3 + 1)
    
    results = []
    total_api_calls = 0
    total_latency = 0
    
    for i, sample in enumerate(test_samples[:n_samples]):
        print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬ {i+1}/{n_samples}: {sample['task_type']}")
        
        start_time = time.time()
        try:
            # æ‰§è¡Œè¯„åˆ†ï¼ˆK=3æŠ•ç¥¨ï¼‰
            dialogue = {
                "query": sample["query"],
                "response": sample["response"],
                "needs_clarification": sample["needs_clarification"]
            }
            
            result = scorer.evaluate_dialogue(dialogue)
            
            end_time = time.time()
            latency = end_time - start_time
            total_latency += latency
            
            # æ£€æŸ¥ç»“æœ
            if "error" in result:
                print(f"  âŒ è¯„åˆ†å¤±è´¥: {result['error']}")
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰çœŸå®APIè°ƒç”¨
            api_calls = result.get("api_calls", 0)
            total_api_calls += api_calls
            
            final_score = result.get("final_score", 0)
            variance = result.get("variance", 0)
            
            print(f"  âœ… è¯„åˆ†: {final_score:.3f}, æ–¹å·®: {variance:.3f}")
            print(f"  ğŸ“ APIè°ƒç”¨: {api_calls}æ¬¡, å»¶è¿Ÿ: {latency:.2f}s")
            
            results.append({
                "sample_id": i,
                "final_score": final_score,
                "variance": variance,
                "api_calls": api_calls,
                "latency": latency
            })
            
        except Exception as e:
            print(f"  âŒ è¯„åˆ†å¼‚å¸¸: {e}")
            continue
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 40)
    print("ğŸ¯ æ¢æµ‹ç»“æœæ±‡æ€»:")
    
    if results:
        avg_score = sum(r["final_score"] for r in results) / len(results)
        avg_variance = sum(r["variance"] for r in results) / len(results)
        avg_latency = total_latency / len(results)
        
        print(f"  ğŸ“Š æˆåŠŸæ ·æœ¬: {len(results)}/{n_samples}")
        print(f"  ğŸ“Š å¹³å‡è¯„åˆ†: {avg_score:.3f}")
        print(f"  ğŸ“Š å¹³å‡æ–¹å·®: {avg_variance:.3f}")
        print(f"  ğŸ“Š å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}s")
        print(f"  ğŸ“ æ€»APIè°ƒç”¨: {total_api_calls}æ¬¡")
        
        # éªŒè¯è¿é€šæ€§
        assert total_api_calls > 0, "âŒ æ— çœŸå®APIè°ƒç”¨ï¼Œè¿é€šæ€§å¤±è´¥"
        assert avg_latency > 0.1, "âŒ å»¶è¿Ÿè¿‡ä½ï¼Œç–‘ä¼¼æ¨¡æ‹Ÿå“åº”"
        assert avg_variance >= 0, "âŒ æ–¹å·®å¼‚å¸¸"
        
        print("  âœ… è¿é€šæ€§éªŒè¯é€šè¿‡")
        
        # ä¿å­˜æ¢æµ‹ç»“æœ
        probe_result = {
            "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
            "provider": provider,
            "n_samples": n_samples,
            "successful_samples": len(results),
            "total_api_calls": total_api_calls,
            "avg_latency": avg_latency,
            "avg_score": avg_score,
            "avg_variance": avg_variance
        }
        
        with open("reports/rc1/scorer_probe.json", 'w', encoding='utf-8') as f:
            json.dump(probe_result, f, indent=2)
        
        return True
    else:
        print("  âŒ æ‰€æœ‰æ ·æœ¬è¯„åˆ†å¤±è´¥")
        return False

def main():
    parser = argparse.ArgumentParser(description="æ‰“åˆ†å™¨è¿é€šæ€§æ¢é’ˆ")
    parser.add_argument('--n', type=int, default=8, help='æµ‹è¯•æ ·æœ¬æ•°é‡')
    parser.add_argument('--provider', default='deepseek_r1', help='æ‰“åˆ†å™¨æä¾›å•†')
    parser.add_argument('--live', action='store_true', help='å®æ—¶æ¨¡å¼')
    
    args = parser.parse_args()
    
    success = probe_scorer(args.n, args.provider, args.live)
    return 0 if success else 1

if __name__ == "__main__":
    exit(main())
