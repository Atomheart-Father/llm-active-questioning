#!/usr/bin/env python3
"""
æ‰“åˆ†å™¨è¿é€šæ€§æ¢é’ˆ - éªŒè¯çœŸå®APIè°ƒç”¨
"""

import argparse
import json
import time
import sys
import os
import uuid
import pathlib
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.scoring.providers import gemini as gsc

LEDGER = pathlib.Path("reports/rc1/scoring_ledger.jsonl")
LEDGER.parent.mkdir(parents=True, exist_ok=True)

def _write_ledger(rec: dict):
    with LEDGER.open("a") as f:
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")

def score_once(prompt: str):
    res = gsc.score(prompt, model=os.getenv("GEMINI_MODEL","gemini-2.5-flash"), require_live=True)
    # æœŸæœ›è¾“å‡ºï¼š{'score':float[0,1], 'latency_ms':int, 'usage':{...}, 'raw':str}
    assert 0.0 <= float(res["score"]) <= 1.0
    bill = (res["usage"].get("total_tokens") 
            or (res["usage"].get("prompt_tokens",0) + res["usage"].get("completion_tokens",0)))
    rec = {
        "ts": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "provider": "gemini",
        "billable_tokens": bill,
        "latency_ms": res["latency_ms"],
        "status": "ok",
        "cache_hit": False,
        "request_id": str(uuid.uuid4())
    }
    _write_ledger(rec)
    return res

def probe_scorer(n_samples=8, provider="gemini", live=True):
    """æ¢æµ‹æ‰“åˆ†å™¨è¿é€šæ€§"""
    print(f"ğŸ” æ¢æµ‹æ‰“åˆ†å™¨è¿é€šæ€§: {provider}")
    print("=" * 40)
    
    # æµ‹è¯•æ ·æœ¬
    test_samples = [
        {
            "query": "è®¡ç®— 15 + 23 Ã— 4",
            "response": "æˆ‘æ¥è®¡ç®—è¿™ä¸ªè¡¨è¾¾å¼ï¼š\næŒ‰ç…§è¿ç®—é¡ºåºï¼š15 + 23 Ã— 4 = 15 + 92 = 107",
            "task_type": "math",
            "needs_clarification": False
        },
        {
            "query": "å¸®æˆ‘åˆ†ææŠ•èµ„æ–¹æ¡ˆ",
            "response": "æˆ‘éœ€è¦æ›´å¤šä¿¡æ¯æ¥ä¸ºæ‚¨åˆ†æï¼š\n1. æŠ•èµ„ç±»å‹å’Œé‡‘é¢\n2. é£é™©åå¥½\n3. æŠ•èµ„æœŸé™",
            "task_type": "clarify", 
            "needs_clarification": True
        },
        {
            "query": "è°æ˜¯ç¾å›½ç¬¬ä¸€ä»»æ€»ç»Ÿï¼Ÿä»–çš„ä»»æœŸæ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ",
            "response": "ç¾å›½ç¬¬ä¸€ä»»æ€»ç»Ÿæ˜¯ä¹”æ²»Â·åç››é¡¿ï¼Œä»»æœŸä»1789å¹´åˆ°1797å¹´ã€‚",
            "task_type": "multihop",
            "needs_clarification": False
        }
    ] * (n_samples // 3 + 1)
    
    results = []
    total_api_calls = 0
    total_latency = 0
    
    for i, sample in enumerate(test_samples[:n_samples]):
        print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬ {i+1}/{n_samples}: {sample['task_type']}")
        
        start_time = time.time()
        try:
            # æ„å»ºè¯„åˆ†æç¤º
            prompt = f"""è¯·å¯¹ä»¥ä¸‹å¯¹è¯è¿›è¡Œè¯„åˆ†ï¼ˆ0-1åˆ†ï¼‰ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{sample["query"]}
AIå›ç­”ï¼š{sample["response"]}

è¯·è¿”å›JSONæ ¼å¼ï¼š{{"score": 0.75}}"""
            
            # ç›´æ¥è°ƒç”¨æ–°ç‰ˆé€‚é…å™¨
            result = score_once(prompt)
            
            end_time = time.time()
            latency = end_time - start_time
            total_latency += latency
            
            # æ‰€æœ‰è°ƒç”¨éƒ½æ˜¯çœŸå®APIè°ƒç”¨
            api_calls = 1
            total_api_calls += api_calls
            
            final_score = result["score"]
            variance = 0.0  # å•æ¬¡è°ƒç”¨æ— æ–¹å·®
            
            print(f"  âœ… è¯„åˆ†: {final_score:.3f}, æ–¹å·®: {variance:.3f}")
            print(f"  ğŸ“ APIè°ƒç”¨: {api_calls}æ¬¡, å»¶è¿Ÿ: {latency:.2f}s")
            
            results.append({
                "sample_id": i,
                "final_score": final_score,
                "variance": variance,
                "api_calls": api_calls,
                "latency": latency
            })
            
        except Exception as e:
            print(f"å¯¹è¯è¯„ä¼°å¤±è´¥: {e}")
            print(f"  âŒ è¯„åˆ†å¼‚å¸¸: {e}")
            continue
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 40)
    print("ğŸ¯ æ¢æµ‹ç»“æœæ±‡æ€»:")
    
    if results:
        avg_score = sum(r["final_score"] for r in results) / len(results)
        avg_variance = sum(r["variance"] for r in results) / len(results)
        avg_latency = total_latency / len(results)
        
        print(f"  ğŸ“Š æˆåŠŸæ ·æœ¬: {len(results)}/{n_samples}")
        print(f"  ğŸ“Š å¹³å‡è¯„åˆ†: {avg_score:.3f}")
        print(f"  ğŸ“Š å¹³å‡æ–¹å·®: {avg_variance:.3f}")
        print(f"  ğŸ“Š å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}s")
        print(f"  ğŸ“ æ€»APIè°ƒç”¨: {total_api_calls}æ¬¡")
        
        # éªŒè¯è¿é€šæ€§
        assert total_api_calls > 0, "âŒ æ— çœŸå®APIè°ƒç”¨ï¼Œè¿é€šæ€§å¤±è´¥"
        assert avg_latency > 0.1, "âŒ å»¶è¿Ÿè¿‡ä½ï¼Œç–‘ä¼¼æ¨¡æ‹Ÿå“åº”"
        assert avg_variance >= 0, "âŒ æ–¹å·®å¼‚å¸¸"
        
        print("  âœ… è¿é€šæ€§éªŒè¯é€šè¿‡")
        
        # ä¿å­˜æ¢æµ‹ç»“æœ
        probe_result = {
            "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
            "provider": provider,
            "n_samples": n_samples,
            "successful_samples": len(results),
            "total_api_calls": total_api_calls,
            "avg_latency": avg_latency,
            "avg_score": avg_score,
            "avg_variance": avg_variance
        }
        
        with open("reports/rc1/scorer_probe.json", 'w', encoding='utf-8') as f:
            json.dump(probe_result, f, indent=2)
        
        return True
    else:
        print("  âŒ æ‰€æœ‰æ ·æœ¬è¯„åˆ†å¤±è´¥")
        return False

def main():
    parser = argparse.ArgumentParser(description="æ‰“åˆ†å™¨è¿é€šæ€§æ¢é’ˆ")
    parser.add_argument('--n', type=int, default=8, help='æµ‹è¯•æ ·æœ¬æ•°é‡')
    parser.add_argument('--provider', default='deepseek_r1', help='æ‰“åˆ†å™¨æä¾›å•†')
    parser.add_argument('--live', action='store_true', help='å®æ—¶æ¨¡å¼')
    
    args = parser.parse_args()
    
    success = probe_scorer(args.n, args.provider, args.live)
    return 0 if success else 1

if __name__ == "__main__":
    exit(main())
