#!/usr/bin/env python3
"""
è‡ªåŠ¨è®¡ç®—Round 2é¢„æ£€çŠ¶æ€
æ ¹æ®å„é¡¹æ£€æŸ¥ç»“æœè‡ªåŠ¨åˆ¤å®šæ˜¯å¦é€šè¿‡ï¼Œç¦æ­¢æ‰‹å·¥è®¾ç½®pass=true
"""

import json
import os
import sys
import time
import datetime
import subprocess
from pathlib import Path

def run_command(cmd, description):
    """è¿è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœ"""
    print(f"ğŸ” {description}...")
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        return result.returncode == 0, result.stdout, result.stderr
    except Exception as e:
        print(f"âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥: {e}")
        return False, "", str(e)

def check_round2_requirements():
    """æ£€æŸ¥Round 2æ‰€æœ‰è¦æ±‚"""
    checks = {}
    
    # 1. é˜²ä¼ªæ£€æŸ¥ï¼ˆç¼“å­˜é˜ˆå€¼95%ï¼‰
    success, stdout, stderr = run_command(
        "python scripts/assert_not_simulated.py --cache_hit_lt 0.95",
        "é˜²ä¼ªæ£€æŸ¥ï¼ˆç¼“å­˜<95%ï¼‰"
    )
    checks["anti_simulation"] = {
        "passed": success,
        "description": "é˜²ä¼ªæ£€æŸ¥",
        "details": stdout if success else stderr
    }
    
    # 2. å½±å­è¿è¡Œæ£€æŸ¥
    success, stdout, stderr = run_command(
        "python -m src.evaluation.shadow_run --n 245 --seed 20250820 --stratify --tag pre_run_check",
        "å½±å­è¿è¡Œé¢„æ£€"
    )
    checks["shadow_run"] = {
        "passed": success,
        "description": "å½±å­è¿è¡Œé¢„æ£€",
        "details": stdout if success else stderr
    }
    
    # 3. æ£€æŸ¥å½±å­è¿è¡Œç»“æœæ–‡ä»¶
    shadow_files = list(Path("reports").glob("shadow_run_*.json"))
    if shadow_files:
        latest_shadow = max(shadow_files, key=lambda x: x.stat().st_mtime)
        try:
            with open(latest_shadow, 'r', encoding='utf-8') as f:
                shadow_data = json.load(f)
            
            # æ£€æŸ¥ç¨³æ€æŒ‡æ ‡
            correlations = shadow_data.get("correlations", {}).get("stable_dataset", {})
            spearman = correlations.get("spearman", 0)
            
            overlap_metrics = shadow_data.get("overlap_metrics", {})
            top10_overlap = overlap_metrics.get("top10_overlap", 0)
            
            # é¢„è·‘é—¨æ§›æ£€æŸ¥ï¼ˆæ¯”æ­£å¼RC1éªŒæ”¶é—¨æ§›ä½ï¼‰
            shadow_passed = spearman >= 0.55 and top10_overlap >= 0.60
            
            if not shadow_passed:
                print(f"âŒ å½±å­æŒ‡æ ‡æœªè¾¾é¢„è·‘é—¨æ§›:")
                print(f"   Spearman: {spearman:.3f} (éœ€è¦â‰¥0.55)")
                print(f"   Top10é‡åˆ: {top10_overlap:.3f} (éœ€è¦â‰¥0.60)")
                print(f"   å»ºè®®æ£€æŸ¥å¥–åŠ±èšåˆæˆ–æˆåŠŸæ ‡ç­¾å£å¾„")
            
            checks["shadow_metrics"] = {
                "passed": shadow_passed,
                "description": "å½±å­æŒ‡æ ‡æ£€æŸ¥",
                "details": f"Spearman: {spearman:.3f}, Top10é‡åˆ: {top10_overlap:.3f}",
                "spearman": spearman,
                "top10_overlap": top10_overlap
            }
        except Exception as e:
            checks["shadow_metrics"] = {
                "passed": False,
                "description": "å½±å­æŒ‡æ ‡æ£€æŸ¥",
                "details": f"æ— æ³•è¯»å–å½±å­è¿è¡Œç»“æœ: {e}"
            }
    else:
        checks["shadow_metrics"] = {
            "passed": False,
            "description": "å½±å­æŒ‡æ ‡æ£€æŸ¥", 
            "details": "æœªæ‰¾åˆ°å½±å­è¿è¡Œç»“æœæ–‡ä»¶"
        }
    
    # 4. æ•°æ®è´¨é‡æ£€æŸ¥
    seed_pool_exists = Path("data/rollouts/rc1_seed.jsonl").exists()
    balanced_exists = Path("data/rollouts/rc1_seed.balanced.jsonl").exists()
    
    checks["data_quality"] = {
        "passed": seed_pool_exists and balanced_exists,
        "description": "æ•°æ®è´¨é‡æ£€æŸ¥",
        "details": f"ç§å­æ± : {'âœ…' if seed_pool_exists else 'âŒ'}, å¹³è¡¡ç‰ˆ: {'âœ…' if balanced_exists else 'âŒ'}"
    }
    
    return checks

def generate_round2_report():
    """ç”ŸæˆRound 2æŠ¥å‘Š"""
    print("ğŸ”„ æ‰§è¡ŒRound 2è‡ªåŠ¨é¢„æ£€...")
    
    checks = check_round2_requirements()
    
    # è®¡ç®—æ€»ä½“é€šè¿‡çŠ¶æ€
    all_passed = all(check["passed"] for check in checks.values())
    
    # ç”ŸæˆæŠ¥å‘Š
    report = {
        "round": "round2",
        "timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat(),
        "pass": all_passed,
        "auto_generated": True,
        "checks": checks,
        "summary": {
            "total_checks": len(checks),
            "passed_checks": sum(1 for check in checks.values() if check["passed"]),
            "failed_checks": sum(1 for check in checks.values() if not check["passed"])
        }
    }
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = Path("reports/preflight/round2_pass.json")
    report_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # è¾“å‡ºç»“æœ
    print("\nğŸ“‹ Round 2é¢„æ£€ç»“æœ:")
    print("=" * 40)
    
    for check_name, check_data in checks.items():
        status = "âœ… PASS" if check_data["passed"] else "âŒ FAIL"
        print(f"{status} {check_data['description']}")
        if not check_data["passed"]:
            print(f"    è¯¦æƒ…: {check_data['details']}")
    
    print(f"\nğŸ¯ æ€»ä½“çŠ¶æ€: {'âœ… PASS' if all_passed else 'âŒ FAIL'}")
    print(f"ğŸ“„ æŠ¥å‘Šä¿å­˜: {report_file}")
    
    return 0 if all_passed else 1

if __name__ == "__main__":
    exit(generate_round2_report())
