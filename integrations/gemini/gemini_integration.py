#!/usr/bin/env python3
"""
Gemini APIé›†æˆæ¨¡å—
ç”¨äºæ•°æ®è½¬æ¢å’Œå¤šè½®å¯¹è¯æ„é€ 
"""

import requests
import json
import time
from typing import Dict, List, Any, Optional
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.utils.config import get_config
from src.utils.logging import get_logger


class GeminiDataGenerator:
    """Geminiæ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self, api_key: str = None):
        """
        åˆå§‹åŒ–Geminiæ•°æ®ç”Ÿæˆå™¨
        
        Args:
            api_key: Gemini APIå¯†é’¥
        """
        self.config = get_config()
        self.logger = get_logger("gemini_generator")
        
        # è®¾ç½®APIå¯†é’¥
        self.api_key = api_key or "AIzaSyBLECdu94qJWPFOZ--9dIKpeWaWjSGJ_z0"
        self.base_url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
        
        self.logger.info("Geminiæ•°æ®ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def test_api_connection(self) -> bool:
        """æµ‹è¯•APIè¿æ¥"""
        try:
            response = self._make_request("æµ‹è¯•è¿æ¥ï¼šè¯·ç®€å•å›ç­”'è¿æ¥æˆåŠŸ'")
            if response and "è¿æ¥æˆåŠŸ" in response:
                self.logger.info("âœ… Gemini APIè¿æ¥æµ‹è¯•æˆåŠŸ")
                return True
            else:
                self.logger.warning(f"âš ï¸ APIå“åº”å¼‚å¸¸: {response}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ APIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def _make_request(self, prompt: str, max_retries: int = 3) -> Optional[str]:
        """
        å‘é€Gemini APIè¯·æ±‚
        
        Args:
            prompt: æç¤ºæ–‡æœ¬
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬æˆ–None
        """
        headers = {
            'Content-Type': 'application/json',
            'X-goog-api-key': self.api_key
        }
        
        data = {
            "contents": [
                {
                    "parts": [
                        {
                            "text": prompt
                        }
                    ]
                }
            ]
        }
        
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    self.base_url,
                    headers=headers,
                    json=data,
                    timeout=30
                )
                
                if response.status_code == 200:
                    result = response.json()
                    
                    # è§£æGeminiå“åº”æ ¼å¼
                    if 'candidates' in result and len(result['candidates']) > 0:
                        candidate = result['candidates'][0]
                        if 'content' in candidate and 'parts' in candidate['content']:
                            parts = candidate['content']['parts']
                            if len(parts) > 0 and 'text' in parts[0]:
                                return parts[0]['text'].strip()
                    
                    self.logger.warning(f"æ„å¤–çš„APIå“åº”æ ¼å¼: {result}")
                    return None
                    
                else:
                    self.logger.warning(f"APIè¯·æ±‚å¤±è´¥ (çŠ¶æ€ç : {response.status_code}): {response.text}")
                    if attempt < max_retries - 1:
                        time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    
            except Exception as e:
                self.logger.error(f"è¯·æ±‚å¼‚å¸¸ (å°è¯• {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
        
        return None
    
    def generate_clarifying_dialogue(self, original_question: str, expected_answer: str = None) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ¾„æ¸…å¯¹è¯
        
        Args:
            original_question: åŸå§‹é—®é¢˜
            expected_answer: æœŸæœ›ç­”æ¡ˆï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ç”Ÿæˆçš„å¯¹è¯ç»“æ„
        """
        prompt = f"""
è¯·åŸºäºä»¥ä¸‹åŸå§‹é—®é¢˜ç”Ÿæˆä¸€ä¸ªå¤šè½®æ¾„æ¸…å¯¹è¯ã€‚

åŸå§‹é—®é¢˜: "{original_question}"

è¦æ±‚ï¼š
1. åˆ¤æ–­è¿™ä¸ªé—®é¢˜æ˜¯å¦æ¨¡ç³Šæˆ–ç¼ºå°‘ä¿¡æ¯
2. å¦‚æœæ¨¡ç³Šï¼Œç”Ÿæˆä¸€ä¸ªæ¾„æ¸…é—®é¢˜
3. æä¾›åˆç†çš„ç”¨æˆ·æ¾„æ¸…å›ç­”
4. ç»™å‡ºæœ€ç»ˆçš„å‡†ç¡®ç­”æ¡ˆ

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
{{
    "is_ambiguous": true/false,
    "clarifying_question": "æ¾„æ¸…é—®é¢˜ï¼ˆå¦‚æœéœ€è¦ï¼‰",
    "user_clarification": "ç”¨æˆ·çš„æ¾„æ¸…å›ç­”",
    "final_answer": "æœ€ç»ˆç­”æ¡ˆ",
    "reasoning": "åˆ¤æ–­ç†ç”±"
}}

ç¤ºä¾‹ï¼š
åŸå§‹é—®é¢˜: "ä»–ä»€ä¹ˆæ—¶å€™å‡ºç”Ÿçš„ï¼Ÿ"
è¾“å‡º: {{"is_ambiguous": true, "clarifying_question": "è¯·é—®æ‚¨æŒ‡çš„æ˜¯å“ªä½äººç‰©ï¼Ÿ", "user_clarification": "æˆ‘æŒ‡çš„æ˜¯çˆ±å› æ–¯å¦", "final_answer": "çˆ±å› æ–¯å¦äº1879å¹´3æœˆ14æ—¥å‡ºç”Ÿ", "reasoning": "é—®é¢˜ä¸­çš„'ä»–'æŒ‡ä»£ä¸æ˜ç¡®"}}
"""
        
        response = self._make_request(prompt)
        
        if response:
            try:
                # å°è¯•è§£æJSON
                dialogue_data = json.loads(response)
                return dialogue_data
            except json.JSONDecodeError:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•æå–å†…å®¹
                self.logger.warning("JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬è§£æ")
                return {
                    "is_ambiguous": "?" in original_question or "ä»–" in original_question or "å¥¹" in original_question,
                    "clarifying_question": "è¯·æ‚¨æä¾›æ›´å¤šå…·ä½“ä¿¡æ¯ã€‚",
                    "user_clarification": "ç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯",
                    "final_answer": response,
                    "reasoning": "APIè¿”å›æ ¼å¼å¼‚å¸¸"
                }
        
        return None
    
    def generate_multi_hop_dialogue(self, question: str, context: str = None) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¤šè·³æ¨ç†å¯¹è¯
        
        Args:
            question: å¤æ‚é—®é¢˜
            context: èƒŒæ™¯ä¿¡æ¯
            
        Returns:
            å¤šè·³æ¨ç†å¯¹è¯ç»“æ„
        """
        prompt = f"""
è¯·ä¸ºä»¥ä¸‹å¤æ‚é—®é¢˜è®¾è®¡ä¸€ä¸ªå¤šè½®æ¨ç†å¯¹è¯ï¼Œæ¨¡æ‹ŸAIåŠ©æ‰‹é€šè¿‡é€æ­¥æé—®æ¥æ”¶é›†ä¿¡æ¯å¹¶æ¨ç†çš„è¿‡ç¨‹ã€‚

é—®é¢˜: "{question}"
{f"èƒŒæ™¯ä¿¡æ¯: {context}" if context else ""}

è¦æ±‚ï¼š
1. å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸º2-3ä¸ªå­é—®é¢˜
2. è®¾è®¡AIåŠ©æ‰‹çš„é€æ­¥æé—®
3. æä¾›ç”¨æˆ·çš„åˆç†å›ç­”
4. å±•ç¤ºæœ€ç»ˆçš„æ¨ç†å’Œç­”æ¡ˆ

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
{{
    "original_question": "åŸå§‹é—®é¢˜",
    "reasoning_steps": [
        {{
            "step": 1,
            "ai_question": "AIçš„é—®é¢˜",
            "user_answer": "ç”¨æˆ·å›ç­”",
            "reasoning": "è¿™ä¸€æ­¥çš„æ¨ç†"
        }}
    ],
    "final_answer": "æœ€ç»ˆç­”æ¡ˆ",
    "confidence": "high/medium/low"
}}
"""
        
        response = self._make_request(prompt)
        
        if response:
            try:
                return json.loads(response)
            except json.JSONDecodeError:
                return {
                    "original_question": question,
                    "reasoning_steps": [
                        {
                            "step": 1,
                            "ai_question": "è®©æˆ‘å¸®æ‚¨åˆ†æè¿™ä¸ªé—®é¢˜ï¼Œè¯·é—®æ‚¨éœ€è¦ä»€ä¹ˆå…·ä½“ä¿¡æ¯ï¼Ÿ",
                            "user_answer": "ç”¨æˆ·æä¾›ç›¸å…³ä¿¡æ¯",
                            "reasoning": "éœ€è¦æ”¶é›†æ›´å¤šä¿¡æ¯è¿›è¡Œæ¨ç†"
                        }
                    ],
                    "final_answer": response,
                    "confidence": "medium"
                }
        
        return None
    
    def batch_generate_dialogues(self, questions: List[str], dialogue_type: str = "clarifying") -> List[Dict[str, Any]]:
        """
        æ‰¹é‡ç”Ÿæˆå¯¹è¯æ•°æ®
        
        Args:
            questions: é—®é¢˜åˆ—è¡¨
            dialogue_type: å¯¹è¯ç±»å‹ ("clarifying" æˆ– "multi_hop")
            
        Returns:
            ç”Ÿæˆçš„å¯¹è¯åˆ—è¡¨
        """
        self.logger.info(f"å¼€å§‹æ‰¹é‡ç”Ÿæˆ{len(questions)}ä¸ª{dialogue_type}å¯¹è¯...")
        
        results = []
        for i, question in enumerate(questions):
            try:
                if dialogue_type == "clarifying":
                    dialogue = self.generate_clarifying_dialogue(question)
                elif dialogue_type == "multi_hop":
                    dialogue = self.generate_multi_hop_dialogue(question)
                else:
                    self.logger.warning(f"æœªçŸ¥çš„å¯¹è¯ç±»å‹: {dialogue_type}")
                    continue
                
                if dialogue:
                    dialogue["source_question"] = question
                    dialogue["generation_type"] = dialogue_type
                    results.append(dialogue)
                    
                    self.logger.info(f"ç”Ÿæˆå®Œæˆ {i+1}/{len(questions)}: {question[:30]}...")
                else:
                    self.logger.warning(f"ç”Ÿæˆå¤±è´¥: {question}")
                
                # é¿å…APIé™æµ
                if i % 5 == 4:
                    time.sleep(2)
                    
            except Exception as e:
                self.logger.error(f"ç”Ÿæˆå¯¹è¯å¤±è´¥ ({question}): {e}")
                continue
        
        self.logger.info(f"æ‰¹é‡ç”Ÿæˆå®Œæˆï¼ŒæˆåŠŸç”Ÿæˆ{len(results)}ä¸ªå¯¹è¯")
        return results
    
    def convert_qa_to_dialogue(self, qa_dataset: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """
        å°†QAæ•°æ®é›†è½¬æ¢ä¸ºå¤šè½®å¯¹è¯æ ¼å¼
        
        Args:
            qa_dataset: QAæ•°æ®é›† [{"question": "...", "answer": "..."}]
            
        Returns:
            è½¬æ¢åçš„å¯¹è¯æ•°æ®é›†
        """
        self.logger.info(f"å¼€å§‹è½¬æ¢{len(qa_dataset)}ä¸ªQAå¯¹ä¸ºå¯¹è¯æ ¼å¼...")
        
        questions = [item["question"] for item in qa_dataset]
        
        # ç”Ÿæˆæ¾„æ¸…å¯¹è¯
        clarifying_dialogues = self.batch_generate_dialogues(questions, "clarifying")
        
        # åˆå¹¶åŸå§‹ç­”æ¡ˆä¿¡æ¯
        for i, dialogue in enumerate(clarifying_dialogues):
            if i < len(qa_dataset):
                dialogue["original_answer"] = qa_dataset[i].get("answer", "")
                dialogue["dataset_info"] = qa_dataset[i]
        
        return clarifying_dialogues
    
    def save_generated_data(self, dialogues: List[Dict[str, Any]], output_file: str):
        """ä¿å­˜ç”Ÿæˆçš„å¯¹è¯æ•°æ®"""
        output_path = Path(output_file)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(dialogues, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"å·²ä¿å­˜{len(dialogues)}ä¸ªå¯¹è¯åˆ°: {output_path}")


def test_gemini_integration():
    """æµ‹è¯•Geminié›†æˆåŠŸèƒ½"""
    print("å¼€å§‹æµ‹è¯•Gemini APIé›†æˆ...")
    print("="*50)
    
    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    generator = GeminiDataGenerator()
    
    # æµ‹è¯•APIè¿æ¥
    print("ğŸ”„ æµ‹è¯•APIè¿æ¥...")
    if not generator.test_api_connection():
        print("âŒ APIè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥å¯†é’¥å’Œç½‘ç»œ")
        return
    
    # æµ‹è¯•æ¾„æ¸…å¯¹è¯ç”Ÿæˆ
    print("\nğŸ§ª æµ‹è¯•æ¾„æ¸…å¯¹è¯ç”Ÿæˆ...")
    test_questions = [
        "ä»–ä»€ä¹ˆæ—¶å€™å‡ºç”Ÿçš„ï¼Ÿ",
        "é‚£å®¶é¤å…å¥½åƒå—ï¼Ÿ",
        "é¢„è®¢ä¸€å¼ ç¥¨",
        "åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"  # æ§åˆ¶æ¡ˆä¾‹ï¼šä¸æ¨¡ç³Š
    ]
    
    clarifying_results = generator.batch_generate_dialogues(test_questions, "clarifying")
    
    print(f"\nâœ… æˆåŠŸç”Ÿæˆ{len(clarifying_results)}ä¸ªæ¾„æ¸…å¯¹è¯")
    
    # æ˜¾ç¤ºç¤ºä¾‹
    if clarifying_results:
        print("\nğŸ“ ç¤ºä¾‹å¯¹è¯:")
        example = clarifying_results[0]
        print(f"åŸé—®é¢˜: {example.get('source_question', '')}")
        print(f"æ˜¯å¦æ¨¡ç³Š: {example.get('is_ambiguous', False)}")
        if example.get('is_ambiguous'):
            print(f"æ¾„æ¸…é—®é¢˜: {example.get('clarifying_question', '')}")
            print(f"ç”¨æˆ·å›ç­”: {example.get('user_clarification', '')}")
        print(f"æœ€ç»ˆç­”æ¡ˆ: {example.get('final_answer', '')}")
    
    # æµ‹è¯•å¤šè·³æ¨ç†å¯¹è¯ç”Ÿæˆ
    print("\nğŸ” æµ‹è¯•å¤šè·³æ¨ç†å¯¹è¯ç”Ÿæˆ...")
    complex_questions = [
        "è°æ˜¯å†™ã€Šå“ˆåˆ©æ³¢ç‰¹ã€‹çš„ä½œè€…çš„ä¸ˆå¤«ï¼Ÿ",
        "ä¸–ç•Œä¸Šæœ€é«˜å±±å³°æ‰€åœ¨å›½å®¶çš„é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    multi_hop_results = generator.batch_generate_dialogues(complex_questions, "multi_hop")
    
    print(f"\nâœ… æˆåŠŸç”Ÿæˆ{len(multi_hop_results)}ä¸ªå¤šè·³å¯¹è¯")
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    all_results = {
        "clarifying_dialogues": clarifying_results,
        "multi_hop_dialogues": multi_hop_results,
        "test_timestamp": time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    generator.save_generated_data([all_results], "gemini_test_results.json")
    
    print("\nğŸ¯ Geminié›†æˆæµ‹è¯•å®Œæˆï¼")
    print("ğŸ“‹ å¯ä»¥å¼€å§‹å¤§è§„æ¨¡æ•°æ®è½¬æ¢å·¥ä½œ")


if __name__ == "__main__":
    test_gemini_integration()
