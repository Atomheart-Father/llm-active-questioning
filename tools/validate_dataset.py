#!/usr/bin/env python3
"""Dataset Validation Tool for Schema v1.1

æ ¡éªŒæ•°æ®ç»“æ„å’Œè´¨é‡ï¼Œç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šã€‚
"""

import json
import sys
from pathlib import Path
from collections import Counter, defaultdict
from typing import Dict, List, Any

from src.data.loader import DataLoader, Sample

def collect_statistics(samples: List[Sample]) -> Dict[str, Any]:
    """æ”¶é›†æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    stats = {
        "total_samples": len(samples),
        "domain_distribution": Counter(),
        "source_distribution": Counter(),
        "ask_required_distribution": Counter(),
        "ambiguity_types_distribution": Counter(),
        "action_types_distribution": Counter(),
        "turns_length_stats": {
            "min": float('inf'),
            "max": 0,
            "avg": 0,
            "total_turns": 0
        },
        "clarification_questions_stats": {
            "min": float('inf'),
            "max": 0,
            "avg": 0,
            "total_questions": 0
        },
        "minimal_clarifications_stats": {
            "min": float('inf'),
            "max": 0,
            "avg": 0
        }
    }

    total_turns = 0
    total_questions = 0
    total_min_clarifications = 0

    for sample in samples:
        # åŸºç¡€åˆ†å¸ƒç»Ÿè®¡
        stats["domain_distribution"][sample.domain] += 1
        stats["source_distribution"][sample.source] += 1

        if "ask_required" in sample.labels:
            stats["ask_required_distribution"][sample.labels["ask_required"]] += 1

        # æ­§ä¹‰ç±»å‹ç»Ÿè®¡
        if "ambiguity_types" in sample.labels:
            for amb_type in sample.labels["ambiguity_types"]:
                stats["ambiguity_types_distribution"][amb_type] += 1

        # åŠ¨ä½œç±»å‹ç»Ÿè®¡
        if "actions" in sample.reasoning:
            for action in sample.reasoning["actions"]:
                if isinstance(action, dict) and "t" in action:
                    stats["action_types_distribution"][action["t"]] += 1

        # è½®æ¬¡é•¿åº¦ç»Ÿè®¡
        turns_count = len(sample.turns)
        stats["turns_length_stats"]["min"] = min(stats["turns_length_stats"]["min"], turns_count)
        stats["turns_length_stats"]["max"] = max(stats["turns_length_stats"]["max"], turns_count)
        total_turns += turns_count

        # æ¾„æ¸…é—®é¢˜ç»Ÿè®¡
        if "good_question_set" in sample.labels:
            questions_count = len(sample.labels["good_question_set"])
            stats["clarification_questions_stats"]["min"] = min(
                stats["clarification_questions_stats"]["min"], questions_count
            )
            stats["clarification_questions_stats"]["max"] = max(
                stats["clarification_questions_stats"]["max"], questions_count
            )
            total_questions += questions_count

        # æœ€å°‘æ¾„æ¸…æ•°ç»Ÿè®¡
        if "minimal_clarifications" in sample.labels:
            min_clar = sample.labels["minimal_clarifications"]
            stats["minimal_clarifications_stats"]["min"] = min(
                stats["minimal_clarifications_stats"]["min"], min_clar
            )
            stats["minimal_clarifications_stats"]["max"] = max(
                stats["minimal_clarifications_stats"]["max"], min_clar
            )
            total_min_clarifications += min_clar

    # è®¡ç®—å¹³å‡å€¼
    if samples:
        stats["turns_length_stats"]["avg"] = total_turns / len(samples)
        stats["clarification_questions_stats"]["avg"] = total_questions / len(samples)
        stats["minimal_clarifications_stats"]["avg"] = total_min_clarifications / len(samples)

    # å¤„ç†è¾¹ç•Œæƒ…å†µ
    if stats["turns_length_stats"]["min"] == float('inf'):
        stats["turns_length_stats"]["min"] = 0
    if stats["clarification_questions_stats"]["min"] == float('inf'):
        stats["clarification_questions_stats"]["min"] = 0
    if stats["minimal_clarifications_stats"]["min"] == float('inf'):
        stats["minimal_clarifications_stats"]["min"] = 0

    stats["turns_length_stats"]["total_turns"] = total_turns
    stats["clarification_questions_stats"]["total_questions"] = total_questions

    return stats

def generate_markdown_report(stats: Dict[str, Any], validation_report: Dict[str, Any]) -> str:
    """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
    report = []

    # æ ‡é¢˜
    report.append("# æ•°æ®é›†æ ¡éªŒæŠ¥å‘Š")
    report.append("")
    report.append("## æ•°æ®é›†æ¦‚è§ˆ")
    report.append("")
    report.append(f"- **æ€»æ ·æœ¬æ•°**: {stats['total_samples']}")
    report.append(f"- **æ ¡éªŒé”™è¯¯**: {validation_report['error_count']}")
    report.append(f"- **æ ¡éªŒè­¦å‘Š**: {validation_report['warning_count']}")
    report.append("")

    # åˆ†å¸ƒç»Ÿè®¡
    report.append("## åˆ†å¸ƒç»Ÿè®¡")
    report.append("")

    # é¢†åŸŸåˆ†å¸ƒ
    report.append("### é¢†åŸŸåˆ†å¸ƒ")
    for domain, count in sorted(stats["domain_distribution"].items()):
        percentage = (count / stats["total_samples"]) * 100 if stats["total_samples"] > 0 else 0
        report.append(f"- **{domain}**: {count} ({percentage:.1f}%)")
    report.append("")

    # æ¥æºåˆ†å¸ƒ
    report.append("### æ¥æºåˆ†å¸ƒ")
    for source, count in sorted(stats["source_distribution"].items()):
        percentage = (count / stats["total_samples"]) * 100 if stats["total_samples"] > 0 else 0
        report.append(f"- **{source}**: {count} ({percentage:.1f}%)")
    report.append("")

    # æ¾„æ¸…éœ€æ±‚åˆ†å¸ƒ
    report.append("### æ¾„æ¸…éœ€æ±‚åˆ†å¸ƒ")
    for ask_required, count in sorted(stats["ask_required_distribution"].items()):
        percentage = (count / stats["total_samples"]) * 100 if stats["total_samples"] > 0 else 0
        report.append(f"- **éœ€è¦æ¾„æ¸…: {ask_required}**: {count} ({percentage:.1f}%)")
    report.append("")

    # æ­§ä¹‰ç±»å‹åˆ†å¸ƒ
    report.append("### æ­§ä¹‰ç±»å‹åˆ†å¸ƒ")
    for amb_type, count in sorted(stats["ambiguity_types_distribution"].items()):
        percentage = (count / stats["total_samples"]) * 100 if stats["total_samples"] > 0 else 0
        report.append(f"- **{amb_type}**: {count} ({percentage:.1f}%)")
    report.append("")

    # åŠ¨ä½œç±»å‹åˆ†å¸ƒ
    report.append("### æ¨ç†åŠ¨ä½œåˆ†å¸ƒ")
    for action_type, count in sorted(stats["action_types_distribution"].items()):
        total_actions = sum(stats["action_types_distribution"].values())
        percentage = (count / total_actions) * 100 if total_actions > 0 else 0
        report.append(f"- **{action_type}**: {count} ({percentage:.1f}%)")
    report.append("")

    # æ•°å€¼ç»Ÿè®¡
    report.append("## æ•°å€¼ç»Ÿè®¡")
    report.append("")

    # è½®æ¬¡é•¿åº¦
    turns_stats = stats["turns_length_stats"]
    report.append("### å¯¹è¯è½®æ¬¡ç»Ÿè®¡")
    report.append(f"- **æœ€å°è½®æ¬¡**: {turns_stats['min']}")
    report.append(f"- **æœ€å¤§è½®æ¬¡**: {turns_stats['max']}")
    report.append(f"- **å¹³å‡è½®æ¬¡**: {turns_stats['avg']:.1f}")
    report.append(f"- **æ€»è½®æ¬¡æ•°**: {turns_stats['total_turns']}")
    report.append("")

    # æ¾„æ¸…é—®é¢˜
    questions_stats = stats["clarification_questions_stats"]
    report.append("### æ¾„æ¸…é—®é¢˜ç»Ÿè®¡")
    report.append(f"- **æœ€å°é—®é¢˜æ•°**: {questions_stats['min']}")
    report.append(f"- **æœ€å¤§é—®é¢˜æ•°**: {questions_stats['max']}")
    report.append(f"- **å¹³å‡é—®é¢˜æ•°**: {questions_stats['avg']:.1f}")
    report.append(f"- **æ€»é—®é¢˜æ•°**: {questions_stats['total_questions']}")
    report.append("")

    # æœ€å°‘æ¾„æ¸…æ•°
    min_clar_stats = stats["minimal_clarifications_stats"]
    report.append("### æœ€å°‘æ¾„æ¸…æ•°ç»Ÿè®¡")
    report.append(f"- **æœ€å°å€¼**: {min_clar_stats['min']}")
    report.append(f"- **æœ€å¤§å€¼**: {min_clar_stats['max']}")
    report.append(f"- **å¹³å‡å€¼**: {min_clar_stats['avg']:.1f}")
    report.append("")

    # æ ¡éªŒé—®é¢˜
    if validation_report["errors"]:
        report.append("## æ ¡éªŒé—®é¢˜")
        report.append("")
        report.append("### é”™è¯¯")
        for error in validation_report["errors"][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            report.append(f"- **{error['sample_id']}.{error['field']}**: {error['message']}")
        if len(validation_report["errors"]) > 10:
            report.append(f"- ... è¿˜æœ‰ {len(validation_report['errors']) - 10} ä¸ªé”™è¯¯")
        report.append("")

    # æ³¨æ„ï¼šè¯¦ç»†çš„warningsä¿¡æ¯åœ¨loader.errorsä¸­ï¼Œéœ€è¦è¿›ä¸€æ­¥å¤„ç†

    return "\n".join(report)

def main():
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python tools/validate_dataset.py <æ•°æ®æ–‡ä»¶è·¯å¾„>")
        print("ç¤ºä¾‹: python tools/validate_dataset.py data/seed/ALC/seed.jsonl")
        sys.exit(1)

    file_path = sys.argv[1]
    output_path = "reports/data_overview.md"

    try:
        # åŠ è½½æ•°æ®
        loader = DataLoader(strict_mode=False)
        samples = list(loader.load_jsonl(file_path))

        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        stats = collect_statistics(samples)
        validation_report = loader.get_validation_report()

        # ç”ŸæˆæŠ¥å‘Š
        report = generate_markdown_report(stats, validation_report)

        # å†™å…¥æ–‡ä»¶
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)

        # æ§åˆ¶å°è¾“å‡ºæ‘˜è¦
        print("âœ… æ•°æ®é›†æ ¡éªŒå®Œæˆ")
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
        print(f"âŒ æ ¡éªŒé”™è¯¯: {validation_report['error_count']}")
        print(f"âš ï¸  æ ¡éªŒè­¦å‘Š: {validation_report['warning_count']}")
        print(f"ğŸ“ æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_path}")

        # å¦‚æœæœ‰ä¸¥é‡é”™è¯¯ï¼Œé€€å‡ºç éé›¶
        if validation_report["error_count"] > 0:
            print("\nâš ï¸  å‘ç°æ ¡éªŒé”™è¯¯ï¼Œè¯·æ£€æŸ¥æ•°æ®è´¨é‡")
            sys.exit(1)

    except Exception as e:
        print(f"âŒ æ ¡éªŒå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
