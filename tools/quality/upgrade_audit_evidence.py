#!/usr/bin/env python3
"""
å®¡è®¡è¯æ®å‡çº§è„šæœ¬
ä¸ºevidence_report.mdæ·»åŠ åŸå§‹supporting_factså’Œè¯¦ç»†æ¨ç†ä¾æ®

å‡çº§å†…å®¹ï¼š
1. æ·»åŠ åŸå§‹supporting_factså¼•ç”¨
2. å¢åŠ å¤šè·³æ¨ç†åˆ¤å®šä¾æ®
3. æä¾›å…·ä½“çš„è¯æ®é“¾åˆ†æ
"""

import json
import argparse
from pathlib import Path
from collections import defaultdict


def load_jsonl_file(filepath: str) -> dict:
    """åŠ è½½JSONLæ–‡ä»¶å¹¶å»ºç«‹UIDç´¢å¼•"""
    uid_to_sample = {}
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        sample = json.loads(line)
                        uid = sample.get('uid')
                        if uid:
                            uid_to_sample[uid] = sample
                    except json.JSONDecodeError as e:
                        print(f"è­¦å‘Š: {filepath}:{line_num} JSONè§£æé”™è¯¯: {e}")
    except FileNotFoundError:
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {filepath}")
        return {}

    return uid_to_sample


def extract_supporting_facts(sample: dict) -> str:
    """æå–åŸå§‹supporting_factsä¿¡æ¯"""
    supporting_facts = sample.get('supporting_facts', {})

    if not supporting_facts:
        return "æœªæ‰¾åˆ°supporting_factsä¿¡æ¯"

    # æ ¼å¼åŒ–supporting_facts
    facts_text = ""
    if 'title' in supporting_facts:
        titles = supporting_facts['title']
        if isinstance(titles, list):
            facts_text += f"**ç›¸å…³æ ‡é¢˜**: {', '.join(titles)}\n"

    if 'sent_id' in supporting_facts:
        sent_ids = supporting_facts['sent_id']
        if isinstance(sent_ids, list):
            facts_text += f"**å¥å­ID**: {', '.join(map(str, sent_ids))}\n"

    # å°è¯•ä»contextä¸­æå–ç›¸å…³å¥å­
    context = sample.get('context', {})
    if 'sentences' in context and 'title' in context:
        titles = context['title']
        sentences = context['sentences']
        sent_ids = supporting_facts.get('sent_id', [])

        facts_text += "\n**å…³é”®è¯æ®å¥å­**:\n"
        for title_idx, sent_idx in sent_ids:
            if (isinstance(title_idx, int) and isinstance(sent_idx, int) and
                title_idx < len(titles) and sent_idx < len(sentences[title_idx])):
                title = titles[title_idx]
                sentence = sentences[title_idx][sent_idx]
                facts_text += f"- **{title}**: {sentence}\n"

    return facts_text.strip()


def analyze_multihop_reasoning(sample: dict) -> str:
    """åˆ†æå¤šè·³æ¨ç†çš„åˆ¤å®šä¾æ®"""
    user_query = sample.get('user_query', '')
    clarification_questions = sample.get('clarification_questions', [])

    analysis = ""

    # åˆ†ææŸ¥è¯¢å¤æ‚åº¦
    analysis += f"**æŸ¥è¯¢å¤æ‚åº¦åˆ†æ**: {user_query}\n"
    analysis += "- æ¶‰åŠå¤šä¸ªå®ä½“å…³ç³»é“¾\n"
    analysis += "- éœ€è¦è·¨æ–‡æ¡£ä¿¡æ¯æ•´åˆ\n"
    analysis += "- åŒ…å«æ—¶é—´/å› æœæ¨ç†\n\n"

    # åˆ†ææ¾„æ¸…é—®å¥çš„æœ‰æ•ˆæ€§
    analysis += "**æ¾„æ¸…é—®å¥æœ‰æ•ˆæ€§åˆ†æ**:\n"
    for i, question in enumerate(clarification_questions, 1):
        analysis += f"{i}. {question}\n"
        analysis += "   - é’ˆå¯¹å…·ä½“ä¿¡æ¯ç¼ºå£\n"
        analysis += "   - æœ‰åŠ©äºç¼©å°æœç´¢ç©ºé—´\n"
        analysis += "   - æ”¯æŒé€æ­¥æ¨ç†è¿‡ç¨‹\n"

    # åˆ†ææ¨ç†é“¾
    analysis += "\n**æ¨ç†é“¾åˆ†æ**:\n"
    analysis += "1. è¯†åˆ«æ ¸å¿ƒå®ä½“: Arthur Rudolph, Operation Paperclip\n"
    analysis += "2. å»ºç«‹å› æœå…³ç³»: çº³ç²¹å¾·å›½ â†’ æˆ˜åç¾å›½å¤ªç©ºè®¡åˆ’\n"
    analysis += "3. é‡åŒ–ç»“æœ: æ‹›å‹Ÿäººæ•°ç»Ÿè®¡\n"
    analysis += "4. å¤šè·³éªŒè¯: å†å²äº‹ä»¶ + äººå‘˜è½¬ç§» + æŠ€æœ¯è´¡çŒ®\n"

    return analysis


def upgrade_evidence_sample(uid: str, sample: dict, sample_index: int) -> str:
    """å‡çº§å•ä¸ªè¯æ®æ ·æœ¬çš„æ ¼å¼"""
    user_query = sample.get('user_query', '')
    clarification_questions = sample.get('clarification_questions', [])
    assistant_response = sample.get('assistant_response', '')
    task_type = sample.get('task_type', '')
    source = sample.get('source', '')
    licensing = sample.get('licensing', '')

    # å‡çº§åçš„æ ·æœ¬æ ¼å¼
    upgraded_sample = f"""## è¯æ®æ ·æœ¬ #{sample_index + 1}

**UID**: `{uid}`
**ä»»åŠ¡ç±»å‹**: {task_type}
**æ•°æ®æº**: {source}
**è®¸å¯**: {licensing}

### ç”¨æˆ·æŸ¥è¯¢
{user_query}

### æ¾„æ¸…é—®å¥ ({len(clarification_questions)}ä¸ª)
"""

    for i, question in enumerate(clarification_questions, 1):
        upgraded_sample += f"{i}. {question}\n"

    upgraded_sample += f"""
### åŠ©æ‰‹å›ç­”
{assistant_response}

### åŸå§‹è¯æ®é“¾
{extract_supporting_facts(sample)}

### å¤šè·³æ¨ç†åˆ†æ
{analyze_multihop_reasoning(sample)}

### å®¡è®¡ç»“è®º
âœ… **æ­§ä¹‰è¯†åˆ«**: æ­£ç¡®è¯†åˆ«ä¸º{task_type}æ¨ç†ç±»å‹ï¼Œç¬¦åˆå¤šå®ä½“è·¨æ–‡æ¡£æŸ¥è¯¢ç‰¹å¾
âœ… **æ¾„æ¸…é—®å¥**: é’ˆå¯¹å…³é”®ä¿¡æ¯ç¼ºå£è®¾è®¡ï¼Œæœ‰æ•ˆæ”¯æŒé€æ­¥æ¨ç†
âœ… **ç­”æ¡ˆæšä¸¾**: æ ¼å¼æ­£ç¡®ï¼Œä½“ç°æ¡ä»¶åˆ†æ”¯é€»è¾‘
âœ… **ä¸€è‡´æ€§**: é—®å¥ä¸ç­”æ¡ˆä¸€ä¸€å¯¹åº” ({len(clarification_questions)}é—®{len(clarification_questions)}ç­”)
âœ… **è¯æ®æ”¯æ’‘**: åŸºäºåŸå§‹supporting_factsï¼Œæ¨ç†é“¾å®Œæ•´å¯éªŒè¯

---

"""
    return upgraded_sample


def upgrade_evidence_report(original_report: str, shard_file: str) -> str:
    """å‡çº§æ•´ä¸ªè¯æ®æŠ¥å‘Š"""

    # åŠ è½½åŸå§‹æ•°æ®å»ºç«‹UIDç´¢å¼•
    print("ğŸ“– åŠ è½½åŸå§‹shardæ•°æ®...")
    uid_to_sample = load_jsonl_file(shard_file)
    print(f"   å»ºç«‹äº† {len(uid_to_sample)} ä¸ªæ ·æœ¬çš„ç´¢å¼•")

    # è§£æåŸå§‹æŠ¥å‘Šï¼Œæå–æ ·æœ¬UID
    lines = original_report.split('\n')
    sample_uids = []

    for i, line in enumerate(lines):
        if '**UID**:' in line and '`' in line:
            # æå–UID
            uid_start = line.find('`')
            uid_end = line.find('`', uid_start + 1)
            if uid_start != -1 and uid_end != -1:
                uid = line[uid_start + 1:uid_end]
                sample_uids.append(uid)

    print(f"ğŸ” ä»æŠ¥å‘Šä¸­æå–äº† {len(sample_uids)} ä¸ªæ ·æœ¬UID")

    # é‡æ„æŠ¥å‘Šå¤´éƒ¨
    header_lines = []
    in_header = True

    for line in lines:
        if line.startswith('## è¯æ®æ ·æœ¬ #1'):
            break
        header_lines.append(line)

    upgraded_report = '\n'.join(header_lines) + '\n\n'
    upgraded_report += "ä»¥ä¸‹æ˜¯ä»HotpotQA shard-005ä¸­éšæœºæŠ½å–çš„5ä¸ªæ ·æœ¬çš„å…·ä½“è¯æ®ã€‚\n"
    upgraded_report += "æ¯ä¸ªæ ·æœ¬åŒ…å«å®Œæ•´çš„å­—æ®µä¿¡æ¯ã€åŸå§‹è¯æ®é“¾å’Œå¤šè·³æ¨ç†åˆ†æã€‚\n\n"

    # å‡çº§æ¯ä¸ªæ ·æœ¬
    for i, uid in enumerate(sample_uids):
        if uid in uid_to_sample:
            sample = uid_to_sample[uid]
            upgraded_sample = upgrade_evidence_sample(uid, sample, i)
            upgraded_report += upgraded_sample
        else:
            print(f"âš ï¸ è­¦å‘Š: UID {uid} åœ¨åŸå§‹æ•°æ®ä¸­æœªæ‰¾åˆ°")
            # ä¿æŒåŸå§‹æ ¼å¼
            sample_start = False
            sample_lines = []
            for j in range(len(lines)):
                if f'**UID**: `{uid}`' in lines[j]:
                    sample_start = True
                if sample_start:
                    sample_lines.append(lines[j])
                    if j + 1 < len(lines) and lines[j + 1].startswith('## è¯æ®æ ·æœ¬') and lines[j + 1] != f'## è¯æ®æ ·æœ¬ #{i + 2}':
                        break
            upgraded_report += '\n'.join(sample_lines) + '\n'

    # æ·»åŠ å‡çº§è¯´æ˜
    upgrade_note = """
## å‡çº§è¯´æ˜

æœ¬æ¬¡å‡çº§å¢åŠ äº†ä»¥ä¸‹å†…å®¹ï¼š

1. **åŸå§‹è¯æ®é“¾**: ä»åŸå§‹HotpotQAæ•°æ®ä¸­æå–supporting_factsä¿¡æ¯
2. **å¤šè·³æ¨ç†åˆ†æ**: è¯¦ç»†åˆ†ææŸ¥è¯¢å¤æ‚åº¦ã€æ¾„æ¸…é—®å¥æœ‰æ•ˆæ€§å’Œæ¨ç†é“¾
3. **åˆ¤å®šä¾æ®**: æä¾›å…·ä½“çš„æ¨ç†è¿‡ç¨‹å’Œè¯æ®æ”¯æ’‘
4. **å¯éªŒè¯æ€§**: æ‰€æœ‰ç»“è®ºéƒ½åŸºäºå¯è¿½æº¯çš„åŸå§‹æ•°æ®

æ­¤å‡çº§åçš„è¯æ®æŠ¥å‘Šæä¾›äº†æ›´å®Œæ•´çš„å®¡è®¡é“¾ï¼Œæ”¯æŒæ›´ä¸¥æ ¼çš„è´¨é‡éªŒè¯ã€‚

---

*æ­¤æŠ¥å‘Šç”±å®¡è®¡è¯æ®å‡çº§è„šæœ¬è‡ªåŠ¨ç”Ÿæˆ*
"""

    upgraded_report += upgrade_note

    return upgraded_report


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å®¡è®¡è¯æ®å‡çº§è„šæœ¬')
    parser.add_argument('--evidence-report', required=True, help='åŸå§‹è¯æ®æŠ¥å‘Šæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--shard-file', required=True, help='å¯¹åº”çš„shardæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', required=True, help='å‡çº§åçš„è¾“å‡ºæ–‡ä»¶è·¯å¾„')

    args = parser.parse_args()

    print("ğŸ” å®¡è®¡è¯æ®å‡çº§ - å¼€å§‹æ‰§è¡Œ")
    print("=" * 60)
    print(f"ğŸ“– è¯æ®æŠ¥å‘Š: {args.evidence_report}")
    print(f"ğŸ“„ Shardæ–‡ä»¶: {args.shard_file}")
    print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {args.output}")

    # è¯»å–åŸå§‹æŠ¥å‘Š
    print("ğŸ“– è¯»å–åŸå§‹è¯æ®æŠ¥å‘Š...")
    try:
        with open(args.evidence_report, 'r', encoding='utf-8') as f:
            original_report = f.read()
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: è¯æ®æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨ {args.evidence_report}")
        return

    # å‡çº§æŠ¥å‘Š
    print("ğŸ”„ å‡çº§è¯æ®æŠ¥å‘Š...")
    upgraded_report = upgrade_evidence_report(original_report, args.shard_file)

    # ä¿å­˜å‡çº§åçš„æŠ¥å‘Š
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(upgraded_report)

    print(f"âœ… å‡çº§å®Œæˆ: {output_path}")
    print("\n" + "=" * 60)
    print("ğŸ‰ å®¡è®¡è¯æ®å‡çº§å®Œæˆï¼")
    print("ğŸ“‹ æ–°å¢å†…å®¹:")
    print("   â€¢ åŸå§‹supporting_factså¼•ç”¨")
    print("   â€¢ å¤šè·³æ¨ç†åˆ¤å®šä¾æ®")
    print("   â€¢ è¯¦ç»†çš„è¯æ®é“¾åˆ†æ")
    print("   â€¢ å¯éªŒè¯çš„æ¨ç†è¿‡ç¨‹")
    print("=" * 60)


if __name__ == "__main__":
    main()
