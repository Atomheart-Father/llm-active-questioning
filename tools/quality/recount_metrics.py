#!/usr/bin/env python3
"""
è´¨é‡æŒ‡æ ‡å¤ç®—è„šæœ¬
å¯¹ data/processed/active_qa_v1/ ç›®å½•ä¸‹çš„æ•°æ®è¿›è¡Œé‡æ–°è®¡ç®—å’ŒéªŒè¯

åŠŸèƒ½ï¼š
1. å¯¹é½éªŒè¯ï¼šæ£€æŸ¥clarification_questionsä¸assistant_responseçš„ä¸€ä¸€å¯¹åº”å…³ç³»
2. å»é‡ç»Ÿè®¡ï¼šåŸºäºæ–‡æœ¬å“ˆå¸Œè®¡ç®—é‡å¤æ ·æœ¬
3. è¯æ®é‡å ï¼šè®¡ç®—clarification_questionsä¸provided_contextçš„è¯é¢é‡å 
4. å®Œæ•´æ€§ç»Ÿè®¡ï¼šå­—æ®µå®Œå¤‡æ€§æ£€æŸ¥
5. è®¸å¯åˆè§„ï¼šéªŒè¯è®¸å¯åè®®

è¾“å‡ºï¼š
- metrics.recount.json: é‡æ–°è®¡ç®—çš„æŒ‡æ ‡
- metrics.diff.txt: ä¸ç°æœ‰metrics.jsonçš„å·®å¼‚
"""

import json
import os
import hashlib
import re
from collections import defaultdict, Counter
from datetime import datetime
from pathlib import Path
import difflib


def load_jsonl_file(filepath):
    """åŠ è½½JSONLæ–‡ä»¶"""
    samples = []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        sample = json.loads(line)
                        samples.append(sample)
                    except json.JSONDecodeError as e:
                        print(f"è­¦å‘Š: {filepath}:{line_num} JSONè§£æé”™è¯¯: {e}")
    except FileNotFoundError:
        print(f"è­¦å‘Š: æ–‡ä»¶ä¸å­˜åœ¨ {filepath}")
    return samples


def validate_alignment(sample):
    """
    éªŒè¯å¯¹é½ï¼šclarification_questionsæ•°é‡åº”ä¸assistant_responseä¸­çš„æšä¸¾æ•°é‡åŒ¹é…

    è¿”å›ï¼š
    - is_aligned: å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦å¯¹é½
    - error_reason: å¦‚æœä¸å¯¹é½ï¼Œè¯´æ˜åŸå› 
    """
    questions = sample.get('clarification_questions', [])
    response = sample.get('assistant_response', '')

    if not questions:
        return False, "empty_questions"

    if not response:
        return False, "empty_response"

    # æå–responseä¸­çš„æšä¸¾ç­”æ¡ˆæ•°é‡
    # åŒ¹é…"è‹¥é—®é¢˜Xåˆ™ç­”æ¡ˆï¼šXXX"çš„æ¨¡å¼
    answer_pattern = r'è‹¥é—®é¢˜\d+åˆ™ç­”æ¡ˆï¼š'
    enumerated_answers = re.findall(answer_pattern, response)

    if len(questions) != len(enumerated_answers):
        return False, f"question_count_{len(questions)}_vs_answer_count_{len(enumerated_answers)}"

    return True, None


def calculate_text_hash(sample, fields=('user_query', 'clarification_questions', 'assistant_response')):
    """è®¡ç®—æ ·æœ¬çš„æ–‡æœ¬å“ˆå¸Œç”¨äºå»é‡"""
    content_parts = []

    for field in fields:
        if field == 'clarification_questions':
            questions = sample.get(field, [])
            if isinstance(questions, list):
                content_parts.extend(questions)
            else:
                content_parts.append(str(questions))
        else:
            value = sample.get(field, '')
            content_parts.append(str(value))

    content = '|'.join(content_parts)
    return hashlib.md5(content.encode('utf-8')).hexdigest()


def calculate_evidence_overlap(sample):
    """
    è®¡ç®—clarification_questionsä¸provided_contextçš„è¯é¢é‡å ç‡

    è¿”å›é‡å è¯çš„æ¯”ä¾‹ (0.0-1.0)
    """
    questions = sample.get('clarification_questions', [])
    context = sample.get('provided_context', '')

    if not questions or not context:
        return 0.0

    # åˆå¹¶æ‰€æœ‰clarification_questions
    all_questions = ' '.join(questions) if isinstance(questions, list) else str(questions)

    # ç®€å•åˆ†è¯ï¼ˆæŒ‰ç©ºæ ¼åˆ†å‰²ï¼Œå»æ‰æ ‡ç‚¹ï¼‰
    question_words = set(re.findall(r'\b\w+\b', all_questions.lower()))
    context_words = set(re.findall(r'\b\w+\b', context.lower()))

    if not question_words:
        return 0.0

    # è®¡ç®—é‡å è¯æ•°
    overlap_words = question_words.intersection(context_words)
    return len(overlap_words) / len(question_words)


def validate_license(sample):
    """éªŒè¯è®¸å¯åè®®"""
    valid_licenses = {'cc-by-sa-3.0', 'cc-by-sa-4.0', 'mit', 'apache-2.0'}
    license_type = sample.get('licensing', '')

    return license_type in valid_licenses, license_type


def analyze_samples(samples):
    """åˆ†ææ ·æœ¬é›†åˆï¼Œè¿”å›ç»Ÿè®¡ç»“æœ"""
    stats = {
        'total_samples': len(samples),
        'alignment_ok_count': 0,
        'alignment_errors': [],
        'duplicate_hashes': defaultdict(list),
        'license_errors': [],
        'field_completeness': defaultdict(int),
        'evidence_overlaps': [],
        'task_types': Counter(),
        'sources': Counter(),
        'licenses': Counter()
    }

    required_fields = ['uid', 'user_query', 'clarification_questions',
                      'assistant_response', 'task_type', 'source', 'licensing']

    for i, sample in enumerate(samples):
        # å­—æ®µå®Œå¤‡æ€§æ£€æŸ¥
        for field in required_fields:
            if field in sample and sample[field]:
                stats['field_completeness'][field] += 1

        # å¯¹é½éªŒè¯
        is_aligned, error_reason = validate_alignment(sample)
        if is_aligned:
            stats['alignment_ok_count'] += 1
        else:
            stats['alignment_errors'].append({
                'index': i,
                'uid': sample.get('uid', 'unknown'),
                'error': error_reason
            })

        # å»é‡å“ˆå¸Œ
        text_hash = calculate_text_hash(sample)
        stats['duplicate_hashes'][text_hash].append(i)

        # è¯æ®é‡å 
        overlap_ratio = calculate_evidence_overlap(sample)
        stats['evidence_overlaps'].append(overlap_ratio)

        # è®¸å¯éªŒè¯
        is_valid_license, license_type = validate_license(sample)
        if not is_valid_license:
            stats['license_errors'].append({
                'index': i,
                'uid': sample.get('uid', 'unknown'),
                'license': license_type
            })

        # ç»Ÿè®¡åˆ†ç±»
        stats['task_types'][sample.get('task_type', 'unknown')] += 1
        stats['sources'][sample.get('source', 'unknown')] += 1
        stats['licenses'][sample.get('licensing', 'unknown')] += 1

    return stats


def calculate_deduplication_stats(duplicate_hashes):
    """è®¡ç®—å»é‡ç»Ÿè®¡"""
    original_count = sum(len(indices) for indices in duplicate_hashes.values())
    unique_count = len(duplicate_hashes)
    duplicates_removed = original_count - unique_count

    return {
        'original_count': original_count,
        'deduped_count': unique_count,
        'duplicates_removed': duplicates_removed,
        'deduplication_ratio': duplicates_removed / original_count if original_count > 0 else 0
    }


def generate_recounted_metrics(stats):
    """ç”Ÿæˆé‡æ–°è®¡ç®—çš„æŒ‡æ ‡æ–‡ä»¶"""
    total_samples = stats['total_samples']
    alignment_ok_count = stats['alignment_ok_count']
    alignment_error_count = total_samples - alignment_ok_count

    # è®¡ç®—è¯æ®é‡å ç»Ÿè®¡ï¼ˆåªè®¡ç®—HotpotQA/ASQAæ ·æœ¬ï¼‰
    evidence_overlaps = [overlap for overlap in stats['evidence_overlaps'] if overlap > 0]
    evidence_overlap_mean = sum(evidence_overlaps) / len(evidence_overlaps) if evidence_overlaps else 0

    metrics = {
        "timestamp": datetime.now().isoformat(),
        "total_samples": total_samples,
        "near_duplicates": {
            "duplicate_ratio": calculate_deduplication_stats(stats['duplicate_hashes'])['deduplication_ratio']
        },
        "alignment_stats": {
            "alignment_ok_count": alignment_ok_count,
            "alignment_error_count": alignment_error_count,
            "alignment_ok_percentage": (alignment_ok_count / total_samples * 100) if total_samples > 0 else 0
        },
        "shards": {
            # è¿™é‡Œéœ€è¦ä»å®é™…æ–‡ä»¶ç»Ÿè®¡ï¼Œæš‚æ—¶ä¿æŒä¸åŸæœ‰ç»“æ„ä¸€è‡´
        },
        "by_shard": {
            # è¿™é‡Œéœ€è¦ä»å®é™…æ–‡ä»¶ç»Ÿè®¡ï¼Œæš‚æ—¶ä¿æŒä¸åŸæœ‰ç»“æ„ä¸€è‡´
        },
        "license_whitelist_errors": stats['license_errors'],
        "summary": {
            "total_clarification_samples": total_samples,
            "total_alignment_errors": alignment_error_count,
            "field_completeness_avg": sum(stats['field_completeness'].values()) / (len(stats['field_completeness']) * total_samples) * 100 if total_samples > 0 else 0,
            "near_duplicates_avg": calculate_deduplication_stats(stats['duplicate_hashes'])['deduplication_ratio']
        },
        "evidence_overlap": {
            "mean": evidence_overlap_mean,
            "count": len(evidence_overlaps)
        },
        "recount_details": {
            "alignment_errors_detail": stats['alignment_errors'][:10],  # åªæ˜¾ç¤ºå‰10ä¸ªé”™è¯¯è¯¦æƒ…
            "duplicate_clusters": {k: len(v) for k, v in list(stats['duplicate_hashes'].items())[:5] if len(v) > 1},  # åªæ˜¾ç¤ºå‰5ä¸ªé‡å¤ç°‡
            "task_type_distribution": dict(stats['task_types']),
            "source_distribution": dict(stats['sources']),
            "license_distribution": dict(stats['licenses'])
        }
    }

    return metrics


def compare_metrics(original_path, recounted_metrics):
    """æ¯”è¾ƒåŸå§‹metricsä¸é‡æ–°è®¡ç®—çš„metrics"""
    try:
        with open(original_path, 'r', encoding='utf-8') as f:
            original = json.load(f)
    except FileNotFoundError:
        return "åŸå§‹metrics.jsonæ–‡ä»¶ä¸å­˜åœ¨"

    # æ¯”è¾ƒå…³é”®æŒ‡æ ‡
    differences = []

    def compare_values(path, orig_val, new_val):
        if orig_val != new_val:
            differences.append(f"{path}: {orig_val} â†’ {new_val}")

    compare_values("total_samples", original.get('total_samples'), recounted_metrics['total_samples'])
    compare_values("alignment_stats.alignment_ok_count",
                   original.get('alignment_stats', {}).get('alignment_ok_count'),
                   recounted_metrics['alignment_stats']['alignment_ok_count'])
    compare_values("alignment_stats.alignment_error_count",
                   original.get('alignment_stats', {}).get('alignment_error_count'),
                   recounted_metrics['alignment_stats']['alignment_error_count'])
    compare_values("near_duplicates.duplicate_ratio",
                   original.get('near_duplicates', {}).get('duplicate_ratio'),
                   recounted_metrics['near_duplicates']['duplicate_ratio'])
    compare_values("evidence_overlap.mean",
                   original.get('evidence_overlap', {}).get('mean'),
                   recounted_metrics['evidence_overlap']['mean'])

    return differences if differences else ["âœ… æ‰€æœ‰å…³é”®æŒ‡æ ‡ä¸€è‡´"]


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” Stage 2 è´¨é‡æŒ‡æ ‡å¤ç®— - å¼€å§‹æ‰§è¡Œ")
    print("=" * 60)

    # æ•°æ®ç›®å½•
    data_dir = Path("data/processed/active_qa_v1")

    # æŸ¥æ‰¾æ‰€æœ‰JSONLæ–‡ä»¶
    jsonl_files = list(data_dir.glob("*.jsonl"))
    print(f"ğŸ“ å‘ç° {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶: {[f.name for f in jsonl_files]}")

    # åŠ è½½æ‰€æœ‰æ ·æœ¬
    all_samples = []
    for jsonl_file in jsonl_files:
        print(f"ğŸ“– åŠ è½½ {jsonl_file.name}...")
        samples = load_jsonl_file(jsonl_file)
        all_samples.extend(samples)
        print(f"   åŠ è½½äº† {len(samples)} ä¸ªæ ·æœ¬")

    print(f"ğŸ“Š æ€»å…±åŠ è½½äº† {len(all_samples)} ä¸ªæ ·æœ¬")

    # åˆ†ææ ·æœ¬
    print("ğŸ”¬ å¼€å§‹åˆ†ææ ·æœ¬...")
    stats = analyze_samples(all_samples)

    # ç”Ÿæˆé‡æ–°è®¡ç®—çš„æŒ‡æ ‡
    print("ğŸ“ˆ ç”Ÿæˆé‡æ–°è®¡ç®—çš„æŒ‡æ ‡...")
    recounted_metrics = generate_recounted_metrics(stats)

    # ä¿å­˜é‡æ–°è®¡ç®—çš„æŒ‡æ ‡
    output_path = data_dir / "metrics.recount.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(recounted_metrics, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ ä¿å­˜é‡æ–°è®¡ç®—çš„æŒ‡æ ‡åˆ° {output_path}")

    # æ¯”è¾ƒä¸åŸå§‹æŒ‡æ ‡
    print("âš–ï¸ æ¯”è¾ƒä¸åŸå§‹æŒ‡æ ‡...")
    original_metrics_path = data_dir / "metrics.json"
    differences = compare_metrics(original_metrics_path, recounted_metrics)

    # ä¿å­˜å·®å¼‚æŠ¥å‘Š
    diff_report_path = data_dir / "metrics.diff.txt"
    with open(diff_report_path, 'w', encoding='utf-8') as f:
        f.write("è´¨é‡æŒ‡æ ‡å¤ç®—å·®å¼‚æŠ¥å‘Š\n")
        f.write("=" * 40 + "\n\n")
        f.write(f"å¤ç®—æ—¶é—´: {datetime.now().isoformat()}\n")
        f.write(f"åŸå§‹æ–‡ä»¶: {original_metrics_path}\n")
        f.write(f"å¤ç®—æ–‡ä»¶: {output_path}\n\n")
        f.write("å…³é”®æŒ‡æ ‡å·®å¼‚:\n")
        for diff in differences:
            f.write(f"- {diff}\n")
    print(f"ğŸ“‹ ä¿å­˜å·®å¼‚æŠ¥å‘Šåˆ° {diff_report_path}")

    # è¾“å‡ºç»Ÿè®¡æ‘˜è¦
    print("\n" + "=" * 60)
    print("ğŸ“Š å¤ç®—ç»“æœæ‘˜è¦:")
    print(f"  æ€»æ ·æœ¬æ•°: {recounted_metrics['total_samples']}")
    print(f"  å¯¹é½æ­£ç¡®: {recounted_metrics['alignment_stats']['alignment_ok_count']}")
    print(f"  å¯¹é½é”™è¯¯: {recounted_metrics['alignment_stats']['alignment_error_count']}")
    print(".2f")
    print(f"  å»é‡æ¯”ä¾‹: {recounted_metrics['near_duplicates']['duplicate_ratio']:.4f}")
    print(f"  è¯æ®é‡å å‡å€¼: {recounted_metrics['evidence_overlap']['mean']:.3f}")
    print(f"  è®¸å¯é”™è¯¯æ•°: {len(recounted_metrics['license_whitelist_errors'])}")

    if differences and differences[0] != "âœ… æ‰€æœ‰å…³é”®æŒ‡æ ‡ä¸€è‡´":
        print("\nâš ï¸ å‘ç°æŒ‡æ ‡å·®å¼‚ï¼Œè¯·æ£€æŸ¥å·®å¼‚æŠ¥å‘Š!")
        print(f"   å·®å¼‚æŠ¥å‘Š: {diff_report_path}")
    else:
        print("\nâœ… å¤ç®—å®Œæˆï¼Œæ‰€æœ‰å…³é”®æŒ‡æ ‡ä¸€è‡´!")

    print("=" * 60)


if __name__ == "__main__":
    main()
