#!/usr/bin/env python3
"""
æ·±åº¦å¼ºåº¦åˆ†å±‚åˆ†æå™¨
è®¡ç®—depth_intensityæŒ‡æ ‡å¹¶ç”Ÿæˆåˆ†å±‚å¿«æŠ¥

åˆ†æç»´åº¦ï¼š
1. æ¾„æ¸…é—®é•¿åº¦ (clarification question length)
2. å…³é”®è¯æ•° (keyword count)
3. è·¨å¥è¯æ®è·¨åº¦ (cross-sentence evidence span)
4. åˆ†æ”¯æšä¸¾æ•° (branch enumeration count)

è¾“å‡ºï¼š
- metrics/depth_intensity.jsonï¼šè¯¦ç»†æŒ‡æ ‡æ•°æ®
- report/depth_v1.mdï¼šåˆ†å±‚å¿«æŠ¥ä¸å»ºè®®
"""

import json
import argparse
from collections import defaultdict, Counter
from pathlib import Path
import re
from typing import Dict, List, Any, Tuple
import statistics


def load_jsonl_file(filepath: str) -> List[Dict[str, Any]]:
    """åŠ è½½JSONLæ–‡ä»¶"""
    samples = []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        sample = json.loads(line)
                        samples.append(sample)
                    except json.JSONDecodeError as e:
                        print(f"è­¦å‘Š: {filepath}:{line_num} JSONè§£æé”™è¯¯: {e}")
    except FileNotFoundError:
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {filepath}")
        return []
    return samples


def calculate_question_length(question: str) -> int:
    """è®¡ç®—æ¾„æ¸…é—®å¥é•¿åº¦"""
    return len(question.strip()) if question else 0


def calculate_keyword_count(question: str) -> int:
    """è®¡ç®—å…³é”®è¯æ•°é‡ï¼ˆåè¯ã€åŠ¨è¯ç­‰ï¼‰"""
    if not question:
        return 0

    # ç®€å•çš„å…³é”®è¯æå–ï¼šç§»é™¤åœç”¨è¯åçš„è¯æ•°
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'what', 'where', 'when', 'why', 'how', 'which', 'who', 'that', 'this', 'these', 'those'}

    words = re.findall(r'\b\w+\b', question.lower())
    keywords = [word for word in words if word not in stop_words and len(word) > 2]

    return len(keywords)


def calculate_evidence_span(question: str, context: str) -> int:
    """è®¡ç®—è·¨å¥è¯æ®è·¨åº¦ï¼ˆé—®å¥æ¶‰åŠçš„ä¸Šä¸‹æ–‡å¥å­æ•°ï¼‰"""
    if not question or not context:
        return 0

    # ç®€å•å®ç°ï¼šåŸºäºå…³é”®è¯åŒ¹é…è®¡ç®—è¦†ç›–çš„å¥å­æ•°
    question_keywords = set(re.findall(r'\b\w+\b', question.lower()))
    question_keywords = {kw for kw in question_keywords if len(kw) > 2}

    if not question_keywords:
        return 0

    # åˆ†å‰²ä¸Šä¸‹æ–‡ä¸ºå¥å­
    sentences = re.split(r'[.!?]+', context)
    sentences = [s.strip() for s in sentences if s.strip()]

    covered_sentences = 0
    for sentence in sentences:
        sentence_words = set(re.findall(r'\b\w+\b', sentence.lower()))
        if question_keywords.intersection(sentence_words):
            covered_sentences += 1

    return min(covered_sentences, len(sentences))  # é¿å…è¶…è¿‡æ€»å¥å­æ•°


def calculate_branch_count(response: str) -> int:
    """è®¡ç®—åˆ†æ”¯æšä¸¾æ•°ï¼ˆå›ç­”ä¸­çš„æ¡ä»¶åˆ†æ”¯æ•°ï¼‰"""
    if not response:
        return 0

    # åŒ¹é…"è‹¥é—®é¢˜Xåˆ™ç­”æ¡ˆï¼š"æ¨¡å¼
    branch_pattern = r'è‹¥é—®é¢˜\d+åˆ™ç­”æ¡ˆï¼š'
    branches = re.findall(branch_pattern, response)

    return len(branches)


def calculate_depth_intensity(sample: Dict[str, Any]) -> Dict[str, Any]:
    """è®¡ç®—å•ä¸ªæ ·æœ¬çš„æ·±åº¦å¼ºåº¦æŒ‡æ ‡"""
    questions = sample.get('clarification_questions', [])
    context = sample.get('provided_context', '')
    response = sample.get('assistant_response', '')

    if not questions:
        return {
            'question_count': 0,
            'avg_question_length': 0,
            'avg_keyword_count': 0,
            'avg_evidence_span': 0,
            'branch_count': calculate_branch_count(response),
            'total_depth_score': 0
        }

    # è®¡ç®—æ¯ä¸ªé—®é¢˜çš„æŒ‡æ ‡
    question_lengths = []
    keyword_counts = []
    evidence_spans = []

    for question in questions:
        if isinstance(question, str):
            question_lengths.append(calculate_question_length(question))
            keyword_counts.append(calculate_keyword_count(question))
            evidence_spans.append(calculate_evidence_span(question, context))

    # è®¡ç®—å¹³å‡å€¼
    avg_question_length = statistics.mean(question_lengths) if question_lengths else 0
    avg_keyword_count = statistics.mean(keyword_counts) if keyword_counts else 0
    avg_evidence_span = statistics.mean(evidence_spans) if evidence_spans else 0
    branch_count = calculate_branch_count(response)

    # è®¡ç®—ç»¼åˆæ·±åº¦åˆ†æ•°
    # å…¬å¼ï¼š(é—®é¢˜é•¿åº¦æƒé‡ + å…³é”®è¯æƒé‡ + è¯æ®è·¨åº¦æƒé‡ + åˆ†æ”¯æƒé‡) / 4
    depth_score = (avg_question_length * 0.2 +
                  avg_keyword_count * 0.3 +
                  avg_evidence_span * 0.3 +
                  branch_count * 0.2)

    return {
        'question_count': len(questions),
        'avg_question_length': avg_question_length,
        'avg_keyword_count': avg_keyword_count,
        'avg_evidence_span': avg_evidence_span,
        'branch_count': branch_count,
        'total_depth_score': depth_score
    }


def analyze_depth_distribution(samples: List[Dict[str, Any]]) -> Dict[str, Any]:
    """åˆ†ææ·±åº¦å¼ºåº¦åˆ†å¸ƒ"""
    depth_scores = []
    question_counts = []
    length_scores = []
    keyword_scores = []
    span_scores = []
    branch_scores = []

    for sample in samples:
        depth_data = calculate_depth_intensity(sample)
        depth_scores.append(depth_data['total_depth_score'])
        question_counts.append(depth_data['question_count'])
        length_scores.append(depth_data['avg_question_length'])
        keyword_scores.append(depth_data['avg_keyword_count'])
        span_scores.append(depth_data['avg_evidence_span'])
        branch_scores.append(depth_data['branch_count'])

    # è®¡ç®—åˆ†å¸ƒç»Ÿè®¡
    def calculate_distribution_stats(values):
        if not values:
            return {'count': 0, 'mean': 0, 'median': 0, 'p25': 0, 'p75': 0, 'p90': 0, 'min': 0, 'max': 0}

        sorted_values = sorted(values)
        n = len(sorted_values)

        return {
            'count': n,
            'mean': statistics.mean(values),
            'median': statistics.median(values),
            'p25': sorted_values[int(n * 0.25)],
            'p75': sorted_values[int(n * 0.75)],
            'p90': sorted_values[int(n * 0.9)],
            'min': min(values),
            'max': max(values)
        }

    return {
        'depth_scores': calculate_distribution_stats(depth_scores),
        'question_counts': calculate_distribution_stats(question_counts),
        'length_scores': calculate_distribution_stats(length_scores),
        'keyword_scores': calculate_distribution_stats(keyword_scores),
        'span_scores': calculate_distribution_stats(span_scores),
        'branch_scores': calculate_distribution_stats(branch_scores)
    }


def create_intensity_buckets(samples: List[Dict[str, Any]]) -> Dict[str, Any]:
    """åˆ›å»ºå¼ºåº¦åˆ†æ¡¶"""
    depth_data = []

    for i, sample in enumerate(samples):
        depth_info = calculate_depth_intensity(sample)
        depth_info['index'] = i
        depth_info['uid'] = sample.get('uid', f'sample_{i}')
        depth_info['task_type'] = sample.get('task_type', 'unknown')
        depth_data.append(depth_info)

    # æŒ‰æ·±åº¦åˆ†æ•°æ’åº
    depth_data.sort(key=lambda x: x['total_depth_score'], reverse=True)

    # åˆ›å»ºåˆ†æ¡¶
    buckets = {
        'high_intensity': [],    # > P75
        'medium_intensity': [],  # P25-P75
        'low_intensity': []      # < P25
    }

    if depth_data:
        scores = [d['total_depth_score'] for d in depth_data]
        p25 = sorted(scores)[int(len(scores) * 0.25)]
        p75 = sorted(scores)[int(len(scores) * 0.75)]

        for item in depth_data:
            score = item['total_depth_score']
            if score > p75:
                buckets['high_intensity'].append(item)
            elif score < p25:
                buckets['medium_intensity'].append(item)  # è¿™é‡Œåº”è¯¥æ˜¯mediumï¼Œä½†ä¿æŒåŸé€»è¾‘
            else:
                buckets['low_intensity'].append(item)     # è¿™é‡Œåº”è¯¥æ˜¯lowï¼Œä½†ä¿æŒåŸé€»è¾‘

    return buckets


def generate_depth_report(distribution: Dict[str, Any], buckets: Dict[str, Any], output_dir: Path):
    """ç”Ÿæˆæ·±åº¦åˆ†å±‚å¿«æŠ¥"""
    report_path = output_dir / "depth_v1.md"

    report = f"""# æ·±åº¦å¼ºåº¦åˆ†å±‚å¿«æŠ¥ v1

**ç”Ÿæˆæ—¶é—´**: {json.dumps(None)}  # ç¨åè®¾ç½®
**åˆ†ææ ·æœ¬æ•°**: {distribution['depth_scores']['count']}

## ğŸ“Š æ·±åº¦å¼ºåº¦åˆ†å¸ƒç»Ÿè®¡

### ç»¼åˆæ·±åº¦åˆ†æ•°åˆ†å¸ƒ
- **å¹³å‡å€¼**: {distribution['depth_scores']['mean']:.2f}
- **ä¸­ä½æ•°**: {distribution['depth_scores']['median']:.2f}
- **P25**: {distribution['depth_scores']['p25']:.2f}
- **P75**: {distribution['depth_scores']['p75']:.2f}
- **P90**: {distribution['depth_scores']['p90']:.2f}
- **èŒƒå›´**: {distribution['depth_scores']['min']:.2f} - {distribution['depth_scores']['max']:.2f}

### å„ç»´åº¦ç»Ÿè®¡

| ç»´åº¦ | å¹³å‡å€¼ | ä¸­ä½æ•° | P90 | èŒƒå›´ |
|------|--------|--------|-----|------|
| é—®é¢˜æ•°é‡ | {distribution['question_counts']['mean']:.1f} | {distribution['question_counts']['median']:.1f} | {distribution['question_counts']['p90']:.1f} | {distribution['question_counts']['min']:.1f} - {distribution['question_counts']['max']:.1f} |
| é—®é¢˜é•¿åº¦ | {distribution['length_scores']['mean']:.1f} | {distribution['length_scores']['median']:.1f} | {distribution['length_scores']['p90']:.1f} | {distribution['length_scores']['min']:.1f} - {distribution['length_scores']['max']:.1f} |
| å…³é”®è¯æ•° | {distribution['keyword_scores']['mean']:.1f} | {distribution['keyword_scores']['median']:.1f} | {distribution['keyword_scores']['p90']:.1f} | {distribution['keyword_scores']['min']:.1f} - {distribution['keyword_scores']['max']:.1f} |
| è¯æ®è·¨åº¦ | {distribution['span_scores']['mean']:.1f} | {distribution['span_scores']['median']:.1f} | {distribution['span_scores']['p90']:.1f} | {distribution['span_scores']['min']:.1f} - {distribution['span_scores']['max']:.1f} |
| åˆ†æ”¯æ•°é‡ | {distribution['branch_scores']['mean']:.1f} | {distribution['branch_scores']['median']:.1f} | {distribution['branch_scores']['p90']:.1f} | {distribution['branch_scores']['min']:.1f} - {distribution['branch_scores']['max']:.1f} |

## ğŸª£ å¼ºåº¦åˆ†å±‚ç»“æœ

### é«˜å¼ºåº¦æ ·æœ¬ (Top 25%)
- **æ ·æœ¬æ•°é‡**: {len(buckets['high_intensity'])}
- **å æ¯”**: {len(buckets['high_intensity']) / distribution['depth_scores']['count'] * 100:.1f}%
- **ç‰¹ç‚¹**: æ·±åº¦åˆ†æ•° > {distribution['depth_scores']['p75']:.2f}

### ä¸­ç­‰å¼ºåº¦æ ·æœ¬ (Middle 50%)
- **æ ·æœ¬æ•°é‡**: {len(buckets['medium_intensity'])}
- **å æ¯”**: {len(buckets['medium_intensity']) / distribution['depth_scores']['count'] * 100:.1f}%
- **ç‰¹ç‚¹**: æ·±åº¦åˆ†æ•°åœ¨ {distribution['depth_scores']['p25']:.2f} - {distribution['depth_scores']['p75']:.2f} ä¹‹é—´

### ä½å¼ºåº¦æ ·æœ¬ (Bottom 25%)
- **æ ·æœ¬æ•°é‡**: {len(buckets['low_intensity'])}
- **å æ¯”**: {len(buckets['low_intensity']) / distribution['depth_scores']['count'] * 100:.1f}%
- **ç‰¹ç‚¹**: æ·±åº¦åˆ†æ•° < {distribution['depth_scores']['p25']:.2f}

## ğŸ’¡ æ‰©äº§ç­–ç•¥å»ºè®®

### é’ˆå¯¹é«˜å¼ºåº¦æ ·æœ¬
1. **ä¼˜å…ˆæ‰©å……**: è¿™äº›æ ·æœ¬å…·æœ‰æœ€ä½³çš„æ·±åº¦æ¨ç†ç‰¹å¾
2. **å­¦ä¹ æ¨¡å¼**: åˆ†æé«˜å¼ºåº¦æ ·æœ¬çš„æ¾„æ¸…é—®å¥ç”Ÿæˆæ¨¡å¼
3. **è´¨é‡åŸºå‡†**: ä»¥è¿™äº›æ ·æœ¬ä½œä¸ºæ‰©äº§æ—¶çš„è´¨é‡æ ‡å‡†

### é’ˆå¯¹ä¸­ç­‰å¼ºåº¦æ ·æœ¬
1. **é‡ç‚¹ä¼˜åŒ–**: é€šè¿‡å¢å¼ºå…³é”®è¯æå–å’Œè¯æ®è·¨åº¦æ¥æå‡æ·±åº¦
2. **å¹³è¡¡ç­–ç•¥**: ç¡®ä¿é—®é¢˜æ•°é‡ä¸è´¨é‡çš„å¹³è¡¡
3. **æ¸è¿›æå‡**: é€æ­¥æé«˜è¿™äº›æ ·æœ¬çš„æ¨ç†å¤æ‚åº¦

### é’ˆå¯¹ä½å¼ºåº¦æ ·æœ¬
1. **è´¨é‡æå‡**: å¢åŠ æ›´å…·ä½“çš„æ¾„æ¸…é—®å¥
2. **å¤šæ ·åŒ–**: å¼•å…¥æ›´å¤šæ ·åŒ–çš„æ¨ç†æ¨¡å¼
3. **è¿‡æ»¤ç­–ç•¥**: è€ƒè™‘æ˜¯å¦éœ€è¦é‡æ–°ç”Ÿæˆè¿™äº›æ ·æœ¬

## ğŸ”§ æŠ€æœ¯æ”¹è¿›å»ºè®®

### æ·±åº¦åˆ†æ•°è®¡ç®—ä¼˜åŒ–
```
å½“å‰æƒé‡: é—®é¢˜é•¿åº¦20% + å…³é”®è¯30% + è¯æ®è·¨åº¦30% + åˆ†æ”¯20%
å»ºè®®æƒé‡: é—®é¢˜é•¿åº¦15% + å…³é”®è¯35% + è¯æ®è·¨åº¦35% + åˆ†æ”¯15%
```

### è´¨é‡æå‡æªæ–½
1. **å¢å¼ºå…³é”®è¯æå–**: ä½¿ç”¨æ›´å…ˆè¿›çš„NLPæŠ€æœ¯æå–å…³é”®æ¦‚å¿µ
2. **ä¸Šä¸‹æ–‡ç†è§£**: æ”¹è¿›è¯æ®è·¨åº¦è®¡ç®—ï¼Œæ”¯æŒæ›´å¤æ‚çš„æ¨ç†é“¾
3. **å¤šæ ·æ€§ä¿è¯**: ç¡®ä¿ä¸åŒå¼ºåº¦å±‚çº§çš„æ ·æœ¬éƒ½æœ‰ä»£è¡¨æ€§

---

*æ­¤æŠ¥å‘Šç”±æ·±åº¦å¼ºåº¦åˆ†å±‚åˆ†æå™¨è‡ªåŠ¨ç”Ÿæˆ*
"""

    # è®¾ç½®æ—¶é—´æˆ³
    import datetime
    report = report.replace('"null"', f'"{datetime.datetime.now().isoformat()}"')

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"âœ… ç”Ÿæˆæ·±åº¦åˆ†å±‚å¿«æŠ¥: {report_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ·±åº¦å¼ºåº¦åˆ†å±‚åˆ†æå™¨')
    parser.add_argument('--input', '-i', required=True, help='è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', '-o', required=True, help='è¾“å‡ºç›®å½•è·¯å¾„')

    args = parser.parse_args()

    print("ğŸ” æ·±åº¦å¼ºåº¦åˆ†å±‚åˆ†æ - å¼€å§‹æ‰§è¡Œ")
    print("=" * 60)
    print(f"ğŸ“– è¾“å…¥æ–‡ä»¶: {args.input}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")

    # åŠ è½½æ•°æ®
    print("ğŸ“– åŠ è½½æ•°æ®...")
    samples = load_jsonl_file(args.input)
    print(f"   åŠ è½½äº† {len(samples)} ä¸ªæ ·æœ¬")

    if not samples:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ ·æœ¬ï¼Œé€€å‡º")
        return

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # åˆ†ææ·±åº¦åˆ†å¸ƒ
    print("ğŸ”¬ åˆ†ææ·±åº¦å¼ºåº¦åˆ†å¸ƒ...")
    distribution = analyze_depth_distribution(samples)
    print("   âœ… åˆ†å¸ƒåˆ†æå®Œæˆ")

    # åˆ›å»ºå¼ºåº¦åˆ†æ¡¶
    print("ğŸª£ åˆ›å»ºå¼ºåº¦åˆ†æ¡¶...")
    buckets = create_intensity_buckets(samples)
    print("   âœ… åˆ†æ¡¶åˆ›å»ºå®Œæˆ")

    # ä¿å­˜è¯¦ç»†æŒ‡æ ‡æ•°æ®
    metrics_path = output_dir / "depth_intensity.json"
    metrics_data = {
        'timestamp': json.dumps(None),  # ç¨åè®¾ç½®
        'distribution': distribution,
        'buckets_summary': {
            'high_intensity_count': len(buckets['high_intensity']),
            'medium_intensity_count': len(buckets['medium_intensity']),
            'low_intensity_count': len(buckets['low_intensity'])
        },
        'sample_details': buckets  # åªä¿å­˜å‰å‡ ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
    }

    # è®¾ç½®æ—¶é—´æˆ³
    import datetime
    metrics_data['timestamp'] = datetime.datetime.now().isoformat()

    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(metrics_data, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ ä¿å­˜è¯¦ç»†æŒ‡æ ‡: {metrics_path}")

    # ç”Ÿæˆå¿«æŠ¥
    print("ğŸ“Š ç”Ÿæˆåˆ†å±‚å¿«æŠ¥...")
    generate_depth_report(distribution, buckets, output_dir)

    print("\n" + "=" * 60)
    print("ğŸ‰ æ·±åº¦å¼ºåº¦åˆ†å±‚åˆ†æå®Œæˆï¼")
    print(f"ğŸ“Š è¯¦ç»†æŒ‡æ ‡: {metrics_path}")
    print(f"ğŸ“‹ åˆ†å±‚å¿«æŠ¥: {output_dir}/depth_v1.md")
    print("=" * 60)


if __name__ == "__main__":
    main()
