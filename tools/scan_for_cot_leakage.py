#!/usr/bin/env python3
"""Scan for Chain-of-Thought Leakage in Dataset

æ‰«ææ•°æ®é›†ä¸­æ˜¯å¦å­˜åœ¨æ€ç»´é“¾æ³„æ¼åˆ°å¯¹è¯å†å²çš„ç°è±¡ã€‚
æ£€æŸ¥model_targetå­—æ®µä¸­æ˜¯å¦åŒ…å«æ˜æ˜¾çš„CoTç—•è¿¹ã€‚
"""

import json
import sys
import re
from pathlib import Path
from typing import List, Dict, Any

# CoTæ³„æ¼æ£€æµ‹æ¨¡å¼
COT_INDICATORS = [
    # ä¸­æ–‡æ¨ç†å…³é”®è¯
    "é¦–å…ˆ", "å…¶æ¬¡", "ç„¶å", "æœ€å", "æ¥ä¸‹æ¥",
    "å› ä¸º", "æ‰€ä»¥", "å› æ­¤", "ç”±äº", "æ ¹æ®",
    "åˆ†æ", "è€ƒè™‘", "æ€è€ƒ", "æ¨ç†", "åˆ¤æ–­",
    "æ­¥éª¤", "è¿‡ç¨‹", "é˜¶æ®µ", "ç¯èŠ‚", "æ–¹æ³•",
    "ç»¼ä¸Šæ‰€è¿°", "æ€»çš„æ¥è¯´", "ä¹Ÿå°±æ˜¯è¯´", "æ¢å¥è¯è¯´",
    "è®©æˆ‘æƒ³æƒ³", "æˆ‘éœ€è¦", "åº”è¯¥", "å¯ä»¥",

    # è‹±æ–‡æ¨ç†å…³é”®è¯
    "first", "second", "then", "finally", "next",
    "because", "so", "therefore", "since", "according to",
    "analyze", "consider", "think", "reason", "judge",
    "step", "process", "stage", "phase", "method",
    "in conclusion", "overall", "in other words",
    "let me think", "I need", "should", "can",

    # ç‰¹å®šCoTæ¨¡å¼
    "Let's think", "Chain-of-Thought", "CoT",
    "Step by step", "Break it down",
    "æ¨ç†è¿‡ç¨‹", "æ€è€ƒè¿‡ç¨‹", "åˆ†æè¿‡ç¨‹", "å†³ç­–è¿‡ç¨‹",
    "è®©æˆ‘æ¥åˆ†æ", "æˆ‘æ¥æ€è€ƒ", "éœ€è¦è€ƒè™‘",
]

# å…è®¸åœ¨<think>æ ‡ç­¾å†…çš„å…³é”®è¯ï¼ˆä¸è®¡ä¸ºæ³„æ¼ï¼‰
ALLOWED_IN_THINK = [
    "é¦–å…ˆ", "å…¶æ¬¡", "ç„¶å", "æœ€å", "å› ä¸º", "æ‰€ä»¥", "å› æ­¤",
    "åˆ†æ", "è€ƒè™‘", "æ€è€ƒ", "æ¨ç†", "æ­¥éª¤", "è¿‡ç¨‹",
    "Let's think", "let me think", "I need to think",
]

class CoTLeakageScanner:
    """æ€ç»´é“¾æ³„æ¼æ‰«æå™¨"""

    def __init__(self):
        self.leakages = []
        self.total_scanned = 0

    def scan_file(self, file_path: str) -> List[Dict[str, Any]]:
        """æ‰«æå•ä¸ªæ–‡ä»¶"""
        path = Path(file_path)
        if not path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        leakages = []

        with open(path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue

                try:
                    data = json.loads(line)
                    leakage = self._scan_sample(data, line_num, file_path)
                    if leakage:
                        leakages.extend(leakage)
                except json.JSONDecodeError:
                    continue  # è·³è¿‡æ— æ•ˆJSONè¡Œ

        self.leakages.extend(leakages)
        self.total_scanned += 1
        return leakages

    def scan_directory(self, dir_path: str) -> List[Dict[str, Any]]:
        """æ‰«æç›®å½•ä¸‹çš„æ‰€æœ‰JSONLæ–‡ä»¶"""
        path = Path(dir_path)
        if not path.exists():
            raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {dir_path}")

        all_leakages = []

        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰.jsonlæ–‡ä»¶
        for jsonl_file in path.rglob("*.jsonl"):
            try:
                leakages = self.scan_file(str(jsonl_file))
                all_leakages.extend(leakages)
            except Exception as e:
                print(f"âš ï¸  æ‰«ææ–‡ä»¶å¤±è´¥ {jsonl_file}: {e}", file=sys.stderr)

        return all_leakages

    def _scan_sample(self, data: Dict[str, Any], line_num: int, file_path: str) -> List[Dict[str, Any]]:
        """æ‰«æå•ä¸ªæ ·æœ¬"""
        leakages = []
        sample_id = data.get("id", f"line_{line_num}")

        # æ‰«æturnsä¸­çš„model_target
        turns = data.get("turns", [])
        for turn_idx, turn in enumerate(turns):
            if turn.get("role") == "model_target":
                text = turn.get("text", "")
                leakage_info = self._detect_leakage(text, sample_id, turn_idx, file_path, line_num)
                if leakage_info:
                    leakages.append(leakage_info)

        return leakages

    def _detect_leakage(self, text: str, sample_id: str, turn_idx: int,
                        file_path: str, line_num: int) -> Dict[str, Any]:
        """æ£€æµ‹æ–‡æœ¬ä¸­çš„æ€ç»´é“¾æ³„æ¼"""
        if not text:
            return None

        text_lower = text.lower()

        # æå–thinkå†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
        think_content = ""
        if "<think>" in text and "</think>" in text:
            think_match = re.search(r'<think>(.*?)</think>', text, re.DOTALL | re.IGNORECASE)
            if think_match:
                think_content = think_match.group(1).lower()

        # æ£€æŸ¥æ¯ä¸ªCoTæŒ‡æ ‡
        for indicator in COT_INDICATORS:
            indicator_lower = indicator.lower()

            # å¦‚æœæŒ‡æ ‡å‡ºç°åœ¨æ–‡æœ¬ä¸­
            if indicator_lower in text_lower:
                # å¦‚æœåœ¨thinkæ ‡ç­¾å†…ï¼Œä¸”æ˜¯å…è®¸çš„å…³é”®è¯ï¼Œåˆ™è·³è¿‡
                if (indicator in ALLOWED_IN_THINK and
                    indicator_lower in think_content):
                    continue

                # æ£€æŸ¥æ˜¯å¦åœ¨thinkæ ‡ç­¾å¤–å‡ºç°
                if "<think>" in text and "</think>" in text:
                    # æœ‰thinkæ ‡ç­¾ï¼Œæ£€æŸ¥æ³„æ¼å†…å®¹æ˜¯å¦åœ¨æ ‡ç­¾å¤–
                    think_start = text.lower().find("<think>")
                    think_end = text.lower().find("</think>") + len("</think>")

                    # åœ¨æ ‡ç­¾å‰çš„å†…å®¹
                    before_think = text[:think_start]
                    # åœ¨æ ‡ç­¾åçš„å†…å®¹
                    after_think = text[think_end:]

                    if (indicator_lower in before_think.lower() or
                        indicator_lower in after_think.lower()):
                        return {
                            "sample_id": sample_id,
                            "file_path": file_path,
                            "line_num": line_num,
                            "turn_idx": turn_idx,
                            "indicator": indicator,
                            "context": text[:100] + "..." if len(text) > 100 else text,
                            "severity": "high"
                        }
                else:
                    # æ²¡æœ‰thinkæ ‡ç­¾ï¼Œç›´æ¥ç®—æ³„æ¼
                    return {
                        "sample_id": sample_id,
                        "file_path": file_path,
                        "line_num": line_num,
                        "turn_idx": turn_idx,
                        "indicator": indicator,
                        "context": text[:100] + "..." if len(text) > 100 else text,
                        "severity": "high"
                    }

        return None

    def get_summary_report(self) -> Dict[str, Any]:
        """è·å–æ±‡æ€»æŠ¥å‘Š"""
        high_severity = [l for l in self.leakages if l["severity"] == "high"]

        # æŒ‰æ–‡ä»¶åˆ†ç»„ç»Ÿè®¡
        file_stats = {}
        for leakage in self.leakages:
            file_path = leakage["file_path"]
            if file_path not in file_stats:
                file_stats[file_path] = 0
            file_stats[file_path] += 1

        # æŒ‰æŒ‡æ ‡åˆ†ç»„ç»Ÿè®¡
        indicator_stats = {}
        for leakage in self.leakages:
            indicator = leakage["indicator"]
            if indicator not in indicator_stats:
                indicator_stats[indicator] = 0
            indicator_stats[indicator] += 1

        return {
            "total_scanned_files": self.total_scanned,
            "total_leakages": len(self.leakages),
            "high_severity_leakages": len(high_severity),
            "file_stats": file_stats,
            "indicator_stats": indicator_stats,
            "leakages": self.leakages[:20]  # åªè¿”å›å‰20ä¸ªè¯¦ç»†ä¿¡æ¯
        }

def main():
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python tools/scan_for_cot_leakage.py <æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„>")
        print("ç¤ºä¾‹:")
        print("  python tools/scan_for_cot_leakage.py data/seed/ALC/seed.jsonl")
        print("  python tools/scan_for_cot_leakage.py data/seed/")
        sys.exit(1)

    target_path = sys.argv[1]
    path = Path(target_path)

    scanner = CoTLeakageScanner()

    try:
        if path.is_file():
            leakages = scanner.scan_file(target_path)
        elif path.is_dir():
            leakages = scanner.scan_directory(target_path)
        else:
            print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {target_path}")
            sys.exit(1)

        report = scanner.get_summary_report()

        # è¾“å‡ºæŠ¥å‘Š
        print("ğŸ” CoTæ³„æ¼æ‰«ææŠ¥å‘Š")
        print(f"ğŸ“ æ‰«ææ–‡ä»¶æ•°: {report['total_scanned_files']}")
        print(f"ğŸš¨ æ€»æ³„æ¼æ•°: {report['total_leakages']}")
        print(f"âš ï¸  é«˜ä¸¥é‡åº¦æ³„æ¼: {report['high_severity_leakages']}")
        print()

        if report["file_stats"]:
            print("ğŸ“Š æŒ‰æ–‡ä»¶ç»Ÿè®¡:")
            for file_path, count in sorted(report["file_stats"].items()):
                print(f"  {file_path}: {count} ä¸ªæ³„æ¼")
            print()

        if report["indicator_stats"]:
            print("ğŸ” å¸¸è§æ³„æ¼æŒ‡æ ‡:")
            for indicator, count in sorted(report["indicator_stats"].items(),
                                          key=lambda x: x[1], reverse=True)[:10]:
                print(f"  '{indicator}': {count} æ¬¡")
            print()

        if report["leakages"]:
            print("ğŸ“‹ æ³„æ¼è¯¦æƒ… (å‰5ä¸ª):")
            for i, leakage in enumerate(report["leakages"][:5], 1):
                print(f"  {i}. {leakage['sample_id']} ({leakage['file_path']}:{leakage['line_num']})")
                print(f"     æ³„æ¼æŒ‡æ ‡: '{leakage['indicator']}'")
                print(f"     ä¸Šä¸‹æ–‡: {leakage['context']}")
                print()

        # æ ¹æ®æ³„æ¼æƒ…å†µè®¾ç½®é€€å‡ºç 
        if report["high_severity_leakages"] > 0:
            print("âŒ å‘ç°é«˜ä¸¥é‡åº¦CoTæ³„æ¼ï¼Œå»ºè®®ä¿®å¤åé‡æ–°æäº¤")
            sys.exit(1)
        else:
            print("âœ… æœªå‘ç°CoTæ³„æ¼")

    except Exception as e:
        print(f"âŒ æ‰«æå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
