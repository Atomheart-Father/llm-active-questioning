#!/usr/bin/env python3
"""
ç¦»çº¿è¯„æµ‹v1ï¼šç»“æ„å®Œæ•´æ€§è¯„ä¼°
å¯¹æ•°æ®é›†è¿›è¡Œæ— å¤–éƒ¨LLMçš„ç»“æ„è´¨é‡è¯„ä¼°

è¯„ä¼°æŒ‡æ ‡ï¼š
1. ç»“æ„å®Œæ•´ç‡ï¼šå­—æ®µå®Œå¤‡æ€§
2. clarificationè¦†ç›–ç‡ï¼šæ¾„æ¸…é—®å¥çš„æœ‰æ•ˆæ€§
3. branchä¸€è‡´æ€§ï¼šé—®å¥ä¸ç­”æ¡ˆçš„å¯¹åº”å…³ç³»
4. å†—ä½™ç‡ï¼šé‡å¤é—®å¥æ¯”ä¾‹
5. é•¿åº¦/å›åˆæ§åˆ¶ï¼šæ–‡æœ¬é•¿åº¦åˆ†å¸ƒ

è¾“å‡ºï¼šmetrics_eval_v1.json
"""

import json
import argparse
from collections import defaultdict, Counter
from pathlib import Path
import re
from typing import Dict, List, Any


def load_jsonl_file(filepath: str) -> List[Dict[str, Any]]:
    """åŠ è½½JSONLæ–‡ä»¶"""
    samples = []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        sample = json.loads(line)
                        samples.append(sample)
                    except json.JSONDecodeError as e:
                        print(f"è­¦å‘Š: {filepath}:{line_num} JSONè§£æé”™è¯¯: {e}")
    except FileNotFoundError:
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {filepath}")
        return []
    return samples


def evaluate_structural_completeness(samples: List[Dict[str, Any]]) -> Dict[str, Any]:
    """è¯„ä¼°ç»“æ„å®Œæ•´ç‡"""
    required_fields = ['uid', 'user_query', 'clarification_questions',
                      'assistant_response', 'task_type', 'source', 'licensing']

    completeness_stats = defaultdict(int)
    total_samples = len(samples)

    for sample in samples:
        for field in required_fields:
            if field in sample and sample[field]:
                completeness_stats[field] += 1

    # è®¡ç®—å®Œæ•´ç‡
    completeness_rates = {}
    for field, count in completeness_stats.items():
        completeness_rates[field] = {
            'count': count,
            'rate': count / total_samples if total_samples > 0 else 0
        }

    # æ€»ä½“å®Œæ•´ç‡ï¼ˆæ‰€æœ‰å­—æ®µéƒ½å®Œæ•´çš„æ ·æœ¬æ¯”ä¾‹ï¼‰
    fully_complete = sum(1 for sample in samples
                        if all(field in sample and sample[field] for field in required_fields))
    completeness_rates['overall'] = {
        'count': fully_complete,
        'rate': fully_complete / total_samples if total_samples > 0 else 0
    }

    return completeness_rates


def evaluate_clarification_coverage(samples: List[Dict[str, Any]]) -> Dict[str, Any]:
    """è¯„ä¼°clarificationè¦†ç›–ç‡"""
    coverage_stats = {
        'total_samples': len(samples),
        'with_clarifications': 0,
        'empty_clarifications': 0,
        'clarification_lengths': [],
        'clarification_word_counts': []
    }

    for sample in samples:
        questions = sample.get('clarification_questions', [])

        if questions:
            coverage_stats['with_clarifications'] += 1
            coverage_stats['clarification_lengths'].append(len(questions))

            # è®¡ç®—æ€»è¯æ•°
            total_words = 0
            for q in questions:
                if isinstance(q, str):
                    words = re.findall(r'\b\w+\b', q)
                    total_words += len(words)
            coverage_stats['clarification_word_counts'].append(total_words)
        else:
            coverage_stats['empty_clarifications'] += 1

    # è®¡ç®—ç»Ÿè®¡
    lengths = coverage_stats['clarification_lengths']
    word_counts = coverage_stats['clarification_word_counts']

    coverage_stats['avg_clarification_count'] = sum(lengths) / len(lengths) if lengths else 0
    coverage_stats['avg_word_count'] = sum(word_counts) / len(word_counts) if word_counts else 0
    coverage_stats['coverage_rate'] = coverage_stats['with_clarifications'] / coverage_stats['total_samples']

    return coverage_stats


def evaluate_branch_consistency(samples: List[Dict[str, Any]]) -> Dict[str, Any]:
    """è¯„ä¼°branchä¸€è‡´æ€§ï¼ˆé—®å¥ä¸ç­”æ¡ˆçš„å¯¹åº”å…³ç³»ï¼‰"""
    consistency_stats = {
        'total_samples': len(samples),
        'consistent_samples': 0,
        'inconsistent_samples': 0,
        'consistency_errors': []
    }

    for i, sample in enumerate(samples):
        questions = sample.get('clarification_questions', [])
        response = sample.get('assistant_response', '')

        if not questions or not response:
            continue

        # æå–responseä¸­çš„æšä¸¾ç­”æ¡ˆæ•°é‡
        answer_pattern = r'è‹¥é—®é¢˜\d+åˆ™ç­”æ¡ˆï¼š'
        enumerated_answers = re.findall(answer_pattern, response)

        is_consistent = len(questions) == len(enumerated_answers)

        if is_consistent:
            consistency_stats['consistent_samples'] += 1
        else:
            consistency_stats['inconsistent_samples'] += 1
            consistency_stats['consistency_errors'].append({
                'index': i,
                'uid': sample.get('uid', 'unknown'),
                'question_count': len(questions),
                'answer_count': len(enumerated_answers)
            })

    total_valid = consistency_stats['consistent_samples'] + consistency_stats['inconsistent_samples']
    consistency_stats['consistency_rate'] = (consistency_stats['consistent_samples'] / total_valid
                                           if total_valid > 0 else 0)

    return consistency_stats


def evaluate_redundancy(samples: List[Dict[str, Any]]) -> Dict[str, Any]:
    """è¯„ä¼°å†—ä½™ç‡"""
    redundancy_stats = {
        'total_questions': 0,
        'unique_questions': set(),
        'duplicate_questions': defaultdict(int)
    }

    for sample in samples:
        questions = sample.get('clarification_questions', [])
        for q in questions:
            if isinstance(q, str):
                redundancy_stats['total_questions'] += 1
                redundancy_stats['unique_questions'].add(q.lower().strip())

    unique_count = len(redundancy_stats['unique_questions'])
    total_count = redundancy_stats['total_questions']

    redundancy_stats['redundancy_rate'] = 1 - (unique_count / total_count) if total_count > 0 else 0
    redundancy_stats['unique_count'] = unique_count

    return redundancy_stats


def evaluate_length_control(samples: List[Dict[str, Any]]) -> Dict[str, Any]:
    """è¯„ä¼°é•¿åº¦/å›åˆæ§åˆ¶"""
    length_stats = {
        'query_lengths': [],
        'response_lengths': [],
        'question_lengths': []
    }

    for sample in samples:
        # æŸ¥è¯¢é•¿åº¦
        query = sample.get('user_query', '')
        if query:
            length_stats['query_lengths'].append(len(query))

        # å“åº”é•¿åº¦
        response = sample.get('assistant_response', '')
        if response:
            length_stats['response_lengths'].append(len(response))

        # é—®é¢˜é•¿åº¦
        questions = sample.get('clarification_questions', [])
        for q in questions:
            if isinstance(q, str):
                length_stats['question_lengths'].append(len(q))

    # è®¡ç®—ç»Ÿè®¡é‡
    stats_keys = list(length_stats.keys())  # åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹æ—¶è¿­ä»£
    for key in stats_keys:
        values = length_stats[key]
        if values:
            sorted_values = sorted(values)
            length_stats[f'{key}_stats'] = {
                'count': len(values),
                'mean': sum(values) / len(values),
                'min': min(values),
                'max': max(values),
                'p50': sorted_values[len(values) // 2],
                'p90': sorted_values[int(len(values) * 0.9)]
            }
        else:
            length_stats[f'{key}_stats'] = {'count': 0}

    return length_stats


def generate_evaluation_report(results: Dict[str, Any]) -> Dict[str, Any]:
    """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
    report = {
        'timestamp': json.dumps(None),  # ç¨åè®¾ç½®
        'dataset_info': {
            'total_samples': results['structural_completeness']['overall']['count'],
            'evaluation_scope': 'offline_structural'
        },
        'metrics': {},  # å°†åœ¨ä¸‹é¢å¡«å……
        'recommendations': []
    }

    # ç»“æ„å®Œæ•´ç‡
    completeness = results['structural_completeness']
    report['metrics']['structural_completeness'] = {
        'overall_rate': completeness['overall']['rate'],
        'field_rates': {k: v['rate'] for k, v in completeness.items() if k != 'overall'}
    }

    # clarificationè¦†ç›–ç‡
    coverage = results['clarification_coverage']
    report['metrics']['clarification_coverage'] = {
        'coverage_rate': coverage['coverage_rate'],
        'avg_clarification_count': coverage['avg_clarification_count'],
        'avg_word_count': coverage['avg_word_count']
    }

    # branchä¸€è‡´æ€§
    consistency = results['branch_consistency']
    report['metrics']['branch_consistency'] = {
        'consistency_rate': consistency['consistency_rate'],
        'consistent_samples': consistency['consistent_samples'],
        'inconsistent_samples': consistency['inconsistent_samples']
    }

    # å†—ä½™ç‡
    redundancy = results['redundancy']
    report['metrics']['redundancy'] = {
        'redundancy_rate': redundancy['redundancy_rate'],
        'unique_questions': redundancy['unique_count'],
        'total_questions': redundancy['total_questions']
    }

    # é•¿åº¦æ§åˆ¶
    length = results['length_control']
    report['metrics']['length_control'] = {
        'query_stats': length.get('query_lengths_stats', {}),
        'response_stats': length.get('response_lengths_stats', {}),
        'question_stats': length.get('question_lengths_stats', {})
    }

    # ç”Ÿæˆå»ºè®®
    if completeness['overall']['rate'] < 0.95:
        report['recommendations'].append("ç»“æ„å®Œæ•´ç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®ç”Ÿæˆæµç¨‹")

    if coverage['coverage_rate'] < 0.8:
        report['recommendations'].append("clarificationè¦†ç›–ç‡ä¸è¶³ï¼Œå»ºè®®å¢å¼ºæ¾„æ¸…é—®å¥ç”Ÿæˆ")

    if consistency['consistency_rate'] < 0.9:
        report['recommendations'].append("branchä¸€è‡´æ€§é—®é¢˜ä¸¥é‡ï¼Œå»ºè®®ä¿®å¤é—®ç­”å¯¹åº”å…³ç³»")

    if redundancy['redundancy_rate'] > 0.3:
        report['recommendations'].append("å†—ä½™ç‡è¾ƒé«˜ï¼Œå»ºè®®å»é‡ä¼˜åŒ–")

    return report


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç¦»çº¿è¯„æµ‹v1ï¼šç»“æ„å®Œæ•´æ€§è¯„ä¼°')
    parser.add_argument('--input', '-i', required=True, help='è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', required=True, help='è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')

    args = parser.parse_args()

    print("ğŸ” Stage 2 ç¦»çº¿è¯„æµ‹ v1 - å¼€å§‹æ‰§è¡Œ")
    print("=" * 60)
    print(f"ğŸ“– è¾“å…¥æ–‡ä»¶: {args.input}")
    print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {args.output}")

    # åŠ è½½æ•°æ®
    print("ğŸ“– åŠ è½½æ•°æ®...")
    samples = load_jsonl_file(args.input)
    print(f"   åŠ è½½äº† {len(samples)} ä¸ªæ ·æœ¬")

    if not samples:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ ·æœ¬ï¼Œé€€å‡º")
        return

    # æ‰§è¡Œå„é¡¹è¯„ä¼°
    print("ğŸ”¬ æ‰§è¡Œç»“æ„è¯„ä¼°...")

    results = {}
    results['structural_completeness'] = evaluate_structural_completeness(samples)
    print("   âœ… ç»“æ„å®Œæ•´ç‡è¯„ä¼°å®Œæˆ")

    results['clarification_coverage'] = evaluate_clarification_coverage(samples)
    print("   âœ… clarificationè¦†ç›–ç‡è¯„ä¼°å®Œæˆ")

    results['branch_consistency'] = evaluate_branch_consistency(samples)
    print("   âœ… branchä¸€è‡´æ€§è¯„ä¼°å®Œæˆ")

    results['redundancy'] = evaluate_redundancy(samples)
    print("   âœ… å†—ä½™ç‡è¯„ä¼°å®Œæˆ")

    results['length_control'] = evaluate_length_control(samples)
    print("   âœ… é•¿åº¦æ§åˆ¶è¯„ä¼°å®Œæˆ")

    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    print("ğŸ“Š ç”Ÿæˆç»¼åˆæŠ¥å‘Š...")
    report = generate_evaluation_report(results)

    # æ·»åŠ æ—¶é—´æˆ³
    import datetime
    report['timestamp'] = datetime.datetime.now().isoformat()

    # ä¿å­˜æŠ¥å‘Š
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print(f"ğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")

    # è¾“å‡ºå…³é”®æŒ‡æ ‡
    print("\n" + "=" * 60)
    print("ğŸ“Š å…³é”®æŒ‡æ ‡æ±‡æ€»:")

    metrics = report['metrics']
    print(".1%")
    print(".1%")
    print(".1%")
    print(".1%")

    if report['recommendations']:
        print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        for rec in report['recommendations']:
            print(f"   â€¢ {rec}")

    print("=" * 60)


if __name__ == "__main__":
    main()
