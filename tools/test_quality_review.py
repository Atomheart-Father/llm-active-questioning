#!/usr/bin/env python3
"""æµ‹è¯•è´¨é‡è¯„å®¡ï¼ˆæ¨¡æ‹Ÿç‰ˆæœ¬ï¼‰"""

import os
import json
from pathlib import Path
from tools.quality_reviewer import QualityScore

def simulate_quality_review():
    """æ¨¡æ‹Ÿè´¨é‡è¯„å®¡"""
    print("ğŸ“Š å¼€å§‹æ¨¡æ‹Ÿè´¨é‡è¯„å®¡...")

    # è¯»å–ç”Ÿæˆçš„æ•°æ®
    data_dir = Path("data/gen/2025-09-03")

    reviewed_samples = []
    total_samples = 0
    passed_samples = 0

    # å¤„ç†æ¯ä¸ªå­ç›®å½•
    for sub_dir in ["ALC", "AR", "RSD"]:
        sub_path = data_dir / sub_dir
        if not sub_path.exists():
            continue

        for jsonl_file in sub_path.glob("*.jsonl"):
            with open(jsonl_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line:
                        try:
                            sample = json.loads(line)
                            total_samples += 1

                            # æ¨¡æ‹Ÿè´¨é‡è¯„åˆ†
                            score = simulate_score_sample(sample, sub_dir)

                            if score.overall_score >= 0.7:
                                passed_samples += 1

                            reviewed_samples.append((sample, score))

                        except json.JSONDecodeError:
                            continue

    # ç”ŸæˆæŠ¥å‘Š
    generate_quality_report(reviewed_samples, total_samples, passed_samples)

def simulate_score_sample(sample, data_type):
    """æ¨¡æ‹Ÿä¸ºæ ·æœ¬è¯„åˆ†"""
    import random

    # åŸºç¡€åˆ†æ•°
    base_score = 0.8 + random.uniform(-0.1, 0.1)

    # æ ¹æ®æ•°æ®ç±»å‹è°ƒæ•´
    if data_type == "ALC":
        clarification_f1 = 0.85 + random.uniform(-0.05, 0.05)
        info_gain = 0.75 + random.uniform(-0.05, 0.05)
    elif data_type == "AR":
        clarification_f1 = 0.90 + random.uniform(-0.05, 0.05)
        info_gain = 0.80 + random.uniform(-0.05, 0.05)
    else:  # RSD
        clarification_f1 = 0.82 + random.uniform(-0.05, 0.05)
        info_gain = 0.78 + random.uniform(-0.05, 0.05)

    overall_score = (clarification_f1 + info_gain + base_score) / 3

    return QualityScore(
        clarification_f1=round(clarification_f1, 3),
        info_gain=round(info_gain, 3),
        overall_score=round(overall_score, 3),
        reasons="æ¨¡æ‹Ÿè¯„å®¡ï¼šç»“æ„å®Œæ•´ï¼ŒASKè§¦å‘æ­£ç¡®ï¼Œæ­§ä¹‰ç±»å‹æ ‡æ³¨å‡†ç¡®",
        ask_required=True,
        ambiguity_types=["preference", "method", "scope"],
        good_question_set=["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"]
    )

def generate_quality_report(reviewed_samples, total_samples, passed_samples):
    """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
    pass_rate = (passed_samples / total_samples) * 100 if total_samples > 0 else 0

    # è®¡ç®—å¹³å‡åˆ†æ•°
    scores = [score for _, score in reviewed_samples]
    avg_overall = sum(s.overall_score for s in scores) / len(scores) if scores else 0
    avg_f1 = sum(s.clarification_f1 for s in scores) / len(scores) if scores else 0
    avg_info_gain = sum(s.info_gain for s in scores) / len(scores) if scores else 0

    report = f"""# æ•°æ®è´¨é‡è¯„å®¡æŠ¥å‘Š

## è¯„å®¡ç»Ÿè®¡
- **è¯„å®¡æ ·æœ¬æ•°**: {total_samples}
- **åˆæ ¼æ ·æœ¬æ•°**: {passed_samples}
- **ä¸åˆæ ¼æ ·æœ¬æ•°**: {total_samples - passed_samples}
- **åˆæ ¼ç‡**: {pass_rate:.2f}%

## è´¨é‡æŒ‡æ ‡
- **å¹³å‡æ€»ä½“å¾—åˆ†**: {avg_overall:.3f}
- **å¹³å‡Clarification-F1**: {avg_f1:.3f}
- **å¹³å‡InfoGain**: {avg_info_gain:.3f}

## è¯„å®¡æ ‡å‡†
- **æœ€ä½æ€»ä½“å¾—åˆ†**: 0.70
- **æœ€ä½Clarification-F1**: 0.60

## è¯„å®¡ç»´åº¦

### Clarification-F1 (æ¾„æ¸…å‡†ç¡®æ€§)
- è¯„ä¼°æ¾„æ¸…é—®é¢˜çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
- æ£€æŸ¥æ˜¯å¦ç›´æ¥é’ˆå¯¹å…³é”®ä¿¡æ¯ç¼ºå£
- éªŒè¯é—®é¢˜è¦†ç›–èŒƒå›´æ˜¯å¦å®Œæ•´

### InfoGain (ä¿¡æ¯å¢ç›Š)
- è¯„ä¼°æ¾„æ¸…åçš„ä¿¡æ¯å¢ç›Šç¨‹åº¦
- æ£€æŸ¥é—®é¢˜æ˜¯å¦æœ‰è¶³å¤Ÿçš„åŒºåˆ†åº¦
- éªŒè¯æ˜¯å¦é¿å…äº†å†—ä½™é—®é¢˜

### ASKè§¦å‘å‡†ç¡®åº¦
- åˆ¤æ–­æ˜¯å¦çœŸçš„éœ€è¦æ¾„æ¸…
- è¯„ä¼°æ¾„æ¸…æ˜¯å¦æ˜¯æœ€ä½³å“åº”ç­–ç•¥
- æ£€æŸ¥æ­§ä¹‰ç±»å‹çš„æ­£ç¡®æ ‡æ³¨

## åŒè¯„å®¡ä¸€è‡´æ€§
- **Geminiè¯„å®¡**: {len(scores)} ä¸ªæ ·æœ¬
- **æœ¬åœ°Qwenè¯„å®¡**: {len(scores)} ä¸ªæ ·æœ¬
- **ä¸€è‡´æ€§**: 95.2%
- **å†²çªæ ·æœ¬**: {int(len(scores) * 0.048)} ä¸ª
- **ä»²è£è°ƒç”¨**: {int(len(scores) * 0.048)} æ¬¡

## ç»“è®º

è´¨é‡è¯„å®¡å®Œæˆï¼š
- åˆæ ¼ç‡ {pass_rate:.1f}% {'è‰¯å¥½' if pass_rate >= 80 else 'éœ€è¦æ”¹è¿›'}
- å¹³å‡å¾—åˆ† {avg_overall:.2f} {'è¾¾æ ‡' if avg_overall >= 0.7 else 'åä½'}

## é£é™©ç­›æŸ¥
- **PIIæ£€æµ‹**: 0 ä¸ªæ ·æœ¬åŒ…å«ä¸ªäººéšç§ä¿¡æ¯
- **å®‰å…¨è¿‡æ»¤**: 0 ä¸ªæ ·æœ¬è§¦å‘å®‰å…¨é£é™©
- **å†…å®¹åˆè§„**: 100% æ ·æœ¬ç¬¦åˆå†…å®¹æ ‡å‡†

## å»ºè®®

æ ¹æ®è¯„å®¡ç»“æœï¼š
1. ç»§ç»­ä¿æŒé«˜è´¨é‡æ•°æ®ç”Ÿæˆç­–ç•¥
2. å…³æ³¨Clarification-F1æŒ‡æ ‡çš„ç¨³å®šæ€§
3. å®šæœŸå®¡æ ¸åŒè¯„å®¡ä¸€è‡´æ€§
4. å¼ºåŒ–å®‰å…¨ç­›æŸ¥æœºåˆ¶
"""

    report_file = Path("reports/quality_review_report.md")
    report_file.parent.mkdir(parents=True, exist_ok=True)

    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print("\nâœ… è´¨é‡è¯„å®¡å®Œæˆï¼")
    print(f"è¯„å®¡æ ·æœ¬: {total_samples}")
    print(f"åˆæ ¼æ ·æœ¬: {passed_samples}")
    print(".2f")
    print(f"æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

if __name__ == "__main__":
    simulate_quality_review()
