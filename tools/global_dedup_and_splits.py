#!/usr/bin/env python3
"""
Stage 2 Global Deduplication and Train/Dev/Test Splits
å¯¹å·²åˆæˆçš„åˆ†ç‰‡è¿›è¡Œå…¨å±€å»é‡ï¼Œç„¶åæŒ‰task_typeåˆ†å±‚åˆ‡åˆ†ä¸ºtrain/dev/test
"""

import json
import hashlib
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Set, Tuple
import random

def load_all_shards(base_path: Path) -> List[Dict]:
    """åŠ è½½æ‰€æœ‰å·²åˆæˆçš„åˆ†ç‰‡æ ·æœ¬"""
    shards = [
        'shard-000', 'shard-001', 'shard-002', 'shard-003',
        'shard-004', 'shard-004a', 'shard-005'
    ]

    all_samples = []
    for shard_name in shards:
        shard_file = base_path / f"{shard_name}.jsonl"
        if shard_file.exists():
            print(f"ğŸ“‚ åŠ è½½åˆ†ç‰‡: {shard_name}")
            with open(shard_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    if line.strip():
                        try:
                            sample = json.loads(line.strip())
                            sample['_shard'] = shard_name
                            sample['_line_num'] = line_num
                            all_samples.append(sample)
                        except json.JSONDecodeError as e:
                            print(f"âš ï¸  è·³è¿‡ {shard_name} ç¬¬{line_num}è¡Œ: {e}")

    print(f"âœ… åŠ è½½å®Œæˆ: {len(all_samples)} ä¸ªæ ·æœ¬")
    return all_samples

def compute_text_hash(text: str) -> str:
    """è®¡ç®—æ–‡æœ¬çš„å“ˆå¸Œå€¼ç”¨äºå»é‡"""
    # ç®€å•æ ‡å‡†åŒ–ï¼šå»é™¤å¤šä½™ç©ºæ ¼ï¼Œè½¬æ¢ä¸ºå°å†™
    normalized = ' '.join(text.lower().split())
    return hashlib.md5(normalized.encode('utf-8')).hexdigest()

def deduplicate_samples(samples: List[Dict], threshold: float = 0.9) -> Tuple[List[Dict], Dict]:
    """
    åŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦çš„å»é‡
    è¿™é‡Œä½¿ç”¨ç®€å•çš„å“ˆå¸Œå»é‡ï¼Œå®é™…é¡¹ç›®ä¸­å¯ä»¥é›†æˆMinHashç­‰æ›´å¤æ‚çš„ç®—æ³•
    """
    print("ğŸ” å¼€å§‹å…¨å±€å»é‡...")

    # æŒ‰task_typeåˆ†ç»„
    samples_by_type = defaultdict(list)
    for sample in samples:
        task_type = sample.get('task_type', 'unknown')
        samples_by_type[task_type].append(sample)

    deduped_samples = []
    duplicates_removed = 0

    # ä¸ºæ¯ä¸ªtask_typeåˆ†åˆ«å»é‡
    for task_type, type_samples in samples_by_type.items():
        print(f"  å¤„ç† {task_type}: {len(type_samples)} ä¸ªæ ·æœ¬")

        # ä½¿ç”¨ç®€å•å“ˆå¸Œå»é‡ï¼ˆå®é™…é¡¹ç›®ä¸­åº”ä½¿ç”¨æ›´å¤æ‚çš„ç›¸ä¼¼åº¦ç®—æ³•ï¼‰
        seen_hashes = set()
        type_deduped = []

        for sample in type_samples:
            # åŸºäºclarification_questionså’Œassistant_responseç”Ÿæˆå“ˆå¸Œ
            questions = sample.get('clarification_questions', [])
            response = sample.get('assistant_response', '')

            # ç»„åˆå…³é”®å­—æ®µè¿›è¡Œå“ˆå¸Œ
            combined_text = ' '.join(questions) + ' ' + response
            text_hash = compute_text_hash(combined_text)

            if text_hash not in seen_hashes:
                seen_hashes.add(text_hash)
                type_deduped.append(sample)
            else:
                duplicates_removed += 1

        deduped_samples.extend(type_deduped)
        print(f"    {task_type} å»é‡å: {len(type_deduped)} ä¸ªæ ·æœ¬")

    dedup_stats = {
        'original_count': len(samples),
        'deduped_count': len(deduped_samples),
        'duplicates_removed': duplicates_removed,
        'deduplication_ratio': duplicates_removed / len(samples) if samples else 0
    }

    print("âœ… å»é‡å®Œæˆ:")
    print(f"  åŸæ ·æœ¬æ•°: {dedup_stats['original_count']}")
    print(f"  å»é‡åæ ·æœ¬æ•°: {dedup_stats['deduped_count']}")
    print(".3f")

    return deduped_samples, dedup_stats

def stratified_split(samples: List[Dict], train_ratio: float = 0.8,
                    dev_ratio: float = 0.1, test_ratio: float = 0.1) -> Tuple[List[Dict], List[Dict], List[Dict]]:
    """
    æŒ‰task_typeåˆ†å±‚åˆ‡åˆ†æ•°æ®é›†
    """
    print("âœ‚ï¸  å¼€å§‹åˆ†å±‚åˆ‡åˆ†...")

    # æŒ‰task_typeåˆ†ç»„
    samples_by_type = defaultdict(list)
    for sample in samples:
        task_type = sample.get('task_type', 'unknown')
        samples_by_type[task_type].append(sample)

    train_samples = []
    dev_samples = []
    test_samples = []

    # ä¸ºæ¯ä¸ªtask_typeåˆ†åˆ«åˆ‡åˆ†
    for task_type, type_samples in samples_by_type.items():
        print(f"  åˆ‡åˆ† {task_type}: {len(type_samples)} ä¸ªæ ·æœ¬")

        # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°
        random.seed(42)

        # éšæœºæ‰“ä¹±
        shuffled = type_samples.copy()
        random.shuffle(shuffled)

        # è®¡ç®—åˆ‡åˆ†ç‚¹
        n_total = len(shuffled)
        n_train = int(n_total * train_ratio)
        n_dev = int(n_total * dev_ratio)
        n_test = n_total - n_train - n_dev

        # åˆ‡åˆ†
        type_train = shuffled[:n_train]
        type_dev = shuffled[n_train:n_train + n_dev]
        type_test = shuffled[n_train + n_dev:]

        train_samples.extend(type_train)
        dev_samples.extend(type_dev)
        test_samples.extend(type_test)

        print(f"    {task_type} - è®­ç»ƒ: {len(type_train)}, éªŒè¯: {len(type_dev)}, æµ‹è¯•: {len(type_test)}")

    # æœ€ç»ˆéšæœºæ‰“ä¹±ä»¥é¿å…æŒ‰task_typeæ’åº
    random.seed(42)
    random.shuffle(train_samples)
    random.shuffle(dev_samples)
    random.shuffle(test_samples)

    print("âœ… åˆ‡åˆ†å®Œæˆ:")
    print(f"  è®­ç»ƒé›†: {len(train_samples)} æ ·æœ¬ ({len(train_samples)/len(samples)*100:.1f}%)")
    print(f"  éªŒè¯é›†: {len(dev_samples)} æ ·æœ¬ ({len(dev_samples)/len(samples)*100:.1f}%)")
    print(f"  æµ‹è¯•é›†: {len(test_samples)} æ ·æœ¬ ({len(test_samples)/len(samples)*100:.1f}%)")

    return train_samples, dev_samples, test_samples

def save_split_to_file(samples: List[Dict], output_path: Path, split_name: str):
    """ä¿å­˜åˆ‡åˆ†ç»“æœåˆ°æ–‡ä»¶"""
    print(f"ğŸ’¾ ä¿å­˜{split_name}é›†åˆ°: {output_path}")

    with open(output_path, 'w', encoding='utf-8') as f:
        for sample in samples:
            # ç§»é™¤å†…éƒ¨ä½¿ç”¨çš„å­—æ®µ
            clean_sample = {k: v for k, v in sample.items() if not k.startswith('_')}
            f.write(json.dumps(clean_sample, ensure_ascii=False) + '\n')

    print(f"  âœ… {split_name}é›†ä¿å­˜å®Œæˆ: {len(samples)} æ ·æœ¬")

def update_metrics_with_splits(metrics_path: Path, train_count: int, dev_count: int, test_count: int, dedup_stats: Dict):
    """æ›´æ–°metrics.jsonä¸­çš„splitsä¿¡æ¯"""
    print("ğŸ“Š æ›´æ–°metrics.jsonçš„splitsä¿¡æ¯...")

    # è¯»å–ç°æœ‰metrics
    with open(metrics_path, 'r', encoding='utf-8') as f:
        metrics = json.load(f)

    # æ·»åŠ splitsä¿¡æ¯
    metrics['splits'] = {
        'train': {
            'count': train_count,
            'percentage': train_count / (train_count + dev_count + test_count) * 100
        },
        'dev': {
            'count': dev_count,
            'percentage': dev_count / (train_count + dev_count + test_count) * 100
        },
        'test': {
            'count': test_count,
            'percentage': test_count / (train_count + dev_count + test_count) * 100
        }
    }

    # æ·»åŠ å»é‡ç»Ÿè®¡
    metrics['deduplication'] = dedup_stats

    # ä¿å­˜æ›´æ–°åçš„metrics
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)

    print("âœ… metrics.jsonæ›´æ–°å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Stage 2 å…¨å±€å»é‡ + åˆ†å±‚åˆ‡åˆ† - å¼€å§‹æ‰§è¡Œ")
    print("=" * 60)

    # è®¾ç½®è·¯å¾„
    shards_path = Path("data/interim/shards/stage2_v1")
    output_path = Path("data/processed/active_qa_v1")
    metrics_path = output_path / "metrics.json"

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path.mkdir(parents=True, exist_ok=True)

    # 1. åŠ è½½æ‰€æœ‰åˆ†ç‰‡
    print("ğŸ“‚ ç¬¬ä¸€æ­¥: åŠ è½½æ‰€æœ‰åˆ†ç‰‡")
    all_samples = load_all_shards(shards_path)

    if not all_samples:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•æ ·æœ¬æ–‡ä»¶")
        return 1

    # 2. å…¨å±€å»é‡
    print("\nğŸ” ç¬¬äºŒæ­¥: å…¨å±€å»é‡")
    deduped_samples, dedup_stats = deduplicate_samples(all_samples, threshold=0.9)

    # 3. åˆ†å±‚åˆ‡åˆ†
    print("\nâœ‚ï¸  ç¬¬ä¸‰æ­¥: åˆ†å±‚åˆ‡åˆ†")
    train_samples, dev_samples, test_samples = stratified_split(
        deduped_samples,
        train_ratio=0.8,
        dev_ratio=0.1,
        test_ratio=0.1
    )

    # 4. ä¿å­˜åˆ‡åˆ†ç»“æœ
    print("\nğŸ’¾ ç¬¬å››æ­¥: ä¿å­˜åˆ‡åˆ†ç»“æœ")
    save_split_to_file(train_samples, output_path / "train.jsonl", "è®­ç»ƒ")
    save_split_to_file(dev_samples, output_path / "dev.jsonl", "éªŒè¯")
    save_split_to_file(test_samples, output_path / "test.jsonl", "æµ‹è¯•")

    # 5. æ›´æ–°metrics
    print("\nğŸ“Š ç¬¬äº”æ­¥: æ›´æ–°metrics.json")
    update_metrics_with_splits(
        metrics_path,
        len(train_samples),
        len(dev_samples),
        len(test_samples),
        dedup_stats
    )

    # è¾“å‡ºæ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ‰ å…¨å±€å»é‡ + åˆ†å±‚åˆ‡åˆ†å®Œæˆï¼")
    print("=" * 60)
    print("ğŸ“ˆ å¤„ç†ç»Ÿè®¡:")
    print(f"  åŸå§‹æ ·æœ¬æ•°: {dedup_stats['original_count']}")
    print(f"  å»é‡åæ ·æœ¬æ•°: {dedup_stats['deduped_count']}")
    print(".3f")
    print()
    print("ğŸ“Š åˆ‡åˆ†ç»“æœ:")
    print(f"  è®­ç»ƒé›†: {len(train_samples)} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(dev_samples)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(test_samples)} æ ·æœ¬")
    print()
    print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"  è®­ç»ƒé›†: data/processed/active_qa_v1/train.jsonl")
    print(f"  éªŒè¯é›†: data/processed/active_qa_v1/dev.jsonl")
    print(f"  æµ‹è¯•é›†: data/processed/active_qa_v1/test.jsonl")
    print()
    print("ğŸ’¡ å»ºè®®è¿è¡Œå®ˆæŠ¤æ ¡éªŒç¡®è®¤ç»“æœ: python3 tools/guard_check_metrics.py")

    return 0

if __name__ == "__main__":
    exit(main())
