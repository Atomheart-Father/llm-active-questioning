#!/usr/bin/env python3
"""
æ–°ç³»ç»Ÿé›†æˆæµ‹è¯•
æµ‹è¯•å¼‚æ­¥æ‰§è¡Œå™¨å’Œå¤šç»´åº¦å¥–åŠ±ç³»ç»Ÿ
"""

import asyncio
import json
import time
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.utils.async_executor import AsyncCommandExecutor
from src.evaluation.advanced_reward_system import MultiDimensionalRewardSystem
from src.utils.logging import get_logger

logger = get_logger("test_new_systems")

async def test_async_executor():
    """æµ‹è¯•å¼‚æ­¥æ‰§è¡Œå™¨"""
    print("ğŸš€ æµ‹è¯•å¼‚æ­¥å‘½ä»¤æ‰§è¡Œå™¨")
    print("=" * 50)
    
    # æµ‹è¯•å‘½ä»¤é›†åˆ
    test_commands = [
        "echo 'Hello AsyncExecutor!'",
        "python -c 'import time; time.sleep(1); print(\"Pythonå»¶è¿Ÿæµ‹è¯•\")'",
        "ls -la | head -5",
        "date",
        "python -c 'print(\"è®¡ç®—æµ‹è¯•:\", 2+3*4)'",
        # "sleep 5",  # é•¿æ—¶é—´å‘½ä»¤
        # "false",    # å¤±è´¥å‘½ä»¤  
    ]
    
    # åˆ›å»ºæ‰§è¡Œå™¨
    executor = AsyncCommandExecutor(
        max_concurrent=3,
        timeout_s=10,
        retries=1,
        log_dir="logs/test_executor"
    )
    
    print(f"æ‰§è¡Œ {len(test_commands)} ä¸ªæµ‹è¯•å‘½ä»¤...")
    start_time = time.time()
    
    # æ‰§è¡Œå‘½ä»¤æ‰¹æ¬¡
    results = await executor.execute_batch(test_commands)
    
    execution_time = time.time() - start_time
    
    # åˆ†æç»“æœ
    success_count = sum(1 for r in results if r.ok)
    failed_count = len(results) - success_count
    
    print(f"\nğŸ“Š æ‰§è¡Œç»“æœ:")
    print(f"æ€»å‘½ä»¤æ•°: {len(results)}")
    print(f"æˆåŠŸ: {success_count}")
    print(f"å¤±è´¥: {failed_count}")
    print(f"æ€»è€—æ—¶: {execution_time:.2f}ç§’")
    print(f"å¹³å‡è€—æ—¶: {execution_time/len(results):.2f}ç§’/å‘½ä»¤")
    
    print(f"\nğŸ“ è¯¦ç»†ç»“æœ:")
    for i, result in enumerate(results):
        status = "âœ…" if result.ok else "âŒ"
        cmd_short = result.cmd[:40] + "..." if len(result.cmd) > 40 else result.cmd
        print(f"{status} {i+1}. {cmd_short}")
        print(f"    è€—æ—¶: {result.latency_ms}ms, é‡è¯•: {result.tries}æ¬¡")
        
        if result.stdout:
            stdout_short = result.stdout.strip()[:100]
            print(f"    è¾“å‡º: {stdout_short}")
        
        if not result.ok and result.stderr:
            stderr_short = result.stderr.strip()[:100]
            print(f"    é”™è¯¯: {stderr_short}")
    
    # è·å–æ‰§è¡Œæ‘˜è¦
    summary = executor.get_execution_summary()
    print(f"\nğŸ“ˆ æ‰§è¡Œæ‘˜è¦:")
    print(json.dumps(summary, indent=2, ensure_ascii=False))
    
    return results

def test_reward_system():
    """æµ‹è¯•å¤šç»´åº¦å¥–åŠ±ç³»ç»Ÿ"""
    print("\nğŸ¯ æµ‹è¯•å¤šç»´åº¦å¥–åŠ±ç³»ç»Ÿ")
    print("=" * 50)
    
    # æµ‹è¯•å¯¹è¯æ ·æœ¬
    test_dialogues = [
        {
            "id": "math_test",
            "type": "math_reasoning",
            "turns": [
                {"role": "user", "content": "ä¸€ä¸ªæ­£æ–¹å½¢çš„å‘¨é•¿æ˜¯20å˜ç±³ï¼Œé¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ"},
                {"role": "assistant", "content": "<think>æ­£æ–¹å½¢å‘¨é•¿=4Ã—è¾¹é•¿ï¼Œæ‰€ä»¥è¾¹é•¿=20Ã·4=5å˜ç±³ã€‚é¢ç§¯=è¾¹é•¿Â²=5Â²=25å¹³æ–¹å˜ç±³ã€‚</think>\n\nè®¡ç®—æ­¥éª¤ï¼š\n1. æ±‚è¾¹é•¿ï¼šå‘¨é•¿Ã·4 = 20Ã·4 = 5å˜ç±³\n2. æ±‚é¢ç§¯ï¼šè¾¹é•¿Â² = 5Â² = 25å¹³æ–¹å˜ç±³\n\nç­”æ¡ˆï¼š25å¹³æ–¹å˜ç±³"}
            ]
        },
        {
            "id": "clarification_test", 
            "type": "ambiguity_clarification",
            "turns": [
                {"role": "user", "content": "ä»–ä»€ä¹ˆæ—¶å€™æ¥çš„ï¼Ÿ"},
                {"role": "assistant", "content": "æŠ±æ­‰ï¼Œæˆ‘éœ€è¦æ›´å…·ä½“çš„ä¿¡æ¯ã€‚è¯·é—®æ‚¨æŒ‡çš„æ˜¯å“ªä½äººå£«å‘¢ï¼Ÿ"},
                {"role": "user", "content": "æˆ‘è¯´çš„æ˜¯å¼ æ•™æˆ"},
                {"role": "assistant", "content": "æ˜ç™½äº†ï¼å…³äºå¼ æ•™æˆçš„åˆ°è¾¾æ—¶é—´ï¼Œæˆ‘éœ€è¦æŸ¥çœ‹å…·ä½“çš„æ—¥ç¨‹å®‰æ’æ‰èƒ½ç»™æ‚¨å‡†ç¡®ç­”å¤ã€‚"}
            ]
        },
        {
            "id": "multi_hop_test",
            "type": "multi_hop",
            "turns": [
                {"role": "user", "content": "ä¸–ç•Œæœ€é«˜å³°åœ¨å“ªä¸ªå›½å®¶ï¼Ÿè¿™ä¸ªå›½å®¶çš„é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ"},
                {"role": "assistant", "content": "è®©æˆ‘åˆ†æ­¥å›ç­”ï¼š\n\nç¬¬ä¸€æ­¥ï¼šä¸–ç•Œæœ€é«˜å³°æ˜¯ç ç©†æœ—ç›å³°ï¼Œä½äºä¸­å›½å’Œå°¼æ³Šå°”è¾¹å¢ƒã€‚\n\nç¬¬äºŒæ­¥ï¼šæ¶‰åŠä¸¤ä¸ªå›½å®¶çš„é¦–éƒ½ï¼š\n- ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬\n- å°¼æ³Šå°”çš„é¦–éƒ½æ˜¯åŠ å¾·æ»¡éƒ½\n\næ‰€ä»¥ç­”æ¡ˆæ˜¯ï¼šç ç©†æœ—ç›å³°ä½äºä¸­å›½å’Œå°¼æ³Šå°”è¾¹å¢ƒï¼Œä¸¤å›½é¦–éƒ½åˆ†åˆ«æ˜¯åŒ—äº¬å’ŒåŠ å¾·æ»¡éƒ½ã€‚"}
            ]
        }
    ]
    
    # åˆ›å»ºå¥–åŠ±ç³»ç»Ÿ
    reward_system = MultiDimensionalRewardSystem(
        model_name="gemini-2.5-pro",
        prompt_version="test_v1",
        cache_db="logs/test_cache.sqlite"
    )
    
    print(f"è¯„ä¼° {len(test_dialogues)} ä¸ªæµ‹è¯•å¯¹è¯...")
    
    all_results = []
    for dialogue in test_dialogues:
        print(f"\nğŸ“‹ è¯„ä¼°å¯¹è¯: {dialogue['id']} ({dialogue['type']})")
        
        start_time = time.time()
        result = reward_system.evaluate_dialogue(dialogue)
        eval_time = time.time() - start_time
        
        result["evaluation_time_ms"] = int(eval_time * 1000)
        all_results.append(result)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"ä¸»å¥–åŠ±: {result['primary_reward']:.3f}")
        print(f"è¯„ä¼°è€—æ—¶: {eval_time*1000:.1f}ms")
        
        print("ç»„ä»¶åˆ†æ•°:")
        for key, score in result["component_scores"].items():
            print(f"  {key}: {score:.3f}")
        
        print("äºŒå…ƒæŒ‡æ ‡:")
        for key, value in result["binary_indicators"].items():
            indicator = "âœ…" if value else "âŒ"
            print(f"  {indicator} {key}")
        
        print(f"ç¡¬è§„åˆ™åˆ†æ•°: {result['hard_rules']['rules_score']:.3f}")
        print(f"è¯„åˆ†æ–¹å·®: {result['meta']['variance']:.4f}")
    
    # ç³»ç»Ÿçº§ç»Ÿè®¡
    print(f"\nğŸ“Š ç³»ç»Ÿè¯„ä¼°ç»Ÿè®¡:")
    primary_rewards = [r["primary_reward"] for r in all_results]
    eval_times = [r["evaluation_time_ms"] for r in all_results]
    
    print(f"å¹³å‡ä¸»å¥–åŠ±: {sum(primary_rewards)/len(primary_rewards):.3f}")
    print(f"å¥–åŠ±èŒƒå›´: {min(primary_rewards):.3f} - {max(primary_rewards):.3f}")
    print(f"å¹³å‡è¯„ä¼°æ—¶é—´: {sum(eval_times)/len(eval_times):.1f}ms")
    
    # ç¼“å­˜ç»Ÿè®¡
    cache_stats = reward_system.get_cache_stats()
    print(f"\nğŸ’¾ ç¼“å­˜ç»Ÿè®¡:")
    print(json.dumps(cache_stats, indent=2, ensure_ascii=False))
    
    return all_results

async def test_integration():
    """é›†æˆæµ‹è¯•ï¼šç»“åˆå¼‚æ­¥æ‰§è¡Œå™¨å’Œå¥–åŠ±ç³»ç»Ÿ"""
    print("\nğŸ”— é›†æˆæµ‹è¯•")
    print("=" * 50)
    
    # åˆ›å»ºä¸€äº›éœ€è¦å¼‚æ­¥æ‰§è¡Œçš„æ•°æ®å¤„ç†å‘½ä»¤
    data_commands = [
        "python -c 'import json; print(json.dumps({\"test\": \"data1\", \"score\": 0.85}))'",
        "python -c 'import json; print(json.dumps({\"test\": \"data2\", \"score\": 0.72}))'",
        "python -c 'import json; print(json.dumps({\"test\": \"data3\", \"score\": 0.91}))'",
    ]
    
    executor = AsyncCommandExecutor(max_concurrent=2, log_dir="logs/integration_test")
    
    print("æ‰§è¡Œæ•°æ®ç”Ÿæˆå‘½ä»¤...")
    results = await executor.execute_batch(data_commands)
    
    # å¤„ç†å‘½ä»¤ç»“æœå¹¶ç”¨å¥–åŠ±ç³»ç»Ÿè¯„ä¼°
    reward_system = MultiDimensionalRewardSystem(cache_db="logs/integration_cache.sqlite")
    
    processed_data = []
    for i, result in enumerate(results):
        if result.ok:
            try:
                # è§£æå‘½ä»¤è¾“å‡ºçš„JSONæ•°æ®
                data = json.loads(result.stdout.strip())
                
                # æ„å»ºæ¨¡æ‹Ÿå¯¹è¯
                dialogue = {
                    "id": f"integration_test_{i}",
                    "turns": [
                        {"role": "user", "content": f"å¤„ç†æ•°æ®: {data['test']}"},
                        {"role": "assistant", "content": f"æ•°æ®å¤„ç†å®Œæˆï¼Œé¢„æµ‹åˆ†æ•°: {data['score']}"}
                    ],
                    "command_result": result.__dict__
                }
                
                # ä½¿ç”¨å¥–åŠ±ç³»ç»Ÿè¯„ä¼°
                reward_result = reward_system.evaluate_dialogue(dialogue)
                
                processed_data.append({
                    "dialogue_id": dialogue["id"],
                    "command_success": True,
                    "command_time_ms": result.latency_ms,
                    "reward_score": reward_result["primary_reward"],
                    "evaluation": reward_result
                })
                
                print(f"âœ… å¤„ç† {dialogue['id']}: å‘½ä»¤{result.latency_ms}ms, å¥–åŠ±{reward_result['primary_reward']:.3f}")
                
            except json.JSONDecodeError as e:
                print(f"âŒ è§£æå‘½ä»¤è¾“å‡ºå¤±è´¥: {e}")
        else:
            print(f"âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥: {result.cmd}")
    
    print(f"\nğŸ¯ é›†æˆæµ‹è¯•å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(processed_data)} ä¸ªæ ·æœ¬")
    
    if processed_data:
        avg_reward = sum(d["reward_score"] for d in processed_data) / len(processed_data)
        avg_cmd_time = sum(d["command_time_ms"] for d in processed_data) / len(processed_data)
        
        print(f"å¹³å‡å¥–åŠ±åˆ†æ•°: {avg_reward:.3f}")
        print(f"å¹³å‡å‘½ä»¤æ‰§è¡Œæ—¶é—´: {avg_cmd_time:.1f}ms")
    
    return processed_data

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª æ–°ç³»ç»Ÿé›†æˆæµ‹è¯•")
    print("=" * 60)
    
    try:
        # æµ‹è¯•1: å¼‚æ­¥æ‰§è¡Œå™¨
        executor_results = await test_async_executor()
        
        # æµ‹è¯•2: å¥–åŠ±ç³»ç»Ÿ
        reward_results = test_reward_system()
        
        # æµ‹è¯•3: é›†æˆæµ‹è¯•
        integration_results = await test_integration()
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        test_summary = {
            "test_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "async_executor": {
                "total_commands": len(executor_results),
                "success_count": sum(1 for r in executor_results if r.ok),
                "avg_latency_ms": sum(r.latency_ms for r in executor_results) / len(executor_results)
            },
            "reward_system": {
                "total_dialogues": len(reward_results),
                "avg_primary_reward": sum(r["primary_reward"] for r in reward_results) / len(reward_results),
                "avg_eval_time_ms": sum(r["evaluation_time_ms"] for r in reward_results) / len(reward_results)
            },
            "integration": {
                "processed_samples": len(integration_results),
                "avg_reward": sum(d["reward_score"] for d in integration_results) / len(integration_results) if integration_results else 0
            }
        }
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = "logs/test_new_systems_results.json"
        Path("logs").mkdir(exist_ok=True)
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                "summary": test_summary,
                "executor_results": [r.__dict__ for r in executor_results],
                "reward_results": reward_results,
                "integration_results": integration_results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        print(f"\nğŸ“Š æµ‹è¯•æ‘˜è¦:")
        print(json.dumps(test_summary, indent=2, ensure_ascii=False))
        
        # éªŒè¯ä¸æ—§ç³»ç»Ÿçš„å…¼å®¹æ€§æç¤º
        print(f"\nğŸ”„ ä¸‹ä¸€æ­¥å»ºè®®:")
        print(f"1. å°†å¼‚æ­¥æ‰§è¡Œå™¨é›†æˆåˆ°æ•°æ®ç”Ÿæˆç®¡é“")
        print(f"2. åœ¨ç°æœ‰45ä¸ªå¯¹è¯ä¸Šæ¯”è¾ƒæ–°æ—§å¥–åŠ±ç³»ç»Ÿ")
        print(f"3. è¿›è¡Œå°è§„æ¨¡PPOè¯•éªŒéªŒè¯æ–°å¥–åŠ±ç³»ç»Ÿ")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())
